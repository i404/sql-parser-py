# Generated from SparkSqlBase.g4 by ANTLR 4.7.1
# encoding: utf-8
from antlr4 import *
from io import StringIO
from typing.io import TextIO
import sys

def serializedATN():
    with StringIO() as buf:
        buf.write("\3\u608b\ua72a\u8133\ub9ed\u417c\u3be7\u7786\u5964\3\u0100")
        buf.write("\u098f\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7")
        buf.write("\4\b\t\b\4\t\t\t\4\n\t\n\4\13\t\13\4\f\t\f\4\r\t\r\4\16")
        buf.write("\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22\t\22\4\23\t\23")
        buf.write("\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31")
        buf.write("\t\31\4\32\t\32\4\33\t\33\4\34\t\34\4\35\t\35\4\36\t\36")
        buf.write("\4\37\t\37\4 \t \4!\t!\4\"\t\"\4#\t#\4$\t$\4%\t%\4&\t")
        buf.write("&\4\'\t\'\4(\t(\4)\t)\4*\t*\4+\t+\4,\t,\4-\t-\4.\t.\4")
        buf.write("/\t/\4\60\t\60\4\61\t\61\4\62\t\62\4\63\t\63\4\64\t\64")
        buf.write("\4\65\t\65\4\66\t\66\4\67\t\67\48\t8\49\t9\4:\t:\4;\t")
        buf.write(";\4<\t<\4=\t=\4>\t>\4?\t?\4@\t@\4A\tA\4B\tB\4C\tC\4D\t")
        buf.write("D\4E\tE\4F\tF\4G\tG\4H\tH\4I\tI\4J\tJ\4K\tK\4L\tL\4M\t")
        buf.write("M\4N\tN\4O\tO\4P\tP\4Q\tQ\4R\tR\4S\tS\4T\tT\4U\tU\4V\t")
        buf.write("V\4W\tW\4X\tX\4Y\tY\4Z\tZ\4[\t[\4\\\t\\\4]\t]\4^\t^\4")
        buf.write("_\t_\4`\t`\4a\ta\4b\tb\4c\tc\4d\td\4e\te\4f\tf\3\2\3\2")
        buf.write("\3\2\3\3\3\3\3\3\3\4\3\4\3\4\3\5\3\5\3\5\3\6\3\6\3\6\3")
        buf.write("\7\3\7\3\7\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\5\b\u00e7\n")
        buf.write("\b\3\b\3\b\3\b\5\b\u00ec\n\b\3\b\5\b\u00ef\n\b\3\b\3\b")
        buf.write("\3\b\5\b\u00f4\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\3\b\3\b\5\b\u0101\n\b\3\b\3\b\5\b\u0105\n\b\3\b\3\b\3")
        buf.write("\b\3\b\3\b\5\b\u010c\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3")
        buf.write("\b\3\b\3\b\3\b\3\b\7\b\u011a\n\b\f\b\16\b\u011d\13\b\3")
        buf.write("\b\5\b\u0120\n\b\3\b\5\b\u0123\n\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\5\b\u012a\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\3\b\3\b\7\b\u013b\n\b\f\b\16\b\u013e\13\b")
        buf.write("\3\b\5\b\u0141\n\b\3\b\5\b\u0144\n\b\3\b\3\b\3\b\3\b\3")
        buf.write("\b\5\b\u014b\n\b\3\b\3\b\3\b\3\b\5\b\u0151\n\b\3\b\3\b")
        buf.write("\3\b\3\b\5\b\u0157\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\3\b\5\b\u0162\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3")
        buf.write("\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\5\b\u0182\n\b\3\b\3\b\3")
        buf.write("\b\3\b\3\b\3\b\5\b\u018a\n\b\3\b\3\b\5\b\u018e\n\b\3\b")
        buf.write("\3\b\3\b\5\b\u0193\n\b\3\b\3\b\3\b\3\b\5\b\u0199\n\b\3")
        buf.write("\b\3\b\3\b\3\b\3\b\3\b\5\b\u01a1\n\b\3\b\3\b\3\b\3\b\5")
        buf.write("\b\u01a7\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3")
        buf.write("\b\5\b\u01b4\n\b\3\b\6\b\u01b7\n\b\r\b\16\b\u01b8\3\b")
        buf.write("\3\b\3\b\3\b\3\b\3\b\3\b\5\b\u01c2\n\b\3\b\6\b\u01c5\n")
        buf.write("\b\r\b\16\b\u01c6\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\3\b\3\b\5\b\u01d7\n\b\3\b\3\b\3\b\7\b\u01dc")
        buf.write("\n\b\f\b\16\b\u01df\13\b\3\b\5\b\u01e2\n\b\3\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\5\b\u01ea\n\b\3\b\3\b\3\b\7\b\u01ef\n\b\f")
        buf.write("\b\16\b\u01f2\13\b\3\b\3\b\3\b\3\b\5\b\u01f8\n\b\3\b\3")
        buf.write("\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\5\b\u0207")
        buf.write("\n\b\3\b\3\b\5\b\u020b\n\b\3\b\3\b\3\b\3\b\5\b\u0211\n")
        buf.write("\b\3\b\3\b\3\b\3\b\5\b\u0217\n\b\3\b\5\b\u021a\n\b\3\b")
        buf.write("\5\b\u021d\n\b\3\b\3\b\3\b\3\b\5\b\u0223\n\b\3\b\3\b\5")
        buf.write("\b\u0227\n\b\3\b\3\b\5\b\u022b\n\b\3\b\3\b\3\b\5\b\u0230")
        buf.write("\n\b\3\b\3\b\5\b\u0234\n\b\3\b\3\b\3\b\3\b\3\b\3\b\5\b")
        buf.write("\u023c\n\b\3\b\5\b\u023f\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3")
        buf.write("\b\5\b\u0248\n\b\3\b\3\b\3\b\5\b\u024d\n\b\3\b\3\b\3\b")
        buf.write("\3\b\5\b\u0253\n\b\3\b\3\b\3\b\3\b\3\b\5\b\u025a\n\b\3")
        buf.write("\b\5\b\u025d\n\b\3\b\3\b\3\b\3\b\5\b\u0263\n\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\3\b\3\b\7\b\u026c\n\b\f\b\16\b\u026f\13\b")
        buf.write("\5\b\u0271\n\b\3\b\3\b\5\b\u0275\n\b\3\b\3\b\3\b\5\b\u027a")
        buf.write("\n\b\3\b\3\b\3\b\5\b\u027f\n\b\3\b\3\b\3\b\3\b\3\b\5\b")
        buf.write("\u0286\n\b\3\b\5\b\u0289\n\b\3\b\5\b\u028c\n\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\5\b\u0293\n\b\3\b\3\b\3\b\5\b\u0298\n\b\3")
        buf.write("\b\3\b\3\b\5\b\u029d\n\b\3\b\5\b\u02a0\n\b\3\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\3\b\5\b\u02a9\n\b\3\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\5\b\u02b1\n\b\3\b\3\b\3\b\3\b\5\b\u02b7\n\b\3\b\3\b\5")
        buf.write("\b\u02bb\n\b\3\b\3\b\5\b\u02bf\n\b\3\b\3\b\5\b\u02c3\n")
        buf.write("\b\5\b\u02c5\n\b\3\b\3\b\3\b\3\b\3\b\3\b\3\b\5\b\u02ce")
        buf.write("\n\b\3\b\3\b\3\b\3\b\5\b\u02d4\n\b\3\b\3\b\3\b\5\b\u02d9")
        buf.write("\n\b\3\b\5\b\u02dc\n\b\3\b\3\b\5\b\u02e0\n\b\3\b\5\b\u02e3")
        buf.write("\n\b\3\b\3\b\3\b\3\b\3\b\3\b\7\b\u02eb\n\b\f\b\16\b\u02ee")
        buf.write("\13\b\5\b\u02f0\n\b\3\b\3\b\5\b\u02f4\n\b\3\b\3\b\3\b")
        buf.write("\3\b\5\b\u02fa\n\b\3\b\5\b\u02fd\n\b\3\b\5\b\u0300\n\b")
        buf.write("\3\b\3\b\3\b\3\b\5\b\u0306\n\b\3\b\3\b\3\b\3\b\3\b\3\b")
        buf.write("\5\b\u030e\n\b\3\b\3\b\3\b\5\b\u0313\n\b\3\b\3\b\3\b\3")
        buf.write("\b\5\b\u0319\n\b\3\b\3\b\3\b\3\b\5\b\u031f\n\b\3\b\3\b")
        buf.write("\3\b\3\b\3\b\3\b\3\b\7\b\u0328\n\b\f\b\16\b\u032b\13\b")
        buf.write("\3\b\3\b\3\b\7\b\u0330\n\b\f\b\16\b\u0333\13\b\3\b\3\b")
        buf.write("\7\b\u0337\n\b\f\b\16\b\u033a\13\b\3\b\3\b\3\b\7\b\u033f")
        buf.write("\n\b\f\b\16\b\u0342\13\b\5\b\u0344\n\b\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\5\t\u034c\n\t\3\t\3\t\5\t\u0350\n\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\5\t\u0357\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3")
        buf.write("\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u03cb")
        buf.write("\n\t\3\t\3\t\3\t\3\t\3\t\3\t\5\t\u03d3\n\t\3\t\3\t\3\t")
        buf.write("\3\t\3\t\3\t\5\t\u03db\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\5\t\u03e4\n\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t\3\t")
        buf.write("\5\t\u03f0\n\t\3\n\3\n\5\n\u03f4\n\n\3\n\5\n\u03f7\n\n")
        buf.write("\3\n\3\n\3\n\3\n\5\n\u03fd\n\n\3\n\3\n\3\13\3\13\3\13")
        buf.write("\3\13\3\13\3\13\5\13\u0407\n\13\3\13\3\13\3\13\3\13\3")
        buf.write("\f\3\f\3\f\3\f\3\f\3\f\5\f\u0413\n\f\3\f\3\f\3\f\5\f\u0418")
        buf.write("\n\f\3\r\3\r\3\r\3\16\5\16\u041e\n\16\3\16\3\16\3\17\3")
        buf.write("\17\3\17\3\17\3\17\3\17\3\17\3\17\5\17\u042a\n\17\5\17")
        buf.write("\u042c\n\17\3\17\3\17\3\17\5\17\u0431\n\17\3\17\3\17\5")
        buf.write("\17\u0435\n\17\3\17\3\17\3\17\5\17\u043a\n\17\3\17\3\17")
        buf.write("\3\17\5\17\u043f\n\17\3\17\5\17\u0442\n\17\3\17\3\17\3")
        buf.write("\17\5\17\u0447\n\17\3\17\3\17\5\17\u044b\n\17\3\17\3\17")
        buf.write("\3\17\5\17\u0450\n\17\5\17\u0452\n\17\3\20\3\20\5\20\u0456")
        buf.write("\n\20\3\21\3\21\3\21\3\21\3\21\7\21\u045d\n\21\f\21\16")
        buf.write("\21\u0460\13\21\3\21\3\21\3\22\3\22\3\22\5\22\u0467\n")
        buf.write("\22\3\23\3\23\3\23\3\23\3\23\5\23\u046e\n\23\3\24\3\24")
        buf.write("\3\24\7\24\u0473\n\24\f\24\16\24\u0476\13\24\3\25\3\25")
        buf.write("\3\25\3\25\7\25\u047c\n\25\f\25\16\25\u047f\13\25\3\26")
        buf.write("\3\26\5\26\u0483\n\26\3\26\3\26\3\26\3\26\3\27\3\27\3")
        buf.write("\27\3\30\3\30\3\30\3\30\7\30\u0490\n\30\f\30\16\30\u0493")
        buf.write("\13\30\3\30\3\30\3\31\3\31\5\31\u0499\n\31\3\31\5\31\u049c")
        buf.write("\n\31\3\32\3\32\3\32\7\32\u04a1\n\32\f\32\16\32\u04a4")
        buf.write("\13\32\3\32\5\32\u04a7\n\32\3\33\3\33\3\33\3\33\5\33\u04ad")
        buf.write("\n\33\3\34\3\34\3\34\3\34\7\34\u04b3\n\34\f\34\16\34\u04b6")
        buf.write("\13\34\3\34\3\34\3\35\3\35\3\35\3\35\7\35\u04be\n\35\f")
        buf.write("\35\16\35\u04c1\13\35\3\35\3\35\3\36\3\36\3\36\3\36\3")
        buf.write("\36\3\36\5\36\u04cb\n\36\3\37\3\37\3\37\3\37\3\37\5\37")
        buf.write("\u04d2\n\37\3 \3 \3 \3 \5 \u04d8\n \3!\3!\3!\3\"\5\"\u04de")
        buf.write("\n\"\3\"\3\"\3\"\3\"\3\"\6\"\u04e5\n\"\r\"\16\"\u04e6")
        buf.write("\5\"\u04e9\n\"\3#\3#\3#\3#\3#\7#\u04f0\n#\f#\16#\u04f3")
        buf.write("\13#\5#\u04f5\n#\3#\3#\3#\3#\3#\7#\u04fc\n#\f#\16#\u04ff")
        buf.write("\13#\5#\u0501\n#\3#\3#\3#\3#\3#\7#\u0508\n#\f#\16#\u050b")
        buf.write("\13#\5#\u050d\n#\3#\3#\3#\3#\3#\7#\u0514\n#\f#\16#\u0517")
        buf.write("\13#\5#\u0519\n#\3#\5#\u051c\n#\3#\3#\3#\5#\u0521\n#\5")
        buf.write("#\u0523\n#\3$\5$\u0526\n$\3$\3$\3$\3%\3%\3%\3%\3%\3%\3")
        buf.write("%\5%\u0532\n%\3%\3%\3%\3%\3%\5%\u0539\n%\3%\3%\3%\3%\3")
        buf.write("%\5%\u0540\n%\3%\7%\u0543\n%\f%\16%\u0546\13%\3&\3&\3")
        buf.write("&\3&\3&\3&\3&\3&\5&\u0550\n&\3\'\3\'\5\'\u0554\n\'\3\'")
        buf.write("\3\'\5\'\u0558\n\'\3(\3(\3(\3(\3(\3(\3(\3(\3(\3(\5(\u0564")
        buf.write("\n(\3(\5(\u0567\n(\3(\3(\5(\u056b\n(\3(\3(\3(\3(\3(\3")
        buf.write("(\3(\3(\5(\u0575\n(\3(\3(\5(\u0579\n(\5(\u057b\n(\3(\5")
        buf.write("(\u057e\n(\3(\3(\5(\u0582\n(\3(\5(\u0585\n(\3(\3(\5(\u0589")
        buf.write("\n(\3(\3(\7(\u058d\n(\f(\16(\u0590\13(\3(\5(\u0593\n(")
        buf.write("\3(\3(\5(\u0597\n(\3(\3(\3(\5(\u059c\n(\3(\5(\u059f\n")
        buf.write("(\5(\u05a1\n(\3(\7(\u05a4\n(\f(\16(\u05a7\13(\3(\3(\5")
        buf.write("(\u05ab\n(\3(\5(\u05ae\n(\3(\3(\5(\u05b2\n(\3(\5(\u05b5")
        buf.write("\n(\5(\u05b7\n(\3)\3)\3)\5)\u05bc\n)\3)\7)\u05bf\n)\f")
        buf.write(")\16)\u05c2\13)\3)\3)\3*\3*\3*\3*\3*\3*\7*\u05cc\n*\f")
        buf.write("*\16*\u05cf\13*\3*\3*\5*\u05d3\n*\3+\3+\3+\3+\7+\u05d9")
        buf.write("\n+\f+\16+\u05dc\13+\3+\7+\u05df\n+\f+\16+\u05e2\13+\3")
        buf.write("+\5+\u05e5\n+\3,\3,\3,\3,\3,\7,\u05ec\n,\f,\16,\u05ef")
        buf.write("\13,\3,\3,\3,\3,\3,\3,\3,\3,\3,\3,\7,\u05fb\n,\f,\16,")
        buf.write("\u05fe\13,\3,\3,\5,\u0602\n,\3,\3,\3,\3,\3,\3,\3,\3,\7")
        buf.write(",\u060c\n,\f,\16,\u060f\13,\3,\3,\5,\u0613\n,\3-\3-\3")
        buf.write("-\3-\7-\u0619\n-\f-\16-\u061c\13-\5-\u061e\n-\3-\3-\5")
        buf.write("-\u0622\n-\3.\3.\3.\3.\3.\3.\3.\3.\3.\3.\7.\u062e\n.\f")
        buf.write(".\16.\u0631\13.\3.\3.\3.\3/\3/\3/\3/\3/\7/\u063b\n/\f")
        buf.write("/\16/\u063e\13/\3/\3/\5/\u0642\n/\3\60\3\60\5\60\u0646")
        buf.write("\n\60\3\60\5\60\u0649\n\60\3\61\3\61\3\61\5\61\u064e\n")
        buf.write("\61\3\61\3\61\3\61\3\61\3\61\7\61\u0655\n\61\f\61\16\61")
        buf.write("\u0658\13\61\5\61\u065a\n\61\3\61\3\61\3\61\5\61\u065f")
        buf.write("\n\61\3\61\3\61\3\61\7\61\u0664\n\61\f\61\16\61\u0667")
        buf.write("\13\61\5\61\u0669\n\61\3\62\3\62\3\63\3\63\7\63\u066f")
        buf.write("\n\63\f\63\16\63\u0672\13\63\3\64\3\64\3\64\3\64\5\64")
        buf.write("\u0678\n\64\3\64\3\64\3\64\3\64\3\64\5\64\u067f\n\64\3")
        buf.write("\65\5\65\u0682\n\65\3\65\3\65\3\65\5\65\u0687\n\65\3\65")
        buf.write("\3\65\3\65\3\65\5\65\u068d\n\65\3\65\3\65\5\65\u0691\n")
        buf.write("\65\3\65\5\65\u0694\n\65\3\65\5\65\u0697\n\65\3\66\3\66")
        buf.write("\3\66\3\66\5\66\u069d\n\66\3\67\3\67\3\67\5\67\u06a2\n")
        buf.write("\67\3\67\3\67\38\58\u06a7\n8\38\38\38\38\38\38\38\38\3")
        buf.write("8\38\38\38\38\38\38\38\58\u06b9\n8\58\u06bb\n8\38\58\u06be")
        buf.write("\n8\39\39\39\39\3:\3:\3:\7:\u06c7\n:\f:\16:\u06ca\13:")
        buf.write("\3;\3;\3;\3;\7;\u06d0\n;\f;\16;\u06d3\13;\3;\3;\3<\3<")
        buf.write("\5<\u06d9\n<\3=\3=\3=\3=\7=\u06df\n=\f=\16=\u06e2\13=")
        buf.write("\3=\3=\3>\3>\3>\5>\u06e9\n>\3?\3?\5?\u06ed\n?\3?\3?\3")
        buf.write("?\3?\3?\3?\5?\u06f5\n?\3?\3?\3?\3?\3?\3?\5?\u06fd\n?\3")
        buf.write("?\3?\3?\3?\5?\u0703\n?\3@\3@\3@\3@\7@\u0709\n@\f@\16@")
        buf.write("\u070c\13@\3@\3@\3A\3A\3A\3A\3A\7A\u0715\nA\fA\16A\u0718")
        buf.write("\13A\5A\u071a\nA\3A\3A\3A\3B\5B\u0720\nB\3B\3B\5B\u0724")
        buf.write("\nB\5B\u0726\nB\3C\3C\3C\3C\3C\3C\3C\5C\u072f\nC\3C\3")
        buf.write("C\3C\3C\3C\3C\3C\3C\3C\3C\5C\u073b\nC\5C\u073d\nC\3C\3")
        buf.write("C\3C\3C\3C\5C\u0744\nC\3C\3C\3C\3C\3C\5C\u074b\nC\3C\3")
        buf.write("C\3C\3C\5C\u0751\nC\3C\3C\3C\3C\5C\u0757\nC\5C\u0759\n")
        buf.write("C\3D\3D\3D\5D\u075e\nD\3D\3D\3E\3E\3E\5E\u0765\nE\3E\3")
        buf.write("E\3F\3F\5F\u076b\nF\3F\3F\5F\u076f\nF\5F\u0771\nF\3G\3")
        buf.write("G\3G\7G\u0776\nG\fG\16G\u0779\13G\3H\3H\3I\3I\3I\3I\3")
        buf.write("I\3I\3I\3I\3I\3I\5I\u0787\nI\5I\u0789\nI\3I\3I\3I\3I\3")
        buf.write("I\3I\7I\u0791\nI\fI\16I\u0794\13I\3J\5J\u0797\nJ\3J\3")
        buf.write("J\3J\3J\3J\3J\5J\u079f\nJ\3J\3J\3J\3J\3J\7J\u07a6\nJ\f")
        buf.write("J\16J\u07a9\13J\3J\3J\3J\5J\u07ae\nJ\3J\3J\3J\3J\3J\3")
        buf.write("J\5J\u07b6\nJ\3J\3J\3J\3J\5J\u07bc\nJ\3J\3J\3J\5J\u07c1")
        buf.write("\nJ\3J\3J\3J\5J\u07c6\nJ\3K\3K\3K\3K\5K\u07cc\nK\3K\3")
        buf.write("K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\3K\7")
        buf.write("K\u07e1\nK\fK\16K\u07e4\13K\3L\3L\3L\6L\u07e9\nL\rL\16")
        buf.write("L\u07ea\3L\3L\5L\u07ef\nL\3L\3L\3L\3L\3L\6L\u07f6\nL\r")
        buf.write("L\16L\u07f7\3L\3L\5L\u07fc\nL\3L\3L\3L\3L\3L\3L\3L\3L")
        buf.write("\3L\3L\3L\3L\3L\3L\7L\u080c\nL\fL\16L\u080f\13L\5L\u0811")
        buf.write("\nL\3L\3L\3L\3L\3L\3L\5L\u0819\nL\3L\3L\3L\3L\3L\3L\3")
        buf.write("L\5L\u0822\nL\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3")
        buf.write("L\3L\3L\3L\3L\3L\6L\u0837\nL\rL\16L\u0838\3L\3L\3L\3L")
        buf.write("\3L\3L\3L\3L\3L\5L\u0844\nL\3L\3L\3L\7L\u0849\nL\fL\16")
        buf.write("L\u084c\13L\5L\u084e\nL\3L\3L\3L\5L\u0853\nL\3L\3L\3L")
        buf.write("\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\6L\u0864\nL\rL\16")
        buf.write("L\u0865\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\3L\5")
        buf.write("L\u0877\nL\3L\3L\3L\3L\3L\3L\3L\3L\7L\u0881\nL\fL\16L")
        buf.write("\u0884\13L\3M\3M\3M\3M\3M\3M\3M\3M\6M\u088e\nM\rM\16M")
        buf.write("\u088f\5M\u0892\nM\3N\3N\3O\3O\3P\3P\3Q\3Q\3R\3R\7R\u089e")
        buf.write("\nR\fR\16R\u08a1\13R\3S\3S\3S\3S\5S\u08a7\nS\3T\5T\u08aa")
        buf.write("\nT\3T\3T\5T\u08ae\nT\3U\3U\3U\5U\u08b3\nU\3V\3V\3V\3")
        buf.write("V\3V\3V\3V\3V\3V\3V\3V\3V\3V\3V\3V\5V\u08c4\nV\3V\3V\5")
        buf.write("V\u08c8\nV\3V\3V\3V\3V\3V\7V\u08cf\nV\fV\16V\u08d2\13")
        buf.write("V\3V\5V\u08d5\nV\5V\u08d7\nV\3W\3W\3W\7W\u08dc\nW\fW\16")
        buf.write("W\u08df\13W\3X\3X\3X\3X\5X\u08e5\nX\3Y\3Y\3Y\7Y\u08ea")
        buf.write("\nY\fY\16Y\u08ed\13Y\3Z\3Z\3Z\3Z\3Z\5Z\u08f4\nZ\3[\3[")
        buf.write("\3[\3[\3[\3\\\3\\\3\\\3\\\7\\\u08ff\n\\\f\\\16\\\u0902")
        buf.write("\13\\\3]\3]\3]\3]\3^\3^\3^\3^\3^\3^\3^\7^\u090f\n^\f^")
        buf.write("\16^\u0912\13^\3^\3^\3^\3^\3^\7^\u0919\n^\f^\16^\u091c")
        buf.write("\13^\5^\u091e\n^\3^\3^\3^\3^\3^\7^\u0925\n^\f^\16^\u0928")
        buf.write("\13^\5^\u092a\n^\5^\u092c\n^\3^\5^\u092f\n^\3^\5^\u0932")
        buf.write("\n^\3_\3_\3_\3_\3_\3_\3_\3_\3_\3_\3_\3_\3_\3_\3_\3_\5")
        buf.write("_\u0944\n_\3`\3`\3`\3`\3`\3`\3`\5`\u094d\n`\3a\3a\3a\7")
        buf.write("a\u0952\na\fa\16a\u0955\13a\3b\3b\3b\3b\3b\3b\3b\3b\3")
        buf.write("b\3b\3b\3b\3b\3b\3b\5b\u0966\nb\3c\3c\3c\5c\u096b\nc\3")
        buf.write("d\3d\3e\5e\u0970\ne\3e\3e\5e\u0974\ne\3e\3e\5e\u0978\n")
        buf.write("e\3e\3e\5e\u097c\ne\3e\3e\5e\u0980\ne\3e\3e\5e\u0984\n")
        buf.write("e\3e\3e\5e\u0988\ne\3e\5e\u098b\ne\3f\3f\3f\7\u02ec\u0329")
        buf.write("\u0331\u0338\u0340\6H\u0090\u0094\u0096g\2\4\6\b\n\f\16")
        buf.write("\20\22\24\26\30\32\34\36 \"$&(*,.\60\62\64\668:<>@BDF")
        buf.write("HJLNPRTVXZ\\^`bdfhjlnprtvxz|~\u0080\u0082\u0084\u0086")
        buf.write("\u0088\u008a\u008c\u008e\u0090\u0092\u0094\u0096\u0098")
        buf.write("\u009a\u009c\u009e\u00a0\u00a2\u00a4\u00a6\u00a8\u00aa")
        buf.write("\u00ac\u00ae\u00b0\u00b2\u00b4\u00b6\u00b8\u00ba\u00bc")
        buf.write("\u00be\u00c0\u00c2\u00c4\u00c6\u00c8\u00ca\2\35\3\2\u00ca")
        buf.write("\u00cb\4\2RRTT\5\2\\^\u00b0\u00b0\u00b6\u00b6\4\2\16\16")
        buf.write("!!\4\2..YY\4\2\u00b0\u00b0\u00b6\u00b6\4\2\17\17\u00d7")
        buf.write("\u00d7\3\2hk\3\2hj\3\2-.\4\2KKMM\4\2\21\21\23\23\3\2\u00f6")
        buf.write("\u00f7\3\2&\'\4\2\u008c\u008d\u0092\u0092\3\2\u008e\u0091")
        buf.write("\4\2\u008c\u008d\u0095\u0095\3\2~\u0080\3\2\u0084\u008b")
        buf.write("\3\2\u008c\u0096\3\2\37\"\3\2*+\3\2\u008c\u008d\4\2DD")
        buf.write("\u009d\u009d\4\2\33\33\u009b\u009b\3\2HI\n\2\r\6588@g")
        buf.write("l\u0083\u0091\u0091\u0097\u00a0\u00a2\u00ee\u00f0\u00f1")
        buf.write("\2\u0b29\2\u00cc\3\2\2\2\4\u00cf\3\2\2\2\6\u00d2\3\2\2")
        buf.write("\2\b\u00d5\3\2\2\2\n\u00d8\3\2\2\2\f\u00db\3\2\2\2\16")
        buf.write("\u0343\3\2\2\2\20\u03ef\3\2\2\2\22\u03f1\3\2\2\2\24\u0400")
        buf.write("\3\2\2\2\26\u040c\3\2\2\2\30\u0419\3\2\2\2\32\u041d\3")
        buf.write("\2\2\2\34\u0451\3\2\2\2\36\u0453\3\2\2\2 \u0457\3\2\2")
        buf.write("\2\"\u0463\3\2\2\2$\u046d\3\2\2\2&\u046f\3\2\2\2(\u0477")
        buf.write("\3\2\2\2*\u0480\3\2\2\2,\u0488\3\2\2\2.\u048b\3\2\2\2")
        buf.write("\60\u0496\3\2\2\2\62\u04a6\3\2\2\2\64\u04ac\3\2\2\2\66")
        buf.write("\u04ae\3\2\2\28\u04b9\3\2\2\2:\u04ca\3\2\2\2<\u04d1\3")
        buf.write("\2\2\2>\u04d3\3\2\2\2@\u04d9\3\2\2\2B\u04e8\3\2\2\2D\u04f4")
        buf.write("\3\2\2\2F\u0525\3\2\2\2H\u052a\3\2\2\2J\u054f\3\2\2\2")
        buf.write("L\u0551\3\2\2\2N\u05b6\3\2\2\2P\u05b8\3\2\2\2R\u05d2\3")
        buf.write("\2\2\2T\u05d4\3\2\2\2V\u0612\3\2\2\2X\u0621\3\2\2\2Z\u0623")
        buf.write("\3\2\2\2\\\u0641\3\2\2\2^\u0643\3\2\2\2`\u064a\3\2\2\2")
        buf.write("b\u066a\3\2\2\2d\u066c\3\2\2\2f\u067e\3\2\2\2h\u0696\3")
        buf.write("\2\2\2j\u069c\3\2\2\2l\u069e\3\2\2\2n\u06bd\3\2\2\2p\u06bf")
        buf.write("\3\2\2\2r\u06c3\3\2\2\2t\u06cb\3\2\2\2v\u06d6\3\2\2\2")
        buf.write("x\u06da\3\2\2\2z\u06e5\3\2\2\2|\u0702\3\2\2\2~\u0704\3")
        buf.write("\2\2\2\u0080\u070f\3\2\2\2\u0082\u0725\3\2\2\2\u0084\u0758")
        buf.write("\3\2\2\2\u0086\u075d\3\2\2\2\u0088\u0764\3\2\2\2\u008a")
        buf.write("\u0768\3\2\2\2\u008c\u0772\3\2\2\2\u008e\u077a\3\2\2\2")
        buf.write("\u0090\u0788\3\2\2\2\u0092\u07c5\3\2\2\2\u0094\u07cb\3")
        buf.write("\2\2\2\u0096\u0876\3\2\2\2\u0098\u0891\3\2\2\2\u009a\u0893")
        buf.write("\3\2\2\2\u009c\u0895\3\2\2\2\u009e\u0897\3\2\2\2\u00a0")
        buf.write("\u0899\3\2\2\2\u00a2\u089b\3\2\2\2\u00a4\u08a2\3\2\2\2")
        buf.write("\u00a6\u08ad\3\2\2\2\u00a8\u08b2\3\2\2\2\u00aa\u08d6\3")
        buf.write("\2\2\2\u00ac\u08d8\3\2\2\2\u00ae\u08e0\3\2\2\2\u00b0\u08e6")
        buf.write("\3\2\2\2\u00b2\u08ee\3\2\2\2\u00b4\u08f5\3\2\2\2\u00b6")
        buf.write("\u08fa\3\2\2\2\u00b8\u0903\3\2\2\2\u00ba\u0931\3\2\2\2")
        buf.write("\u00bc\u0943\3\2\2\2\u00be\u094c\3\2\2\2\u00c0\u094e\3")
        buf.write("\2\2\2\u00c2\u0965\3\2\2\2\u00c4\u096a\3\2\2\2\u00c6\u096c")
        buf.write("\3\2\2\2\u00c8\u098a\3\2\2\2\u00ca\u098c\3\2\2\2\u00cc")
        buf.write("\u00cd\5\16\b\2\u00cd\u00ce\7\2\2\3\u00ce\3\3\2\2\2\u00cf")
        buf.write("\u00d0\5\u008aF\2\u00d0\u00d1\7\2\2\3\u00d1\5\3\2\2\2")
        buf.write("\u00d2\u00d3\5\u0086D\2\u00d3\u00d4\7\2\2\3\u00d4\7\3")
        buf.write("\2\2\2\u00d5\u00d6\5\u0088E\2\u00d6\u00d7\7\2\2\3\u00d7")
        buf.write("\t\3\2\2\2\u00d8\u00d9\5\u00aaV\2\u00d9\u00da\7\2\2\3")
        buf.write("\u00da\13\3\2\2\2\u00db\u00dc\5\u00acW\2\u00dc\u00dd\7")
        buf.write("\2\2\3\u00dd\r\3\2\2\2\u00de\u0344\5\32\16\2\u00df\u00e0")
        buf.write("\7d\2\2\u00e0\u0344\5\u00c2b\2\u00e1\u00e2\7Q\2\2\u00e2")
        buf.write("\u00e6\7\u00d1\2\2\u00e3\u00e4\7\u0081\2\2\u00e4\u00e5")
        buf.write("\7\"\2\2\u00e5\u00e7\7$\2\2\u00e6\u00e3\3\2\2\2\u00e6")
        buf.write("\u00e7\3\2\2\2\u00e7\u00e8\3\2\2\2\u00e8\u00eb\5\u00c2")
        buf.write("b\2\u00e9\u00ea\7t\2\2\u00ea\u00ec\7\u00f2\2\2\u00eb\u00e9")
        buf.write("\3\2\2\2\u00eb\u00ec\3\2\2\2\u00ec\u00ee\3\2\2\2\u00ed")
        buf.write("\u00ef\5\30\r\2\u00ee\u00ed\3\2\2\2\u00ee\u00ef\3\2\2")
        buf.write("\2\u00ef\u00f3\3\2\2\2\u00f0\u00f1\7O\2\2\u00f1\u00f2")
        buf.write("\7\u00bc\2\2\u00f2\u00f4\5.\30\2\u00f3\u00f0\3\2\2\2\u00f3")
        buf.write("\u00f4\3\2\2\2\u00f4\u0344\3\2\2\2\u00f5\u00f6\7o\2\2")
        buf.write("\u00f6\u00f7\7\u00d1\2\2\u00f7\u00f8\5\u00c2b\2\u00f8")
        buf.write("\u00f9\7u\2\2\u00f9\u00fa\7\u00bc\2\2\u00fa\u00fb\5.\30")
        buf.write("\2\u00fb\u0344\3\2\2\2\u00fc\u00fd\7g\2\2\u00fd\u0100")
        buf.write("\7\u00d1\2\2\u00fe\u00ff\7\u0081\2\2\u00ff\u0101\7$\2")
        buf.write("\2\u0100\u00fe\3\2\2\2\u0100\u0101\3\2\2\2\u0101\u0102")
        buf.write("\3\2\2\2\u0102\u0104\5\u00c2b\2\u0103\u0105\t\2\2\2\u0104")
        buf.write("\u0103\3\2\2\2\u0104\u0105\3\2\2\2\u0105\u0344\3\2\2\2")
        buf.write("\u0106\u010b\5\22\n\2\u0107\u0108\7\3\2\2\u0108\u0109")
        buf.write("\5\u00acW\2\u0109\u010a\7\4\2\2\u010a\u010c\3\2\2\2\u010b")
        buf.write("\u0107\3\2\2\2\u010b\u010c\3\2\2\2\u010c\u010d\3\2\2\2")
        buf.write("\u010d\u011b\5,\27\2\u010e\u010f\7\u00b9\2\2\u010f\u011a")
        buf.write("\5.\30\2\u0110\u0111\7\u00d9\2\2\u0111\u0112\7\26\2\2")
        buf.write("\u0112\u011a\5p9\2\u0113\u011a\5\24\13\2\u0114\u011a\5")
        buf.write("\30\r\2\u0115\u0116\7t\2\2\u0116\u011a\7\u00f2\2\2\u0117")
        buf.write("\u0118\7\u00bb\2\2\u0118\u011a\5.\30\2\u0119\u010e\3\2")
        buf.write("\2\2\u0119\u0110\3\2\2\2\u0119\u0113\3\2\2\2\u0119\u0114")
        buf.write("\3\2\2\2\u0119\u0115\3\2\2\2\u0119\u0117\3\2\2\2\u011a")
        buf.write("\u011d\3\2\2\2\u011b\u0119\3\2\2\2\u011b\u011c\3\2\2\2")
        buf.write("\u011c\u0122\3\2\2\2\u011d\u011b\3\2\2\2\u011e\u0120\7")
        buf.write("\20\2\2\u011f\u011e\3\2\2\2\u011f\u0120\3\2\2\2\u0120")
        buf.write("\u0121\3\2\2\2\u0121\u0123\5\32\16\2\u0122\u011f\3\2\2")
        buf.write("\2\u0122\u0123\3\2\2\2\u0123\u0344\3\2\2\2\u0124\u0129")
        buf.write("\5\22\n\2\u0125\u0126\7\3\2\2\u0126\u0127\5\u00acW\2\u0127")
        buf.write("\u0128\7\4\2\2\u0128\u012a\3\2\2\2\u0129\u0125\3\2\2\2")
        buf.write("\u0129\u012a\3\2\2\2\u012a\u013c\3\2\2\2\u012b\u012c\7")
        buf.write("t\2\2\u012c\u013b\7\u00f2\2\2\u012d\u012e\7\u00d9\2\2")
        buf.write("\u012e\u012f\7\26\2\2\u012f\u0130\7\3\2\2\u0130\u0131")
        buf.write("\5\u00acW\2\u0131\u0132\7\4\2\2\u0132\u013b\3\2\2\2\u0133")
        buf.write("\u013b\5\24\13\2\u0134\u013b\5\26\f\2\u0135\u013b\5\u0084")
        buf.write("C\2\u0136\u013b\5:\36\2\u0137\u013b\5\30\r\2\u0138\u0139")
        buf.write("\7\u00bb\2\2\u0139\u013b\5.\30\2\u013a\u012b\3\2\2\2\u013a")
        buf.write("\u012d\3\2\2\2\u013a\u0133\3\2\2\2\u013a\u0134\3\2\2\2")
        buf.write("\u013a\u0135\3\2\2\2\u013a\u0136\3\2\2\2\u013a\u0137\3")
        buf.write("\2\2\2\u013a\u0138\3\2\2\2\u013b\u013e\3\2\2\2\u013c\u013a")
        buf.write("\3\2\2\2\u013c\u013d\3\2\2\2\u013d\u0143\3\2\2\2\u013e")
        buf.write("\u013c\3\2\2\2\u013f\u0141\7\20\2\2\u0140\u013f\3\2\2")
        buf.write("\2\u0140\u0141\3\2\2\2\u0141\u0142\3\2\2\2\u0142\u0144")
        buf.write("\5\32\16\2\u0143\u0140\3\2\2\2\u0143\u0144\3\2\2\2\u0144")
        buf.write("\u0344\3\2\2\2\u0145\u0146\7Q\2\2\u0146\u014a\7R\2\2\u0147")
        buf.write("\u0148\7\u0081\2\2\u0148\u0149\7\"\2\2\u0149\u014b\7$")
        buf.write("\2\2\u014a\u0147\3\2\2\2\u014a\u014b\3\2\2\2\u014b\u014c")
        buf.write("\3\2\2\2\u014c\u014d\5\u0086D\2\u014d\u014e\7&\2\2\u014e")
        buf.write("\u0150\5\u0086D\2\u014f\u0151\5\30\r\2\u0150\u014f\3\2")
        buf.write("\2\2\u0150\u0151\3\2\2\2\u0151\u0344\3\2\2\2\u0152\u0153")
        buf.write("\7\u00d5\2\2\u0153\u0154\7R\2\2\u0154\u0156\5\u0086D\2")
        buf.write("\u0155\u0157\5 \21\2\u0156\u0155\3\2\2\2\u0156\u0157\3")
        buf.write("\2\2\2\u0157\u0158\3\2\2\2\u0158\u0159\7\u00d6\2\2\u0159")
        buf.write("\u0161\7\u00d8\2\2\u015a\u0162\5\u00c2b\2\u015b\u015c")
        buf.write("\7/\2\2\u015c\u015d\7b\2\2\u015d\u0162\5r:\2\u015e\u015f")
        buf.write("\7/\2\2\u015f\u0160\7\21\2\2\u0160\u0162\7b\2\2\u0161")
        buf.write("\u015a\3\2\2\2\u0161\u015b\3\2\2\2\u0161\u015e\3\2\2\2")
        buf.write("\u0161\u0162\3\2\2\2\u0162\u0344\3\2\2\2\u0163\u0164\7")
        buf.write("o\2\2\u0164\u0165\7R\2\2\u0165\u0166\5\u0086D\2\u0166")
        buf.write("\u0167\7\17\2\2\u0167\u0168\7b\2\2\u0168\u0169\7\3\2\2")
        buf.write("\u0169\u016a\5\u00acW\2\u016a\u016b\7\4\2\2\u016b\u0344")
        buf.write("\3\2\2\2\u016c\u016d\7o\2\2\u016d\u016e\t\3\2\2\u016e")
        buf.write("\u016f\5\u0086D\2\u016f\u0170\7p\2\2\u0170\u0171\7l\2")
        buf.write("\2\u0171\u0172\5\u0086D\2\u0172\u0344\3\2\2\2\u0173\u0174")
        buf.write("\7o\2\2\u0174\u0175\t\3\2\2\u0175\u0176\5\u0086D\2\u0176")
        buf.write("\u0177\7u\2\2\u0177\u0178\7\u00bb\2\2\u0178\u0179\5.\30")
        buf.write("\2\u0179\u0344\3\2\2\2\u017a\u017b\7o\2\2\u017b\u017c")
        buf.write("\t\3\2\2\u017c\u017d\5\u0086D\2\u017d\u017e\7\u00ba\2")
        buf.write("\2\u017e\u0181\7\u00bb\2\2\u017f\u0180\7\u0081\2\2\u0180")
        buf.write("\u0182\7$\2\2\u0181\u017f\3\2\2\2\u0181\u0182\3\2\2\2")
        buf.write("\u0182\u0183\3\2\2\2\u0183\u0184\5.\30\2\u0184\u0344\3")
        buf.write("\2\2\2\u0185\u0186\7o\2\2\u0186\u0187\7R\2\2\u0187\u0189")
        buf.write("\5\u0086D\2\u0188\u018a\5 \21\2\u0189\u0188\3\2\2\2\u0189")
        buf.write("\u018a\3\2\2\2\u018a\u018b\3\2\2\2\u018b\u018d\7\u00c9")
        buf.write("\2\2\u018c\u018e\7c\2\2\u018d\u018c\3\2\2\2\u018d\u018e")
        buf.write("\3\2\2\2\u018e\u018f\3\2\2\2\u018f\u0190\5\u00c2b\2\u0190")
        buf.write("\u0192\5\u00aeX\2\u0191\u0193\5\u00a8U\2\u0192\u0191\3")
        buf.write("\2\2\2\u0192\u0193\3\2\2\2\u0193\u0344\3\2\2\2\u0194\u0195")
        buf.write("\7o\2\2\u0195\u0196\7R\2\2\u0196\u0198\5\u0086D\2\u0197")
        buf.write("\u0199\5 \21\2\u0198\u0197\3\2\2\2\u0198\u0199\3\2\2\2")
        buf.write("\u0199\u019a\3\2\2\2\u019a\u019b\7u\2\2\u019b\u019c\7")
        buf.write("\u00a2\2\2\u019c\u01a0\7\u00f2\2\2\u019d\u019e\7O\2\2")
        buf.write("\u019e\u019f\7\u00a3\2\2\u019f\u01a1\5.\30\2\u01a0\u019d")
        buf.write("\3\2\2\2\u01a0\u01a1\3\2\2\2\u01a1\u0344\3\2\2\2\u01a2")
        buf.write("\u01a3\7o\2\2\u01a3\u01a4\7R\2\2\u01a4\u01a6\5\u0086D")
        buf.write("\2\u01a5\u01a7\5 \21\2\u01a6\u01a5\3\2\2\2\u01a6\u01a7")
        buf.write("\3\2\2\2\u01a7\u01a8\3\2\2\2\u01a8\u01a9\7u\2\2\u01a9")
        buf.write("\u01aa\7\u00a3\2\2\u01aa\u01ab\5.\30\2\u01ab\u0344\3\2")
        buf.write("\2\2\u01ac\u01ad\7o\2\2\u01ad\u01ae\7R\2\2\u01ae\u01af")
        buf.write("\5\u0086D\2\u01af\u01b3\7\17\2\2\u01b0\u01b1\7\u0081\2")
        buf.write("\2\u01b1\u01b2\7\"\2\2\u01b2\u01b4\7$\2\2\u01b3\u01b0")
        buf.write("\3\2\2\2\u01b3\u01b4\3\2\2\2\u01b4\u01b6\3\2\2\2\u01b5")
        buf.write("\u01b7\5\36\20\2\u01b6\u01b5\3\2\2\2\u01b7\u01b8\3\2\2")
        buf.write("\2\u01b8\u01b6\3\2\2\2\u01b8\u01b9\3\2\2\2\u01b9\u0344")
        buf.write("\3\2\2\2\u01ba\u01bb\7o\2\2\u01bb\u01bc\7T\2\2\u01bc\u01bd")
        buf.write("\5\u0086D\2\u01bd\u01c1\7\17\2\2\u01be\u01bf\7\u0081\2")
        buf.write("\2\u01bf\u01c0\7\"\2\2\u01c0\u01c2\7$\2\2\u01c1\u01be")
        buf.write("\3\2\2\2\u01c1\u01c2\3\2\2\2\u01c2\u01c4\3\2\2\2\u01c3")
        buf.write("\u01c5\5 \21\2\u01c4\u01c3\3\2\2\2\u01c5\u01c6\3\2\2\2")
        buf.write("\u01c6\u01c4\3\2\2\2\u01c6\u01c7\3\2\2\2\u01c7\u0344\3")
        buf.write("\2\2\2\u01c8\u01c9\7o\2\2\u01c9\u01ca\7R\2\2\u01ca\u01cb")
        buf.write("\5\u0086D\2\u01cb\u01cc\5 \21\2\u01cc\u01cd\7p\2\2\u01cd")
        buf.write("\u01ce\7l\2\2\u01ce\u01cf\5 \21\2\u01cf\u0344\3\2\2\2")
        buf.write("\u01d0\u01d1\7o\2\2\u01d1\u01d2\7R\2\2\u01d2\u01d3\5\u0086")
        buf.write("D\2\u01d3\u01d6\7g\2\2\u01d4\u01d5\7\u0081\2\2\u01d5\u01d7")
        buf.write("\7$\2\2\u01d6\u01d4\3\2\2\2\u01d6\u01d7\3\2\2\2\u01d7")
        buf.write("\u01d8\3\2\2\2\u01d8\u01dd\5 \21\2\u01d9\u01da\7\5\2\2")
        buf.write("\u01da\u01dc\5 \21\2\u01db\u01d9\3\2\2\2\u01dc\u01df\3")
        buf.write("\2\2\2\u01dd\u01db\3\2\2\2\u01dd\u01de\3\2\2\2\u01de\u01e1")
        buf.write("\3\2\2\2\u01df\u01dd\3\2\2\2\u01e0\u01e2\7\u00ce\2\2\u01e1")
        buf.write("\u01e0\3\2\2\2\u01e1\u01e2\3\2\2\2\u01e2\u0344\3\2\2\2")
        buf.write("\u01e3\u01e4\7o\2\2\u01e4\u01e5\7T\2\2\u01e5\u01e6\5\u0086")
        buf.write("D\2\u01e6\u01e9\7g\2\2\u01e7\u01e8\7\u0081\2\2\u01e8\u01ea")
        buf.write("\7$\2\2\u01e9\u01e7\3\2\2\2\u01e9\u01ea\3\2\2\2\u01ea")
        buf.write("\u01eb\3\2\2\2\u01eb\u01f0\5 \21\2\u01ec\u01ed\7\5\2\2")
        buf.write("\u01ed\u01ef\5 \21\2\u01ee\u01ec\3\2\2\2\u01ef\u01f2\3")
        buf.write("\2\2\2\u01f0\u01ee\3\2\2\2\u01f0\u01f1\3\2\2\2\u01f1\u0344")
        buf.write("\3\2\2\2\u01f2\u01f0\3\2\2\2\u01f3\u01f4\7o\2\2\u01f4")
        buf.write("\u01f5\7R\2\2\u01f5\u01f7\5\u0086D\2\u01f6\u01f8\5 \21")
        buf.write("\2\u01f7\u01f6\3\2\2\2\u01f7\u01f8\3\2\2\2\u01f8\u01f9")
        buf.write("\3\2\2\2\u01f9\u01fa\7u\2\2\u01fa\u01fb\5\30\r\2\u01fb")
        buf.write("\u0344\3\2\2\2\u01fc\u01fd\7o\2\2\u01fd\u01fe\7R\2\2\u01fe")
        buf.write("\u01ff\5\u0086D\2\u01ff\u0200\7\u00e2\2\2\u0200\u0201")
        buf.write("\7e\2\2\u0201\u0344\3\2\2\2\u0202\u0203\7g\2\2\u0203\u0206")
        buf.write("\7R\2\2\u0204\u0205\7\u0081\2\2\u0205\u0207\7$\2\2\u0206")
        buf.write("\u0204\3\2\2\2\u0206\u0207\3\2\2\2\u0207\u0208\3\2\2\2")
        buf.write("\u0208\u020a\5\u0086D\2\u0209\u020b\7\u00ce\2\2\u020a")
        buf.write("\u0209\3\2\2\2\u020a\u020b\3\2\2\2\u020b\u0344\3\2\2\2")
        buf.write("\u020c\u020d\7g\2\2\u020d\u0210\7T\2\2\u020e\u020f\7\u0081")
        buf.write("\2\2\u020f\u0211\7$\2\2\u0210\u020e\3\2\2\2\u0210\u0211")
        buf.write("\3\2\2\2\u0211\u0212\3\2\2\2\u0212\u0344\5\u0086D\2\u0213")
        buf.write("\u0216\7Q\2\2\u0214\u0215\7\37\2\2\u0215\u0217\7U\2\2")
        buf.write("\u0216\u0214\3\2\2\2\u0216\u0217\3\2\2\2\u0217\u021c\3")
        buf.write("\2\2\2\u0218\u021a\7\u00b7\2\2\u0219\u0218\3\2\2\2\u0219")
        buf.write("\u021a\3\2\2\2\u021a\u021b\3\2\2\2\u021b\u021d\7\u00b8")
        buf.write("\2\2\u021c\u0219\3\2\2\2\u021c\u021d\3\2\2\2\u021d\u021e")
        buf.write("\3\2\2\2\u021e\u0222\7T\2\2\u021f\u0220\7\u0081\2\2\u0220")
        buf.write("\u0221\7\"\2\2\u0221\u0223\7$\2\2\u0222\u021f\3\2\2\2")
        buf.write("\u0222\u0223\3\2\2\2\u0223\u0224\3\2\2\2\u0224\u0226\5")
        buf.write("\u0086D\2\u0225\u0227\5x=\2\u0226\u0225\3\2\2\2\u0226")
        buf.write("\u0227\3\2\2\2\u0227\u022a\3\2\2\2\u0228\u0229\7t\2\2")
        buf.write("\u0229\u022b\7\u00f2\2\2\u022a\u0228\3\2\2\2\u022a\u022b")
        buf.write("\3\2\2\2\u022b\u022f\3\2\2\2\u022c\u022d\7\u00d9\2\2\u022d")
        buf.write("\u022e\7?\2\2\u022e\u0230\5p9\2\u022f\u022c\3\2\2\2\u022f")
        buf.write("\u0230\3\2\2\2\u0230\u0233\3\2\2\2\u0231\u0232\7\u00bb")
        buf.write("\2\2\u0232\u0234\5.\30\2\u0233\u0231\3\2\2\2\u0233\u0234")
        buf.write("\3\2\2\2\u0234\u0235\3\2\2\2\u0235\u0236\7\20\2\2\u0236")
        buf.write("\u0237\5\32\16\2\u0237\u0344\3\2\2\2\u0238\u023b\7Q\2")
        buf.write("\2\u0239\u023a\7\37\2\2\u023a\u023c\7U\2\2\u023b\u0239")
        buf.write("\3\2\2\2\u023b\u023c\3\2\2\2\u023c\u023e\3\2\2\2\u023d")
        buf.write("\u023f\7\u00b7\2\2\u023e\u023d\3\2\2\2\u023e\u023f\3\2")
        buf.write("\2\2\u023f\u0240\3\2\2\2\u0240\u0241\7\u00b8\2\2\u0241")
        buf.write("\u0242\7T\2\2\u0242\u0247\5\u0086D\2\u0243\u0244\7\3\2")
        buf.write("\2\u0244\u0245\5\u00acW\2\u0245\u0246\7\4\2\2\u0246\u0248")
        buf.write("\3\2\2\2\u0247\u0243\3\2\2\2\u0247\u0248\3\2\2\2\u0248")
        buf.write("\u0249\3\2\2\2\u0249\u024c\5,\27\2\u024a\u024b\7\u00b9")
        buf.write("\2\2\u024b\u024d\5.\30\2\u024c\u024a\3\2\2\2\u024c\u024d")
        buf.write("\3\2\2\2\u024d\u0344\3\2\2\2\u024e\u024f\7o\2\2\u024f")
        buf.write("\u0250\7T\2\2\u0250\u0252\5\u0086D\2\u0251\u0253\7\20")
        buf.write("\2\2\u0252\u0251\3\2\2\2\u0252\u0253\3\2\2\2\u0253\u0254")
        buf.write("\3\2\2\2\u0254\u0255\5\32\16\2\u0255\u0344\3\2\2\2\u0256")
        buf.write("\u0259\7Q\2\2\u0257\u0258\7\37\2\2\u0258\u025a\7U\2\2")
        buf.write("\u0259\u0257\3\2\2\2\u0259\u025a\3\2\2\2\u025a\u025c\3")
        buf.write("\2\2\2\u025b\u025d\7\u00b8\2\2\u025c\u025b\3\2\2\2\u025c")
        buf.write("\u025d\3\2\2\2\u025d\u025e\3\2\2\2\u025e\u0262\7\u00af")
        buf.write("\2\2\u025f\u0260\7\u0081\2\2\u0260\u0261\7\"\2\2\u0261")
        buf.write("\u0263\7$\2\2\u0262\u025f\3\2\2\2\u0262\u0263\3\2\2\2")
        buf.write("\u0263\u0264\3\2\2\2\u0264\u0265\5\u00c0a\2\u0265\u0266")
        buf.write("\7\20\2\2\u0266\u0270\7\u00f2\2\2\u0267\u0268\7\u00a1")
        buf.write("\2\2\u0268\u026d\5@!\2\u0269\u026a\7\5\2\2\u026a\u026c")
        buf.write("\5@!\2\u026b\u0269\3\2\2\2\u026c\u026f\3\2\2\2\u026d\u026b")
        buf.write("\3\2\2\2\u026d\u026e\3\2\2\2\u026e\u0271\3\2\2\2\u026f")
        buf.write("\u026d\3\2\2\2\u0270\u0267\3\2\2\2\u0270\u0271\3\2\2\2")
        buf.write("\u0271\u0344\3\2\2\2\u0272\u0274\7g\2\2\u0273\u0275\7")
        buf.write("\u00b8\2\2\u0274\u0273\3\2\2\2\u0274\u0275\3\2\2\2\u0275")
        buf.write("\u0276\3\2\2\2\u0276\u0279\7\u00af\2\2\u0277\u0278\7\u0081")
        buf.write("\2\2\u0278\u027a\7$\2\2\u0279\u0277\3\2\2\2\u0279\u027a")
        buf.write("\3\2\2\2\u027a\u027b\3\2\2\2\u027b\u0344\5\u00c0a\2\u027c")
        buf.write("\u027e\7Z\2\2\u027d\u027f\t\4\2\2\u027e\u027d\3\2\2\2")
        buf.write("\u027e\u027f\3\2\2\2\u027f\u0280\3\2\2\2\u0280\u0344\5")
        buf.write("\16\b\2\u0281\u0282\7`\2\2\u0282\u0285\7a\2\2\u0283\u0284")
        buf.write("\t\5\2\2\u0284\u0286\5\u00c2b\2\u0285\u0283\3\2\2\2\u0285")
        buf.write("\u0286\3\2\2\2\u0286\u028b\3\2\2\2\u0287\u0289\7&\2\2")
        buf.write("\u0288\u0287\3\2\2\2\u0288\u0289\3\2\2\2\u0289\u028a\3")
        buf.write("\2\2\2\u028a\u028c\7\u00f2\2\2\u028b\u0288\3\2\2\2\u028b")
        buf.write("\u028c\3\2\2\2\u028c\u0344\3\2\2\2\u028d\u028e\7`\2\2")
        buf.write("\u028e\u028f\7R\2\2\u028f\u0292\7\u00b0\2\2\u0290\u0291")
        buf.write("\t\5\2\2\u0291\u0293\5\u00c2b\2\u0292\u0290\3\2\2\2\u0292")
        buf.write("\u0293\3\2\2\2\u0293\u0294\3\2\2\2\u0294\u0295\7&\2\2")
        buf.write("\u0295\u0297\7\u00f2\2\2\u0296\u0298\5 \21\2\u0297\u0296")
        buf.write("\3\2\2\2\u0297\u0298\3\2\2\2\u0298\u0344\3\2\2\2\u0299")
        buf.write("\u029a\7`\2\2\u029a\u029f\7\u00d2\2\2\u029b\u029d\7&\2")
        buf.write("\2\u029c\u029b\3\2\2\2\u029c\u029d\3\2\2\2\u029d\u029e")
        buf.write("\3\2\2\2\u029e\u02a0\7\u00f2\2\2\u029f\u029c\3\2\2\2\u029f")
        buf.write("\u02a0\3\2\2\2\u02a0\u0344\3\2\2\2\u02a1\u02a2\7`\2\2")
        buf.write("\u02a2\u02a3\7\u00bb\2\2\u02a3\u02a8\5\u0086D\2\u02a4")
        buf.write("\u02a5\7\3\2\2\u02a5\u02a6\5\62\32\2\u02a6\u02a7\7\4\2")
        buf.write("\2\u02a7\u02a9\3\2\2\2\u02a8\u02a4\3\2\2\2\u02a8\u02a9")
        buf.write("\3\2\2\2\u02a9\u0344\3\2\2\2\u02aa\u02ab\7`\2\2\u02ab")
        buf.write("\u02ac\7b\2\2\u02ac\u02ad\t\5\2\2\u02ad\u02b0\5\u0086")
        buf.write("D\2\u02ae\u02af\t\5\2\2\u02af\u02b1\5\u00c2b\2\u02b0\u02ae")
        buf.write("\3\2\2\2\u02b0\u02b1\3\2\2\2\u02b1\u0344\3\2\2\2\u02b2")
        buf.write("\u02b3\7`\2\2\u02b3\u02b4\7e\2\2\u02b4\u02b6\5\u0086D")
        buf.write("\2\u02b5\u02b7\5 \21\2\u02b6\u02b5\3\2\2\2\u02b6\u02b7")
        buf.write("\3\2\2\2\u02b7\u0344\3\2\2\2\u02b8\u02ba\7`\2\2\u02b9")
        buf.write("\u02bb\5\u00c2b\2\u02ba\u02b9\3\2\2\2\u02ba\u02bb\3\2")
        buf.write("\2\2\u02bb\u02bc\3\2\2\2\u02bc\u02c4\7f\2\2\u02bd\u02bf")
        buf.write("\7&\2\2\u02be\u02bd\3\2\2\2\u02be\u02bf\3\2\2\2\u02bf")
        buf.write("\u02c2\3\2\2\2\u02c0\u02c3\5\u00c0a\2\u02c1\u02c3\7\u00f2")
        buf.write("\2\2\u02c2\u02c0\3\2\2\2\u02c2\u02c1\3\2\2\2\u02c3\u02c5")
        buf.write("\3\2\2\2\u02c4\u02be\3\2\2\2\u02c4\u02c5\3\2\2\2\u02c5")
        buf.write("\u0344\3\2\2\2\u02c6\u02c7\7`\2\2\u02c7\u02c8\7Q\2\2\u02c8")
        buf.write("\u02c9\7R\2\2\u02c9\u0344\5\u0086D\2\u02ca\u02cb\t\6\2")
        buf.write("\2\u02cb\u02cd\7\u00af\2\2\u02cc\u02ce\7\u00b0\2\2\u02cd")
        buf.write("\u02cc\3\2\2\2\u02cd\u02ce\3\2\2\2\u02ce\u02cf\3\2\2\2")
        buf.write("\u02cf\u0344\5$\23\2\u02d0\u02d1\t\6\2\2\u02d1\u02d3\7")
        buf.write("\u00d1\2\2\u02d2\u02d4\7\u00b0\2\2\u02d3\u02d2\3\2\2\2")
        buf.write("\u02d3\u02d4\3\2\2\2\u02d4\u02d5\3\2\2\2\u02d5\u0344\5")
        buf.write("\u00c2b\2\u02d6\u02d8\t\6\2\2\u02d7\u02d9\7R\2\2\u02d8")
        buf.write("\u02d7\3\2\2\2\u02d8\u02d9\3\2\2\2\u02d9\u02db\3\2\2\2")
        buf.write("\u02da\u02dc\t\7\2\2\u02db\u02da\3\2\2\2\u02db\u02dc\3")
        buf.write("\2\2\2\u02dc\u02dd\3\2\2\2\u02dd\u02df\5\u0086D\2\u02de")
        buf.write("\u02e0\5 \21\2\u02df\u02de\3\2\2\2\u02df\u02e0\3\2\2\2")
        buf.write("\u02e0\u02e2\3\2\2\2\u02e1\u02e3\5&\24\2\u02e2\u02e1\3")
        buf.write("\2\2\2\u02e2\u02e3\3\2\2\2\u02e3\u0344\3\2\2\2\u02e4\u02e5")
        buf.write("\7\u00b1\2\2\u02e5\u02e6\7R\2\2\u02e6\u0344\5\u0086D\2")
        buf.write("\u02e7\u02ef\7\u00b1\2\2\u02e8\u02f0\7\u00f2\2\2\u02e9")
        buf.write("\u02eb\13\2\2\2\u02ea\u02e9\3\2\2\2\u02eb\u02ee\3\2\2")
        buf.write("\2\u02ec\u02ed\3\2\2\2\u02ec\u02ea\3\2\2\2\u02ed\u02f0")
        buf.write("\3\2\2\2\u02ee\u02ec\3\2\2\2\u02ef\u02e8\3\2\2\2\u02ef")
        buf.write("\u02ec\3\2\2\2\u02f0\u0344\3\2\2\2\u02f1\u02f3\7\u00b3")
        buf.write("\2\2\u02f2\u02f4\7\u00b5\2\2\u02f3\u02f2\3\2\2\2\u02f3")
        buf.write("\u02f4\3\2\2\2\u02f4\u02f5\3\2\2\2\u02f5\u02f6\7R\2\2")
        buf.write("\u02f6\u02f9\5\u0086D\2\u02f7\u02f8\7\u00b9\2\2\u02f8")
        buf.write("\u02fa\5.\30\2\u02f9\u02f7\3\2\2\2\u02f9\u02fa\3\2\2\2")
        buf.write("\u02fa\u02ff\3\2\2\2\u02fb\u02fd\7\20\2\2\u02fc\u02fb")
        buf.write("\3\2\2\2\u02fc\u02fd\3\2\2\2\u02fd\u02fe\3\2\2\2\u02fe")
        buf.write("\u0300\5\32\16\2\u02ff\u02fc\3\2\2\2\u02ff\u0300\3\2\2")
        buf.write("\2\u0300\u0344\3\2\2\2\u0301\u0302\7\u00b4\2\2\u0302\u0305")
        buf.write("\7R\2\2\u0303\u0304\7\u0081\2\2\u0304\u0306\7$\2\2\u0305")
        buf.write("\u0303\3\2\2\2\u0305\u0306\3\2\2\2\u0306\u0307\3\2\2\2")
        buf.write("\u0307\u0344\5\u0086D\2\u0308\u0309\7\u00b2\2\2\u0309")
        buf.write("\u0344\7\u00b3\2\2\u030a\u030b\7\u00e5\2\2\u030b\u030d")
        buf.write("\7w\2\2\u030c\u030e\7\u00f0\2\2\u030d\u030c\3\2\2\2\u030d")
        buf.write("\u030e\3\2\2\2\u030e\u030f\3\2\2\2\u030f\u0310\7\u00f1")
        buf.write("\2\2\u0310\u0312\7\u00f2\2\2\u0311\u0313\7\u009e\2\2\u0312")
        buf.write("\u0311\3\2\2\2\u0312\u0313\3\2\2\2\u0313\u0314\3\2\2\2")
        buf.write("\u0314\u0315\7X\2\2\u0315\u0316\7R\2\2\u0316\u0318\5\u0086")
        buf.write("D\2\u0317\u0319\5 \21\2\u0318\u0317\3\2\2\2\u0318\u0319")
        buf.write("\3\2\2\2\u0319\u0344\3\2\2\2\u031a\u031b\7\u00d4\2\2\u031b")
        buf.write("\u031c\7R\2\2\u031c\u031e\5\u0086D\2\u031d\u031f\5 \21")
        buf.write("\2\u031e\u031d\3\2\2\2\u031e\u031f\3\2\2\2\u031f\u0344")
        buf.write("\3\2\2\2\u0320\u0321\7\u00e0\2\2\u0321\u0322\7\u00e1\2")
        buf.write("\2\u0322\u0323\7R\2\2\u0323\u0344\5\u0086D\2\u0324\u0325")
        buf.write("\t\b\2\2\u0325\u0329\5\u00c2b\2\u0326\u0328\13\2\2\2\u0327")
        buf.write("\u0326\3\2\2\2\u0328\u032b\3\2\2\2\u0329\u032a\3\2\2\2")
        buf.write("\u0329\u0327\3\2\2\2\u032a\u0344\3\2\2\2\u032b\u0329\3")
        buf.write("\2\2\2\u032c\u032d\7u\2\2\u032d\u0331\7\u00e6\2\2\u032e")
        buf.write("\u0330\13\2\2\2\u032f\u032e\3\2\2\2\u0330\u0333\3\2\2")
        buf.write("\2\u0331\u0332\3\2\2\2\u0331\u032f\3\2\2\2\u0332\u0344")
        buf.write("\3\2\2\2\u0333\u0331\3\2\2\2\u0334\u0338\7u\2\2\u0335")
        buf.write("\u0337\13\2\2\2\u0336\u0335\3\2\2\2\u0337\u033a\3\2\2")
        buf.write("\2\u0338\u0339\3\2\2\2\u0338\u0336\3\2\2\2\u0339\u0344")
        buf.write("\3\2\2\2\u033a\u0338\3\2\2\2\u033b\u0344\7v\2\2\u033c")
        buf.write("\u0340\5\20\t\2\u033d\u033f\13\2\2\2\u033e\u033d\3\2\2")
        buf.write("\2\u033f\u0342\3\2\2\2\u0340\u0341\3\2\2\2\u0340\u033e")
        buf.write("\3\2\2\2\u0341\u0344\3\2\2\2\u0342\u0340\3\2\2\2\u0343")
        buf.write("\u00de\3\2\2\2\u0343\u00df\3\2\2\2\u0343\u00e1\3\2\2\2")
        buf.write("\u0343\u00f5\3\2\2\2\u0343\u00fc\3\2\2\2\u0343\u0106\3")
        buf.write("\2\2\2\u0343\u0124\3\2\2\2\u0343\u0145\3\2\2\2\u0343\u0152")
        buf.write("\3\2\2\2\u0343\u0163\3\2\2\2\u0343\u016c\3\2\2\2\u0343")
        buf.write("\u0173\3\2\2\2\u0343\u017a\3\2\2\2\u0343\u0185\3\2\2\2")
        buf.write("\u0343\u0194\3\2\2\2\u0343\u01a2\3\2\2\2\u0343\u01ac\3")
        buf.write("\2\2\2\u0343\u01ba\3\2\2\2\u0343\u01c8\3\2\2\2\u0343\u01d0")
        buf.write("\3\2\2\2\u0343\u01e3\3\2\2\2\u0343\u01f3\3\2\2\2\u0343")
        buf.write("\u01fc\3\2\2\2\u0343\u0202\3\2\2\2\u0343\u020c\3\2\2\2")
        buf.write("\u0343\u0213\3\2\2\2\u0343\u0238\3\2\2\2\u0343\u024e\3")
        buf.write("\2\2\2\u0343\u0256\3\2\2\2\u0343\u0272\3\2\2\2\u0343\u027c")
        buf.write("\3\2\2\2\u0343\u0281\3\2\2\2\u0343\u028d\3\2\2\2\u0343")
        buf.write("\u0299\3\2\2\2\u0343\u02a1\3\2\2\2\u0343\u02aa\3\2\2\2")
        buf.write("\u0343\u02b2\3\2\2\2\u0343\u02b8\3\2\2\2\u0343\u02c6\3")
        buf.write("\2\2\2\u0343\u02ca\3\2\2\2\u0343\u02d0\3\2\2\2\u0343\u02d6")
        buf.write("\3\2\2\2\u0343\u02e4\3\2\2\2\u0343\u02e7\3\2\2\2\u0343")
        buf.write("\u02f1\3\2\2\2\u0343\u0301\3\2\2\2\u0343\u0308\3\2\2\2")
        buf.write("\u0343\u030a\3\2\2\2\u0343\u031a\3\2\2\2\u0343\u0320\3")
        buf.write("\2\2\2\u0343\u0324\3\2\2\2\u0343\u032c\3\2\2\2\u0343\u0334")
        buf.write("\3\2\2\2\u0343\u033b\3\2\2\2\u0343\u033c\3\2\2\2\u0344")
        buf.write("\17\3\2\2\2\u0345\u0346\7Q\2\2\u0346\u03f0\7\u00e6\2\2")
        buf.write("\u0347\u0348\7g\2\2\u0348\u03f0\7\u00e6\2\2\u0349\u034b")
        buf.write("\7\u00dd\2\2\u034a\u034c\7\u00e6\2\2\u034b\u034a\3\2\2")
        buf.write("\2\u034b\u034c\3\2\2\2\u034c\u03f0\3\2\2\2\u034d\u034f")
        buf.write("\7\u00dc\2\2\u034e\u0350\7\u00e6\2\2\u034f\u034e\3\2\2")
        buf.write("\2\u034f\u0350\3\2\2\2\u0350\u03f0\3\2\2\2\u0351\u0352")
        buf.write("\7`\2\2\u0352\u03f0\7\u00dd\2\2\u0353\u0354\7`\2\2\u0354")
        buf.write("\u0356\7\u00e6\2\2\u0355\u0357\7\u00dd\2\2\u0356\u0355")
        buf.write("\3\2\2\2\u0356\u0357\3\2\2\2\u0357\u03f0\3\2\2\2\u0358")
        buf.write("\u0359\7`\2\2\u0359\u03f0\7\u00e9\2\2\u035a\u035b\7`\2")
        buf.write("\2\u035b\u03f0\7\u00e7\2\2\u035c\u035d\7`\2\2\u035d\u035e")
        buf.write("\7J\2\2\u035e\u03f0\7\u00e7\2\2\u035f\u0360\7\u00e3\2")
        buf.write("\2\u0360\u03f0\7R\2\2\u0361\u0362\7\u00e4\2\2\u0362\u03f0")
        buf.write("\7R\2\2\u0363\u0364\7`\2\2\u0364\u03f0\7\u00e8\2\2\u0365")
        buf.write("\u0366\7`\2\2\u0366\u0367\7Q\2\2\u0367\u03f0\7R\2\2\u0368")
        buf.write("\u0369\7`\2\2\u0369\u03f0\7\u00ea\2\2\u036a\u036b\7`\2")
        buf.write("\2\u036b\u03f0\7\u00ec\2\2\u036c\u036d\7`\2\2\u036d\u03f0")
        buf.write("\7\u00ed\2\2\u036e\u036f\7Q\2\2\u036f\u03f0\7\u00eb\2")
        buf.write("\2\u0370\u0371\7g\2\2\u0371\u03f0\7\u00eb\2\2\u0372\u0373")
        buf.write("\7o\2\2\u0373\u03f0\7\u00eb\2\2\u0374\u0375\7\u00de\2")
        buf.write("\2\u0375\u03f0\7R\2\2\u0376\u0377\7\u00de\2\2\u0377\u03f0")
        buf.write("\7\u00d1\2\2\u0378\u0379\7\u00df\2\2\u0379\u03f0\7R\2")
        buf.write("\2\u037a\u037b\7\u00df\2\2\u037b\u03f0\7\u00d1\2\2\u037c")
        buf.write("\u037d\7Q\2\2\u037d\u037e\7\u00b8\2\2\u037e\u03f0\7|\2")
        buf.write("\2\u037f\u0380\7g\2\2\u0380\u0381\7\u00b8\2\2\u0381\u03f0")
        buf.write("\7|\2\2\u0382\u0383\7o\2\2\u0383\u0384\7R\2\2\u0384\u0385")
        buf.write("\5\u0086D\2\u0385\u0386\7\"\2\2\u0386\u0387\7\u00cc\2")
        buf.write("\2\u0387\u03f0\3\2\2\2\u0388\u0389\7o\2\2\u0389\u038a")
        buf.write("\7R\2\2\u038a\u038b\5\u0086D\2\u038b\u038c\7\u00cc\2\2")
        buf.write("\u038c\u038d\7\26\2\2\u038d\u03f0\3\2\2\2\u038e\u038f")
        buf.write("\7o\2\2\u038f\u0390\7R\2\2\u0390\u0391\5\u0086D\2\u0391")
        buf.write("\u0392\7\"\2\2\u0392\u0393\7\u00cd\2\2\u0393\u03f0\3\2")
        buf.write("\2\2\u0394\u0395\7o\2\2\u0395\u0396\7R\2\2\u0396\u0397")
        buf.write("\5\u0086D\2\u0397\u0398\7\u00be\2\2\u0398\u0399\7\26\2")
        buf.write("\2\u0399\u03f0\3\2\2\2\u039a\u039b\7o\2\2\u039b\u039c")
        buf.write("\7R\2\2\u039c\u039d\5\u0086D\2\u039d\u039e\7\"\2\2\u039e")
        buf.write("\u039f\7\u00be\2\2\u039f\u03f0\3\2\2\2\u03a0\u03a1\7o")
        buf.write("\2\2\u03a1\u03a2\7R\2\2\u03a2\u03a3\5\u0086D\2\u03a3\u03a4")
        buf.write("\7\"\2\2\u03a4\u03a5\7\u00bf\2\2\u03a5\u03a6\7\20\2\2")
        buf.write("\u03a6\u03a7\7\u00c0\2\2\u03a7\u03f0\3\2\2\2\u03a8\u03a9")
        buf.write("\7o\2\2\u03a9\u03aa\7R\2\2\u03aa\u03ab\5\u0086D\2\u03ab")
        buf.write("\u03ac\7u\2\2\u03ac\u03ad\7\u00be\2\2\u03ad\u03ae\7\u00c1")
        buf.write("\2\2\u03ae\u03f0\3\2\2\2\u03af\u03b0\7o\2\2\u03b0\u03b1")
        buf.write("\7R\2\2\u03b1\u03b2\5\u0086D\2\u03b2\u03b3\7\u00c2\2\2")
        buf.write("\u03b3\u03b4\7D\2\2\u03b4\u03f0\3\2\2\2\u03b5\u03b6\7")
        buf.write("o\2\2\u03b6\u03b7\7R\2\2\u03b7\u03b8\5\u0086D\2\u03b8")
        buf.write("\u03b9\7\u00c3\2\2\u03b9\u03ba\7D\2\2\u03ba\u03f0\3\2")
        buf.write("\2\2\u03bb\u03bc\7o\2\2\u03bc\u03bd\7R\2\2\u03bd\u03be")
        buf.write("\5\u0086D\2\u03be\u03bf\7\u00c4\2\2\u03bf\u03c0\7D\2\2")
        buf.write("\u03c0\u03f0\3\2\2\2\u03c1\u03c2\7o\2\2\u03c2\u03c3\7")
        buf.write("R\2\2\u03c3\u03c4\5\u0086D\2\u03c4\u03c5\7\u00c6\2\2\u03c5")
        buf.write("\u03f0\3\2\2\2\u03c6\u03c7\7o\2\2\u03c7\u03c8\7R\2\2\u03c8")
        buf.write("\u03ca\5\u0086D\2\u03c9\u03cb\5 \21\2\u03ca\u03c9\3\2")
        buf.write("\2\2\u03ca\u03cb\3\2\2\2\u03cb\u03cc\3\2\2\2\u03cc\u03cd")
        buf.write("\7\u00c7\2\2\u03cd\u03f0\3\2\2\2\u03ce\u03cf\7o\2\2\u03cf")
        buf.write("\u03d0\7R\2\2\u03d0\u03d2\5\u0086D\2\u03d1\u03d3\5 \21")
        buf.write("\2\u03d2\u03d1\3\2\2\2\u03d2\u03d3\3\2\2\2\u03d3\u03d4")
        buf.write("\3\2\2\2\u03d4\u03d5\7\u00c8\2\2\u03d5\u03f0\3\2\2\2\u03d6")
        buf.write("\u03d7\7o\2\2\u03d7\u03d8\7R\2\2\u03d8\u03da\5\u0086D")
        buf.write("\2\u03d9\u03db\5 \21\2\u03da\u03d9\3\2\2\2\u03da\u03db")
        buf.write("\3\2\2\2\u03db\u03dc\3\2\2\2\u03dc\u03dd\7u\2\2\u03dd")
        buf.write("\u03de\7\u00c5\2\2\u03de\u03f0\3\2\2\2\u03df\u03e0\7o")
        buf.write("\2\2\u03e0\u03e1\7R\2\2\u03e1\u03e3\5\u0086D\2\u03e2\u03e4")
        buf.write("\5 \21\2\u03e3\u03e2\3\2\2\2\u03e3\u03e4\3\2\2\2\u03e4")
        buf.write("\u03e5\3\2\2\2\u03e5\u03e6\7U\2\2\u03e6\u03e7\7b\2\2\u03e7")
        buf.write("\u03f0\3\2\2\2\u03e8\u03e9\7x\2\2\u03e9\u03f0\7y\2\2\u03ea")
        buf.write("\u03f0\7z\2\2\u03eb\u03f0\7{\2\2\u03ec\u03f0\7\u00d3\2")
        buf.write("\2\u03ed\u03ee\7W\2\2\u03ee\u03f0\7\16\2\2\u03ef\u0345")
        buf.write("\3\2\2\2\u03ef\u0347\3\2\2\2\u03ef\u0349\3\2\2\2\u03ef")
        buf.write("\u034d\3\2\2\2\u03ef\u0351\3\2\2\2\u03ef\u0353\3\2\2\2")
        buf.write("\u03ef\u0358\3\2\2\2\u03ef\u035a\3\2\2\2\u03ef\u035c\3")
        buf.write("\2\2\2\u03ef\u035f\3\2\2\2\u03ef\u0361\3\2\2\2\u03ef\u0363")
        buf.write("\3\2\2\2\u03ef\u0365\3\2\2\2\u03ef\u0368\3\2\2\2\u03ef")
        buf.write("\u036a\3\2\2\2\u03ef\u036c\3\2\2\2\u03ef\u036e\3\2\2\2")
        buf.write("\u03ef\u0370\3\2\2\2\u03ef\u0372\3\2\2\2\u03ef\u0374\3")
        buf.write("\2\2\2\u03ef\u0376\3\2\2\2\u03ef\u0378\3\2\2\2\u03ef\u037a")
        buf.write("\3\2\2\2\u03ef\u037c\3\2\2\2\u03ef\u037f\3\2\2\2\u03ef")
        buf.write("\u0382\3\2\2\2\u03ef\u0388\3\2\2\2\u03ef\u038e\3\2\2\2")
        buf.write("\u03ef\u0394\3\2\2\2\u03ef\u039a\3\2\2\2\u03ef\u03a0\3")
        buf.write("\2\2\2\u03ef\u03a8\3\2\2\2\u03ef\u03af\3\2\2\2\u03ef\u03b5")
        buf.write("\3\2\2\2\u03ef\u03bb\3\2\2\2\u03ef\u03c1\3\2\2\2\u03ef")
        buf.write("\u03c6\3\2\2\2\u03ef\u03ce\3\2\2\2\u03ef\u03d6\3\2\2\2")
        buf.write("\u03ef\u03df\3\2\2\2\u03ef\u03e8\3\2\2\2\u03ef\u03ea\3")
        buf.write("\2\2\2\u03ef\u03eb\3\2\2\2\u03ef\u03ec\3\2\2\2\u03ef\u03ed")
        buf.write("\3\2\2\2\u03f0\21\3\2\2\2\u03f1\u03f3\7Q\2\2\u03f2\u03f4")
        buf.write("\7\u00b8\2\2\u03f3\u03f2\3\2\2\2\u03f3\u03f4\3\2\2\2\u03f4")
        buf.write("\u03f6\3\2\2\2\u03f5\u03f7\7\u00da\2\2\u03f6\u03f5\3\2")
        buf.write("\2\2\u03f6\u03f7\3\2\2\2\u03f7\u03f8\3\2\2\2\u03f8\u03fc")
        buf.write("\7R\2\2\u03f9\u03fa\7\u0081\2\2\u03fa\u03fb\7\"\2\2\u03fb")
        buf.write("\u03fd\7$\2\2\u03fc\u03f9\3\2\2\2\u03fc\u03fd\3\2\2\2")
        buf.write("\u03fd\u03fe\3\2\2\2\u03fe\u03ff\5\u0086D\2\u03ff\23\3")
        buf.write("\2\2\2\u0400\u0401\7\u00cc\2\2\u0401\u0402\7\26\2\2\u0402")
        buf.write("\u0406\5p9\2\u0403\u0404\7\u00cd\2\2\u0404\u0405\7\26")
        buf.write("\2\2\u0405\u0407\5t;\2\u0406\u0403\3\2\2\2\u0406\u0407")
        buf.write("\3\2\2\2\u0407\u0408\3\2\2\2\u0408\u0409\7X\2\2\u0409")
        buf.write("\u040a\7\u00f6\2\2\u040a\u040b\7\u00bd\2\2\u040b\25\3")
        buf.write("\2\2\2\u040c\u040d\7\u00be\2\2\u040d\u040e\7\26\2\2\u040e")
        buf.write("\u040f\5p9\2\u040f\u0412\7?\2\2\u0410\u0413\5\66\34\2")
        buf.write("\u0411\u0413\58\35\2\u0412\u0410\3\2\2\2\u0412\u0411\3")
        buf.write("\2\2\2\u0413\u0417\3\2\2\2\u0414\u0415\7\u00bf\2\2\u0415")
        buf.write("\u0416\7\20\2\2\u0416\u0418\7\u00c0\2\2\u0417\u0414\3")
        buf.write("\2\2\2\u0417\u0418\3\2\2\2\u0418\27\3\2\2\2\u0419\u041a")
        buf.write("\7\u00c1\2\2\u041a\u041b\7\u00f2\2\2\u041b\31\3\2\2\2")
        buf.write("\u041c\u041e\5(\25\2\u041d\u041c\3\2\2\2\u041d\u041e\3")
        buf.write("\2\2\2\u041e\u041f\3\2\2\2\u041f\u0420\5B\"\2\u0420\33")
        buf.write("\3\2\2\2\u0421\u0422\7V\2\2\u0422\u0423\7\u009e\2\2\u0423")
        buf.write("\u0424\7R\2\2\u0424\u042b\5\u0086D\2\u0425\u0429\5 \21")
        buf.write("\2\u0426\u0427\7\u0081\2\2\u0427\u0428\7\"\2\2\u0428\u042a")
        buf.write("\7$\2\2\u0429\u0426\3\2\2\2\u0429\u042a\3\2\2\2\u042a")
        buf.write("\u042c\3\2\2\2\u042b\u0425\3\2\2\2\u042b\u042c\3\2\2\2")
        buf.write("\u042c\u0452\3\2\2\2\u042d\u042e\7V\2\2\u042e\u0430\7")
        buf.write("X\2\2\u042f\u0431\7R\2\2\u0430\u042f\3\2\2\2\u0430\u0431")
        buf.write("\3\2\2\2\u0431\u0432\3\2\2\2\u0432\u0434\5\u0086D\2\u0433")
        buf.write("\u0435\5 \21\2\u0434\u0433\3\2\2\2\u0434\u0435\3\2\2\2")
        buf.write("\u0435\u0452\3\2\2\2\u0436\u0437\7V\2\2\u0437\u0439\7")
        buf.write("\u009e\2\2\u0438\u043a\7\u00f0\2\2\u0439\u0438\3\2\2\2")
        buf.write("\u0439\u043a\3\2\2\2\u043a\u043b\3\2\2\2\u043b\u043c\7")
        buf.write("S\2\2\u043c\u043e\7\u00f2\2\2\u043d\u043f\5\u0084C\2\u043e")
        buf.write("\u043d\3\2\2\2\u043e\u043f\3\2\2\2\u043f\u0441\3\2\2\2")
        buf.write("\u0440\u0442\5:\36\2\u0441\u0440\3\2\2\2\u0441\u0442\3")
        buf.write("\2\2\2\u0442\u0452\3\2\2\2\u0443\u0444\7V\2\2\u0444\u0446")
        buf.write("\7\u009e\2\2\u0445\u0447\7\u00f0\2\2\u0446\u0445\3\2\2")
        buf.write("\2\u0446\u0447\3\2\2\2\u0447\u0448\3\2\2\2\u0448\u044a")
        buf.write("\7S\2\2\u0449\u044b\7\u00f2\2\2\u044a\u0449\3\2\2\2\u044a")
        buf.write("\u044b\3\2\2\2\u044b\u044c\3\2\2\2\u044c\u044f\5,\27\2")
        buf.write("\u044d\u044e\7\u00b9\2\2\u044e\u0450\5.\30\2\u044f\u044d")
        buf.write("\3\2\2\2\u044f\u0450\3\2\2\2\u0450\u0452\3\2\2\2\u0451")
        buf.write("\u0421\3\2\2\2\u0451\u042d\3\2\2\2\u0451\u0436\3\2\2\2")
        buf.write("\u0451\u0443\3\2\2\2\u0452\35\3\2\2\2\u0453\u0455\5 \21")
        buf.write("\2\u0454\u0456\5\30\r\2\u0455\u0454\3\2\2\2\u0455\u0456")
        buf.write("\3\2\2\2\u0456\37\3\2\2\2\u0457\u0458\7D\2\2\u0458\u0459")
        buf.write("\7\3\2\2\u0459\u045e\5\"\22\2\u045a\u045b\7\5\2\2\u045b")
        buf.write("\u045d\5\"\22\2\u045c\u045a\3\2\2\2\u045d\u0460\3\2\2")
        buf.write("\2\u045e\u045c\3\2\2\2\u045e\u045f\3\2\2\2\u045f\u0461")
        buf.write("\3\2\2\2\u0460\u045e\3\2\2\2\u0461\u0462\7\4\2\2\u0462")
        buf.write("!\3\2\2\2\u0463\u0466\5\u00c2b\2\u0464\u0465\7\u0084\2")
        buf.write("\2\u0465\u0467\5\u0098M\2\u0466\u0464\3\2\2\2\u0466\u0467")
        buf.write("\3\2\2\2\u0467#\3\2\2\2\u0468\u046e\5\u00c0a\2\u0469\u046e")
        buf.write("\7\u00f2\2\2\u046a\u046e\5\u009aN\2\u046b\u046e\5\u009c")
        buf.write("O\2\u046c\u046e\5\u009eP\2\u046d\u0468\3\2\2\2\u046d\u0469")
        buf.write("\3\2\2\2\u046d\u046a\3\2\2\2\u046d\u046b\3\2\2\2\u046d")
        buf.write("\u046c\3\2\2\2\u046e%\3\2\2\2\u046f\u0474\5\u00c2b\2\u0470")
        buf.write("\u0471\7\6\2\2\u0471\u0473\5\u00c2b\2\u0472\u0470\3\2")
        buf.write("\2\2\u0473\u0476\3\2\2\2\u0474\u0472\3\2\2\2\u0474\u0475")
        buf.write("\3\2\2\2\u0475\'\3\2\2\2\u0476\u0474\3\2\2\2\u0477\u0478")
        buf.write("\7O\2\2\u0478\u047d\5*\26\2\u0479\u047a\7\5\2\2\u047a")
        buf.write("\u047c\5*\26\2\u047b\u0479\3\2\2\2\u047c\u047f\3\2\2\2")
        buf.write("\u047d\u047b\3\2\2\2\u047d\u047e\3\2\2\2\u047e)\3\2\2")
        buf.write("\2\u047f\u047d\3\2\2\2\u0480\u0482\5\u00c2b\2\u0481\u0483")
        buf.write("\7\20\2\2\u0482\u0481\3\2\2\2\u0482\u0483\3\2\2\2\u0483")
        buf.write("\u0484\3\2\2\2\u0484\u0485\7\3\2\2\u0485\u0486\5\32\16")
        buf.write("\2\u0486\u0487\7\4\2\2\u0487+\3\2\2\2\u0488\u0489\7\u00a1")
        buf.write("\2\2\u0489\u048a\5\u00c0a\2\u048a-\3\2\2\2\u048b\u048c")
        buf.write("\7\3\2\2\u048c\u0491\5\60\31\2\u048d\u048e\7\5\2\2\u048e")
        buf.write("\u0490\5\60\31\2\u048f\u048d\3\2\2\2\u0490\u0493\3\2\2")
        buf.write("\2\u0491\u048f\3\2\2\2\u0491\u0492\3\2\2\2\u0492\u0494")
        buf.write("\3\2\2\2\u0493\u0491\3\2\2\2\u0494\u0495\7\4\2\2\u0495")
        buf.write("/\3\2\2\2\u0496\u049b\5\62\32\2\u0497\u0499\7\u0084\2")
        buf.write("\2\u0498\u0497\3\2\2\2\u0498\u0499\3\2\2\2\u0499\u049a")
        buf.write("\3\2\2\2\u049a\u049c\5\64\33\2\u049b\u0498\3\2\2\2\u049b")
        buf.write("\u049c\3\2\2\2\u049c\61\3\2\2\2\u049d\u04a2\5\u00c2b\2")
        buf.write("\u049e\u049f\7\6\2\2\u049f\u04a1\5\u00c2b\2\u04a0\u049e")
        buf.write("\3\2\2\2\u04a1\u04a4\3\2\2\2\u04a2\u04a0\3\2\2\2\u04a2")
        buf.write("\u04a3\3\2\2\2\u04a3\u04a7\3\2\2\2\u04a4\u04a2\3\2\2\2")
        buf.write("\u04a5\u04a7\7\u00f2\2\2\u04a6\u049d\3\2\2\2\u04a6\u04a5")
        buf.write("\3\2\2\2\u04a7\63\3\2\2\2\u04a8\u04ad\7\u00f6\2\2\u04a9")
        buf.write("\u04ad\7\u00f7\2\2\u04aa\u04ad\5\u00a0Q\2\u04ab\u04ad")
        buf.write("\7\u00f2\2\2\u04ac\u04a8\3\2\2\2\u04ac\u04a9\3\2\2\2\u04ac")
        buf.write("\u04aa\3\2\2\2\u04ac\u04ab\3\2\2\2\u04ad\65\3\2\2\2\u04ae")
        buf.write("\u04af\7\3\2\2\u04af\u04b4\5\u0098M\2\u04b0\u04b1\7\5")
        buf.write("\2\2\u04b1\u04b3\5\u0098M\2\u04b2\u04b0\3\2\2\2\u04b3")
        buf.write("\u04b6\3\2\2\2\u04b4\u04b2\3\2\2\2\u04b4\u04b5\3\2\2\2")
        buf.write("\u04b5\u04b7\3\2\2\2\u04b6\u04b4\3\2\2\2\u04b7\u04b8\7")
        buf.write("\4\2\2\u04b8\67\3\2\2\2\u04b9\u04ba\7\3\2\2\u04ba\u04bf")
        buf.write("\5\66\34\2\u04bb\u04bc\7\5\2\2\u04bc\u04be\5\66\34\2\u04bd")
        buf.write("\u04bb\3\2\2\2\u04be\u04c1\3\2\2\2\u04bf\u04bd\3\2\2\2")
        buf.write("\u04bf\u04c0\3\2\2\2\u04c0\u04c2\3\2\2\2\u04c1\u04bf\3")
        buf.write("\2\2\2\u04c2\u04c3\7\4\2\2\u04c39\3\2\2\2\u04c4\u04c5")
        buf.write("\7\u00bf\2\2\u04c5\u04c6\7\20\2\2\u04c6\u04cb\5<\37\2")
        buf.write("\u04c7\u04c8\7\u00bf\2\2\u04c8\u04c9\7\26\2\2\u04c9\u04cb")
        buf.write("\5> \2\u04ca\u04c4\3\2\2\2\u04ca\u04c7\3\2\2\2\u04cb;")
        buf.write("\3\2\2\2\u04cc\u04cd\7\u00cf\2\2\u04cd\u04ce\7\u00f2\2")
        buf.write("\2\u04ce\u04cf\7\u00d0\2\2\u04cf\u04d2\7\u00f2\2\2\u04d0")
        buf.write("\u04d2\5\u00c2b\2\u04d1\u04cc\3\2\2\2\u04d1\u04d0\3\2")
        buf.write("\2\2\u04d2=\3\2\2\2\u04d3\u04d7\7\u00f2\2\2\u04d4\u04d5")
        buf.write("\7O\2\2\u04d5\u04d6\7\u00a3\2\2\u04d6\u04d8\5.\30\2\u04d7")
        buf.write("\u04d4\3\2\2\2\u04d7\u04d8\3\2\2\2\u04d8?\3\2\2\2\u04d9")
        buf.write("\u04da\5\u00c2b\2\u04da\u04db\7\u00f2\2\2\u04dbA\3\2\2")
        buf.write("\2\u04dc\u04de\5\34\17\2\u04dd\u04dc\3\2\2\2\u04dd\u04de")
        buf.write("\3\2\2\2\u04de\u04df\3\2\2\2\u04df\u04e0\5H%\2\u04e0\u04e1")
        buf.write("\5D#\2\u04e1\u04e9\3\2\2\2\u04e2\u04e4\5T+\2\u04e3\u04e5")
        buf.write("\5F$\2\u04e4\u04e3\3\2\2\2\u04e5\u04e6\3\2\2\2\u04e6\u04e4")
        buf.write("\3\2\2\2\u04e6\u04e7\3\2\2\2\u04e7\u04e9\3\2\2\2\u04e8")
        buf.write("\u04dd\3\2\2\2\u04e8\u04e2\3\2\2\2\u04e9C\3\2\2\2\u04ea")
        buf.write("\u04eb\7\33\2\2\u04eb\u04ec\7\26\2\2\u04ec\u04f1\5L\'")
        buf.write("\2\u04ed\u04ee\7\5\2\2\u04ee\u04f0\5L\'\2\u04ef\u04ed")
        buf.write("\3\2\2\2\u04f0\u04f3\3\2\2\2\u04f1\u04ef\3\2\2\2\u04f1")
        buf.write("\u04f2\3\2\2\2\u04f2\u04f5\3\2\2\2\u04f3\u04f1\3\2\2\2")
        buf.write("\u04f4\u04ea\3\2\2\2\u04f4\u04f5\3\2\2\2\u04f5\u0500\3")
        buf.write("\2\2\2\u04f6\u04f7\7\u009c\2\2\u04f7\u04f8\7\26\2\2\u04f8")
        buf.write("\u04fd\5\u008eH\2\u04f9\u04fa\7\5\2\2\u04fa\u04fc\5\u008e")
        buf.write("H\2\u04fb\u04f9\3\2\2\2\u04fc\u04ff\3\2\2\2\u04fd\u04fb")
        buf.write("\3\2\2\2\u04fd\u04fe\3\2\2\2\u04fe\u0501\3\2\2\2\u04ff")
        buf.write("\u04fd\3\2\2\2\u0500\u04f6\3\2\2\2\u0500\u0501\3\2\2\2")
        buf.write("\u0501\u050c\3\2\2\2\u0502\u0503\7\u009d\2\2\u0503\u0504")
        buf.write("\7\26\2\2\u0504\u0509\5\u008eH\2\u0505\u0506\7\5\2\2\u0506")
        buf.write("\u0508\5\u008eH\2\u0507\u0505\3\2\2\2\u0508\u050b\3\2")
        buf.write("\2\2\u0509\u0507\3\2\2\2\u0509\u050a\3\2\2\2\u050a\u050d")
        buf.write("\3\2\2\2\u050b\u0509\3\2\2\2\u050c\u0502\3\2\2\2\u050c")
        buf.write("\u050d\3\2\2\2\u050d\u0518\3\2\2\2\u050e\u050f\7\u009b")
        buf.write("\2\2\u050f\u0510\7\26\2\2\u0510\u0515\5L\'\2\u0511\u0512")
        buf.write("\7\5\2\2\u0512\u0514\5L\'\2\u0513\u0511\3\2\2\2\u0514")
        buf.write("\u0517\3\2\2\2\u0515\u0513\3\2\2\2\u0515\u0516\3\2\2\2")
        buf.write("\u0516\u0519\3\2\2\2\u0517\u0515\3\2\2\2\u0518\u050e\3")
        buf.write("\2\2\2\u0518\u0519\3\2\2\2\u0519\u051b\3\2\2\2\u051a\u051c")
        buf.write("\5\u00b6\\\2\u051b\u051a\3\2\2\2\u051b\u051c\3\2\2\2\u051c")
        buf.write("\u0522\3\2\2\2\u051d\u0520\7\35\2\2\u051e\u0521\7\21\2")
        buf.write("\2\u051f\u0521\5\u008eH\2\u0520\u051e\3\2\2\2\u0520\u051f")
        buf.write("\3\2\2\2\u0521\u0523\3\2\2\2\u0522\u051d\3\2\2\2\u0522")
        buf.write("\u0523\3\2\2\2\u0523E\3\2\2\2\u0524\u0526\5\34\17\2\u0525")
        buf.write("\u0524\3\2\2\2\u0525\u0526\3\2\2\2\u0526\u0527\3\2\2\2")
        buf.write("\u0527\u0528\5N(\2\u0528\u0529\5D#\2\u0529G\3\2\2\2\u052a")
        buf.write("\u052b\b%\1\2\u052b\u052c\5J&\2\u052c\u0544\3\2\2\2\u052d")
        buf.write("\u052e\f\5\2\2\u052e\u052f\6%\3\2\u052f\u0531\t\t\2\2")
        buf.write("\u0530\u0532\5b\62\2\u0531\u0530\3\2\2\2\u0531\u0532\3")
        buf.write("\2\2\2\u0532\u0533\3\2\2\2\u0533\u0543\5H%\6\u0534\u0535")
        buf.write("\f\4\2\2\u0535\u0536\6%\5\2\u0536\u0538\7k\2\2\u0537\u0539")
        buf.write("\5b\62\2\u0538\u0537\3\2\2\2\u0538\u0539\3\2\2\2\u0539")
        buf.write("\u053a\3\2\2\2\u053a\u0543\5H%\5\u053b\u053c\f\3\2\2\u053c")
        buf.write("\u053d\6%\7\2\u053d\u053f\t\n\2\2\u053e\u0540\5b\62\2")
        buf.write("\u053f\u053e\3\2\2\2\u053f\u0540\3\2\2\2\u0540\u0541\3")
        buf.write("\2\2\2\u0541\u0543\5H%\4\u0542\u052d\3\2\2\2\u0542\u0534")
        buf.write("\3\2\2\2\u0542\u053b\3\2\2\2\u0543\u0546\3\2\2\2\u0544")
        buf.write("\u0542\3\2\2\2\u0544\u0545\3\2\2\2\u0545I\3\2\2\2\u0546")
        buf.write("\u0544\3\2\2\2\u0547\u0550\5N(\2\u0548\u0549\7R\2\2\u0549")
        buf.write("\u0550\5\u0086D\2\u054a\u0550\5~@\2\u054b\u054c\7\3\2")
        buf.write("\2\u054c\u054d\5B\"\2\u054d\u054e\7\4\2\2\u054e\u0550")
        buf.write("\3\2\2\2\u054f\u0547\3\2\2\2\u054f\u0548\3\2\2\2\u054f")
        buf.write("\u054a\3\2\2\2\u054f\u054b\3\2\2\2\u0550K\3\2\2\2\u0551")
        buf.write("\u0553\5\u008eH\2\u0552\u0554\t\13\2\2\u0553\u0552\3\2")
        buf.write("\2\2\u0553\u0554\3\2\2\2\u0554\u0557\3\2\2\2\u0555\u0556")
        buf.write("\7,\2\2\u0556\u0558\t\f\2\2\u0557\u0555\3\2\2\2\u0557")
        buf.write("\u0558\3\2\2\2\u0558M\3\2\2\2\u0559\u055a\7\r\2\2\u055a")
        buf.write("\u055b\7\u009f\2\2\u055b\u055c\7\3\2\2\u055c\u055d\5\u008c")
        buf.write("G\2\u055d\u055e\7\4\2\2\u055e\u0564\3\2\2\2\u055f\u0560")
        buf.write("\7r\2\2\u0560\u0564\5\u008cG\2\u0561\u0562\7\u00a0\2\2")
        buf.write("\u0562\u0564\5\u008cG\2\u0563\u0559\3\2\2\2\u0563\u055f")
        buf.write("\3\2\2\2\u0563\u0561\3\2\2\2\u0564\u0566\3\2\2\2\u0565")
        buf.write("\u0567\5\u0084C\2\u0566\u0565\3\2\2\2\u0566\u0567\3\2")
        buf.write("\2\2\u0567\u056a\3\2\2\2\u0568\u0569\7\u00a5\2\2\u0569")
        buf.write("\u056b\7\u00f2\2\2\u056a\u0568\3\2\2\2\u056a\u056b\3\2")
        buf.write("\2\2\u056b\u056c\3\2\2\2\u056c\u056d\7\u00a1\2\2\u056d")
        buf.write("\u057a\7\u00f2\2\2\u056e\u0578\7\20\2\2\u056f\u0579\5")
        buf.write("r:\2\u0570\u0579\5\u00acW\2\u0571\u0574\7\3\2\2\u0572")
        buf.write("\u0575\5r:\2\u0573\u0575\5\u00acW\2\u0574\u0572\3\2\2")
        buf.write("\2\u0574\u0573\3\2\2\2\u0575\u0576\3\2\2\2\u0576\u0577")
        buf.write("\7\4\2\2\u0577\u0579\3\2\2\2\u0578\u056f\3\2\2\2\u0578")
        buf.write("\u0570\3\2\2\2\u0578\u0571\3\2\2\2\u0579\u057b\3\2\2\2")
        buf.write("\u057a\u056e\3\2\2\2\u057a\u057b\3\2\2\2\u057b\u057d\3")
        buf.write("\2\2\2\u057c\u057e\5\u0084C\2\u057d\u057c\3\2\2\2\u057d")
        buf.write("\u057e\3\2\2\2\u057e\u0581\3\2\2\2\u057f\u0580\7\u00a4")
        buf.write("\2\2\u0580\u0582\7\u00f2\2\2\u0581\u057f\3\2\2\2\u0581")
        buf.write("\u0582\3\2\2\2\u0582\u0584\3\2\2\2\u0583\u0585\5T+\2\u0584")
        buf.write("\u0583\3\2\2\2\u0584\u0585\3\2\2\2\u0585\u0588\3\2\2\2")
        buf.write("\u0586\u0587\7\24\2\2\u0587\u0589\5\u0090I\2\u0588\u0586")
        buf.write("\3\2\2\2\u0588\u0589\3\2\2\2\u0589\u05b7\3\2\2\2\u058a")
        buf.write("\u058e\7\r\2\2\u058b\u058d\5P)\2\u058c\u058b\3\2\2\2\u058d")
        buf.write("\u0590\3\2\2\2\u058e\u058c\3\2\2\2\u058e\u058f\3\2\2\2")
        buf.write("\u058f\u0592\3\2\2\2\u0590\u058e\3\2\2\2\u0591\u0593\5")
        buf.write("b\62\2\u0592\u0591\3\2\2\2\u0592\u0593\3\2\2\2\u0593\u0594")
        buf.write("\3\2\2\2\u0594\u0596\5\u008cG\2\u0595\u0597\5T+\2\u0596")
        buf.write("\u0595\3\2\2\2\u0596\u0597\3\2\2\2\u0597\u05a1\3\2\2\2")
        buf.write("\u0598\u059e\5T+\2\u0599\u059b\7\r\2\2\u059a\u059c\5b")
        buf.write("\62\2\u059b\u059a\3\2\2\2\u059b\u059c\3\2\2\2\u059c\u059d")
        buf.write("\3\2\2\2\u059d\u059f\5\u008cG\2\u059e\u0599\3\2\2\2\u059e")
        buf.write("\u059f\3\2\2\2\u059f\u05a1\3\2\2\2\u05a0\u058a\3\2\2\2")
        buf.write("\u05a0\u0598\3\2\2\2\u05a1\u05a5\3\2\2\2\u05a2\u05a4\5")
        buf.write("`\61\2\u05a3\u05a2\3\2\2\2\u05a4\u05a7\3\2\2\2\u05a5\u05a3")
        buf.write("\3\2\2\2\u05a5\u05a6\3\2\2\2\u05a6\u05aa\3\2\2\2\u05a7")
        buf.write("\u05a5\3\2\2\2\u05a8\u05a9\7\24\2\2\u05a9\u05ab\5\u0090")
        buf.write("I\2\u05aa\u05a8\3\2\2\2\u05aa\u05ab\3\2\2\2\u05ab\u05ad")
        buf.write("\3\2\2\2\u05ac\u05ae\5V,\2\u05ad\u05ac\3\2\2\2\u05ad\u05ae")
        buf.write("\3\2\2\2\u05ae\u05b1\3\2\2\2\u05af\u05b0\7\34\2\2\u05b0")
        buf.write("\u05b2\5\u0090I\2\u05b1\u05af\3\2\2\2\u05b1\u05b2\3\2")
        buf.write("\2\2\u05b2\u05b4\3\2\2\2\u05b3\u05b5\5\u00b6\\\2\u05b4")
        buf.write("\u05b3\3\2\2\2\u05b4\u05b5\3\2\2\2\u05b5\u05b7\3\2\2\2")
        buf.write("\u05b6\u0563\3\2\2\2\u05b6\u05a0\3\2\2\2\u05b7O\3\2\2")
        buf.write("\2\u05b8\u05b9\7\7\2\2\u05b9\u05c0\5R*\2\u05ba\u05bc\7")
        buf.write("\5\2\2\u05bb\u05ba\3\2\2\2\u05bb\u05bc\3\2\2\2\u05bc\u05bd")
        buf.write("\3\2\2\2\u05bd\u05bf\5R*\2\u05be\u05bb\3\2\2\2\u05bf\u05c2")
        buf.write("\3\2\2\2\u05c0\u05be\3\2\2\2\u05c0\u05c1\3\2\2\2\u05c1")
        buf.write("\u05c3\3\2\2\2\u05c2\u05c0\3\2\2\2\u05c3\u05c4\7\b\2\2")
        buf.write("\u05c4Q\3\2\2\2\u05c5\u05d3\5\u00c2b\2\u05c6\u05c7\5\u00c2")
        buf.write("b\2\u05c7\u05c8\7\3\2\2\u05c8\u05cd\5\u0096L\2\u05c9\u05ca")
        buf.write("\7\5\2\2\u05ca\u05cc\5\u0096L\2\u05cb\u05c9\3\2\2\2\u05cc")
        buf.write("\u05cf\3\2\2\2\u05cd\u05cb\3\2\2\2\u05cd\u05ce\3\2\2\2")
        buf.write("\u05ce\u05d0\3\2\2\2\u05cf\u05cd\3\2\2\2\u05d0\u05d1\7")
        buf.write("\4\2\2\u05d1\u05d3\3\2\2\2\u05d2\u05c5\3\2\2\2\u05d2\u05c6")
        buf.write("\3\2\2\2\u05d3S\3\2\2\2\u05d4\u05d5\7\16\2\2\u05d5\u05da")
        buf.write("\5d\63\2\u05d6\u05d7\7\5\2\2\u05d7\u05d9\5d\63\2\u05d8")
        buf.write("\u05d6\3\2\2\2\u05d9\u05dc\3\2\2\2\u05da\u05d8\3\2\2\2")
        buf.write("\u05da\u05db\3\2\2\2\u05db\u05e0\3\2\2\2\u05dc\u05da\3")
        buf.write("\2\2\2\u05dd\u05df\5`\61\2\u05de\u05dd\3\2\2\2\u05df\u05e2")
        buf.write("\3\2\2\2\u05e0\u05de\3\2\2\2\u05e0\u05e1\3\2\2\2\u05e1")
        buf.write("\u05e4\3\2\2\2\u05e2\u05e0\3\2\2\2\u05e3\u05e5\5Z.\2\u05e4")
        buf.write("\u05e3\3\2\2\2\u05e4\u05e5\3\2\2\2\u05e5U\3\2\2\2\u05e6")
        buf.write("\u05e7\7\25\2\2\u05e7\u05e8\7\26\2\2\u05e8\u05ed\5\u008e")
        buf.write("H\2\u05e9\u05ea\7\5\2\2\u05ea\u05ec\5\u008eH\2\u05eb\u05e9")
        buf.write("\3\2\2\2\u05ec\u05ef\3\2\2\2\u05ed\u05eb\3\2\2\2\u05ed")
        buf.write("\u05ee\3\2\2\2\u05ee\u0601\3\2\2\2\u05ef\u05ed\3\2\2\2")
        buf.write("\u05f0\u05f1\7O\2\2\u05f1\u0602\7\32\2\2\u05f2\u05f3\7")
        buf.write("O\2\2\u05f3\u0602\7\31\2\2\u05f4\u05f5\7\27\2\2\u05f5")
        buf.write("\u05f6\7\30\2\2\u05f6\u05f7\7\3\2\2\u05f7\u05fc\5X-\2")
        buf.write("\u05f8\u05f9\7\5\2\2\u05f9\u05fb\5X-\2\u05fa\u05f8\3\2")
        buf.write("\2\2\u05fb\u05fe\3\2\2\2\u05fc\u05fa\3\2\2\2\u05fc\u05fd")
        buf.write("\3\2\2\2\u05fd\u05ff\3\2\2\2\u05fe\u05fc\3\2\2\2\u05ff")
        buf.write("\u0600\7\4\2\2\u0600\u0602\3\2\2\2\u0601\u05f0\3\2\2\2")
        buf.write("\u0601\u05f2\3\2\2\2\u0601\u05f4\3\2\2\2\u0601\u0602\3")
        buf.write("\2\2\2\u0602\u0613\3\2\2\2\u0603\u0604\7\25\2\2\u0604")
        buf.write("\u0605\7\26\2\2\u0605\u0606\7\27\2\2\u0606\u0607\7\30")
        buf.write("\2\2\u0607\u0608\7\3\2\2\u0608\u060d\5X-\2\u0609\u060a")
        buf.write("\7\5\2\2\u060a\u060c\5X-\2\u060b\u0609\3\2\2\2\u060c\u060f")
        buf.write("\3\2\2\2\u060d\u060b\3\2\2\2\u060d\u060e\3\2\2\2\u060e")
        buf.write("\u0610\3\2\2\2\u060f\u060d\3\2\2\2\u0610\u0611\7\4\2\2")
        buf.write("\u0611\u0613\3\2\2\2\u0612\u05e6\3\2\2\2\u0612\u0603\3")
        buf.write("\2\2\2\u0613W\3\2\2\2\u0614\u061d\7\3\2\2\u0615\u061a")
        buf.write("\5\u008eH\2\u0616\u0617\7\5\2\2\u0617\u0619\5\u008eH\2")
        buf.write("\u0618\u0616\3\2\2\2\u0619\u061c\3\2\2\2\u061a\u0618\3")
        buf.write("\2\2\2\u061a\u061b\3\2\2\2\u061b\u061e\3\2\2\2\u061c\u061a")
        buf.write("\3\2\2\2\u061d\u0615\3\2\2\2\u061d\u061e\3\2\2\2\u061e")
        buf.write("\u061f\3\2\2\2\u061f\u0622\7\4\2\2\u0620\u0622\5\u008e")
        buf.write("H\2\u0621\u0614\3\2\2\2\u0621\u0620\3\2\2\2\u0622Y\3\2")
        buf.write("\2\2\u0623\u0624\7@\2\2\u0624\u0625\7\3\2\2\u0625\u0626")
        buf.write("\5\u008cG\2\u0626\u0627\7/\2\2\u0627\u0628\5\\/\2\u0628")
        buf.write("\u0629\7!\2\2\u0629\u062a\7\3\2\2\u062a\u062f\5^\60\2")
        buf.write("\u062b\u062c\7\5\2\2\u062c\u062e\5^\60\2\u062d\u062b\3")
        buf.write("\2\2\2\u062e\u0631\3\2\2\2\u062f\u062d\3\2\2\2\u062f\u0630")
        buf.write("\3\2\2\2\u0630\u0632\3\2\2\2\u0631\u062f\3\2\2\2\u0632")
        buf.write("\u0633\7\4\2\2\u0633\u0634\7\4\2\2\u0634[\3\2\2\2\u0635")
        buf.write("\u0642\5\u00c2b\2\u0636\u0637\7\3\2\2\u0637\u063c\5\u00c2")
        buf.write("b\2\u0638\u0639\7\5\2\2\u0639\u063b\5\u00c2b\2\u063a\u0638")
        buf.write("\3\2\2\2\u063b\u063e\3\2\2\2\u063c\u063a\3\2\2\2\u063c")
        buf.write("\u063d\3\2\2\2\u063d\u063f\3\2\2\2\u063e\u063c\3\2\2\2")
        buf.write("\u063f\u0640\7\4\2\2\u0640\u0642\3\2\2\2\u0641\u0635\3")
        buf.write("\2\2\2\u0641\u0636\3\2\2\2\u0642]\3\2\2\2\u0643\u0648")
        buf.write("\5\u008eH\2\u0644\u0646\7\20\2\2\u0645\u0644\3\2\2\2\u0645")
        buf.write("\u0646\3\2\2\2\u0646\u0647\3\2\2\2\u0647\u0649\5\u00c2")
        buf.write("b\2\u0648\u0645\3\2\2\2\u0648\u0649\3\2\2\2\u0649_\3\2")
        buf.write("\2\2\u064a\u064b\7A\2\2\u064b\u064d\7T\2\2\u064c\u064e")
        buf.write("\78\2\2\u064d\u064c\3\2\2\2\u064d\u064e\3\2\2\2\u064e")
        buf.write("\u064f\3\2\2\2\u064f\u0650\5\u00c0a\2\u0650\u0659\7\3")
        buf.write("\2\2\u0651\u0656\5\u008eH\2\u0652\u0653\7\5\2\2\u0653")
        buf.write("\u0655\5\u008eH\2\u0654\u0652\3\2\2\2\u0655\u0658\3\2")
        buf.write("\2\2\u0656\u0654\3\2\2\2\u0656\u0657\3\2\2\2\u0657\u065a")
        buf.write("\3\2\2\2\u0658\u0656\3\2\2\2\u0659\u0651\3\2\2\2\u0659")
        buf.write("\u065a\3\2\2\2\u065a\u065b\3\2\2\2\u065b\u065c\7\4\2\2")
        buf.write("\u065c\u0668\5\u00c2b\2\u065d\u065f\7\20\2\2\u065e\u065d")
        buf.write("\3\2\2\2\u065e\u065f\3\2\2\2\u065f\u0660\3\2\2\2\u0660")
        buf.write("\u0665\5\u00c2b\2\u0661\u0662\7\5\2\2\u0662\u0664\5\u00c2")
        buf.write("b\2\u0663\u0661\3\2\2\2\u0664\u0667\3\2\2\2\u0665\u0663")
        buf.write("\3\2\2\2\u0665\u0666\3\2\2\2\u0666\u0669\3\2\2\2\u0667")
        buf.write("\u0665\3\2\2\2\u0668\u065e\3\2\2\2\u0668\u0669\3\2\2\2")
        buf.write("\u0669a\3\2\2\2\u066a\u066b\t\r\2\2\u066bc\3\2\2\2\u066c")
        buf.write("\u0670\5|?\2\u066d\u066f\5f\64\2\u066e\u066d\3\2\2\2\u066f")
        buf.write("\u0672\3\2\2\2\u0670\u066e\3\2\2\2\u0670\u0671\3\2\2\2")
        buf.write("\u0671e\3\2\2\2\u0672\u0670\3\2\2\2\u0673\u0674\5h\65")
        buf.write("\2\u0674\u0675\7\66\2\2\u0675\u0677\5|?\2\u0676\u0678")
        buf.write("\5j\66\2\u0677\u0676\3\2\2\2\u0677\u0678\3\2\2\2\u0678")
        buf.write("\u067f\3\2\2\2\u0679\u067a\7>\2\2\u067a\u067b\5h\65\2")
        buf.write("\u067b\u067c\7\66\2\2\u067c\u067d\5|?\2\u067d\u067f\3")
        buf.write("\2\2\2\u067e\u0673\3\2\2\2\u067e\u0679\3\2\2\2\u067fg")
        buf.write("\3\2\2\2\u0680\u0682\79\2\2\u0681\u0680\3\2\2\2\u0681")
        buf.write("\u0682\3\2\2\2\u0682\u0697\3\2\2\2\u0683\u0697\7\67\2")
        buf.write("\2\u0684\u0686\7:\2\2\u0685\u0687\78\2\2\u0686\u0685\3")
        buf.write("\2\2\2\u0686\u0687\3\2\2\2\u0687\u0697\3\2\2\2\u0688\u0689")
        buf.write("\7:\2\2\u0689\u0697\7;\2\2\u068a\u068c\7<\2\2\u068b\u068d")
        buf.write("\78\2\2\u068c\u068b\3\2\2\2\u068c\u068d\3\2\2\2\u068d")
        buf.write("\u0697\3\2\2\2\u068e\u0690\7=\2\2\u068f\u0691\78\2\2\u0690")
        buf.write("\u068f\3\2\2\2\u0690\u0691\3\2\2\2\u0691\u0697\3\2\2\2")
        buf.write("\u0692\u0694\7:\2\2\u0693\u0692\3\2\2\2\u0693\u0694\3")
        buf.write("\2\2\2\u0694\u0695\3\2\2\2\u0695\u0697\7\u00ef\2\2\u0696")
        buf.write("\u0681\3\2\2\2\u0696\u0683\3\2\2\2\u0696\u0684\3\2\2\2")
        buf.write("\u0696\u0688\3\2\2\2\u0696\u068a\3\2\2\2\u0696\u068e\3")
        buf.write("\2\2\2\u0696\u0693\3\2\2\2\u0697i\3\2\2\2\u0698\u0699")
        buf.write("\7?\2\2\u0699\u069d\5\u0090I\2\u069a\u069b\7\u00a1\2\2")
        buf.write("\u069b\u069d\5p9\2\u069c\u0698\3\2\2\2\u069c\u069a\3\2")
        buf.write("\2\2\u069dk\3\2\2\2\u069e\u069f\7m\2\2\u069f\u06a1\7\3")
        buf.write("\2\2\u06a0\u06a2\5n8\2\u06a1\u06a0\3\2\2\2\u06a1\u06a2")
        buf.write("\3\2\2\2\u06a2\u06a3\3\2\2\2\u06a3\u06a4\7\4\2\2\u06a4")
        buf.write("m\3\2\2\2\u06a5\u06a7\7\u008d\2\2\u06a6\u06a5\3\2\2\2")
        buf.write("\u06a6\u06a7\3\2\2\2\u06a7\u06a8\3\2\2\2\u06a8\u06a9\t")
        buf.write("\16\2\2\u06a9\u06be\7\u0097\2\2\u06aa\u06ab\5\u008eH\2")
        buf.write("\u06ab\u06ac\7F\2\2\u06ac\u06be\3\2\2\2\u06ad\u06ae\7")
        buf.write("\u0098\2\2\u06ae\u06af\7\u00f6\2\2\u06af\u06b0\7\u0099")
        buf.write("\2\2\u06b0\u06b1\7\u009a\2\2\u06b1\u06ba\7\u00f6\2\2\u06b2")
        buf.write("\u06b8\7?\2\2\u06b3\u06b9\5\u00c2b\2\u06b4\u06b5\5\u00c0")
        buf.write("a\2\u06b5\u06b6\7\3\2\2\u06b6\u06b7\7\4\2\2\u06b7\u06b9")
        buf.write("\3\2\2\2\u06b8\u06b3\3\2\2\2\u06b8\u06b4\3\2\2\2\u06b9")
        buf.write("\u06bb\3\2\2\2\u06ba\u06b2\3\2\2\2\u06ba\u06bb\3\2\2\2")
        buf.write("\u06bb\u06be\3\2\2\2\u06bc\u06be\5\u008eH\2\u06bd\u06a6")
        buf.write("\3\2\2\2\u06bd\u06aa\3\2\2\2\u06bd\u06ad\3\2\2\2\u06bd")
        buf.write("\u06bc\3\2\2\2\u06beo\3\2\2\2\u06bf\u06c0\7\3\2\2\u06c0")
        buf.write("\u06c1\5r:\2\u06c1\u06c2\7\4\2\2\u06c2q\3\2\2\2\u06c3")
        buf.write("\u06c8\5\u00c2b\2\u06c4\u06c5\7\5\2\2\u06c5\u06c7\5\u00c2")
        buf.write("b\2\u06c6\u06c4\3\2\2\2\u06c7\u06ca\3\2\2\2\u06c8\u06c6")
        buf.write("\3\2\2\2\u06c8\u06c9\3\2\2\2\u06c9s\3\2\2\2\u06ca\u06c8")
        buf.write("\3\2\2\2\u06cb\u06cc\7\3\2\2\u06cc\u06d1\5v<\2\u06cd\u06ce")
        buf.write("\7\5\2\2\u06ce\u06d0\5v<\2\u06cf\u06cd\3\2\2\2\u06d0\u06d3")
        buf.write("\3\2\2\2\u06d1\u06cf\3\2\2\2\u06d1\u06d2\3\2\2\2\u06d2")
        buf.write("\u06d4\3\2\2\2\u06d3\u06d1\3\2\2\2\u06d4\u06d5\7\4\2\2")
        buf.write("\u06d5u\3\2\2\2\u06d6\u06d8\5\u00c2b\2\u06d7\u06d9\t\13")
        buf.write("\2\2\u06d8\u06d7\3\2\2\2\u06d8\u06d9\3\2\2\2\u06d9w\3")
        buf.write("\2\2\2\u06da\u06db\7\3\2\2\u06db\u06e0\5z>\2\u06dc\u06dd")
        buf.write("\7\5\2\2\u06dd\u06df\5z>\2\u06de\u06dc\3\2\2\2\u06df\u06e2")
        buf.write("\3\2\2\2\u06e0\u06de\3\2\2\2\u06e0\u06e1\3\2\2\2\u06e1")
        buf.write("\u06e3\3\2\2\2\u06e2\u06e0\3\2\2\2\u06e3\u06e4\7\4\2\2")
        buf.write("\u06e4y\3\2\2\2\u06e5\u06e8\5\u00c2b\2\u06e6\u06e7\7t")
        buf.write("\2\2\u06e7\u06e9\7\u00f2\2\2\u06e8\u06e6\3\2\2\2\u06e8")
        buf.write("\u06e9\3\2\2\2\u06e9{\3\2\2\2\u06ea\u06ec\5\u0086D\2\u06eb")
        buf.write("\u06ed\5l\67\2\u06ec\u06eb\3\2\2\2\u06ec\u06ed\3\2\2\2")
        buf.write("\u06ed\u06ee\3\2\2\2\u06ee\u06ef\5\u0082B\2\u06ef\u0703")
        buf.write("\3\2\2\2\u06f0\u06f1\7\3\2\2\u06f1\u06f2\5B\"\2\u06f2")
        buf.write("\u06f4\7\4\2\2\u06f3\u06f5\5l\67\2\u06f4\u06f3\3\2\2\2")
        buf.write("\u06f4\u06f5\3\2\2\2\u06f5\u06f6\3\2\2\2\u06f6\u06f7\5")
        buf.write("\u0082B\2\u06f7\u0703\3\2\2\2\u06f8\u06f9\7\3\2\2\u06f9")
        buf.write("\u06fa\5d\63\2\u06fa\u06fc\7\4\2\2\u06fb\u06fd\5l\67\2")
        buf.write("\u06fc\u06fb\3\2\2\2\u06fc\u06fd\3\2\2\2\u06fd\u06fe\3")
        buf.write("\2\2\2\u06fe\u06ff\5\u0082B\2\u06ff\u0703\3\2\2\2\u0700")
        buf.write("\u0703\5~@\2\u0701\u0703\5\u0080A\2\u0702\u06ea\3\2\2")
        buf.write("\2\u0702\u06f0\3\2\2\2\u0702\u06f8\3\2\2\2\u0702\u0700")
        buf.write("\3\2\2\2\u0702\u0701\3\2\2\2\u0703}\3\2\2\2\u0704\u0705")
        buf.write("\7P\2\2\u0705\u070a\5\u008eH\2\u0706\u0707\7\5\2\2\u0707")
        buf.write("\u0709\5\u008eH\2\u0708\u0706\3\2\2\2\u0709\u070c\3\2")
        buf.write("\2\2\u070a\u0708\3\2\2\2\u070a\u070b\3\2\2\2\u070b\u070d")
        buf.write("\3\2\2\2\u070c\u070a\3\2\2\2\u070d\u070e\5\u0082B\2\u070e")
        buf.write("\177\3\2\2\2\u070f\u0710\5\u00c2b\2\u0710\u0719\7\3\2")
        buf.write("\2\u0711\u0716\5\u008eH\2\u0712\u0713\7\5\2\2\u0713\u0715")
        buf.write("\5\u008eH\2\u0714\u0712\3\2\2\2\u0715\u0718\3\2\2\2\u0716")
        buf.write("\u0714\3\2\2\2\u0716\u0717\3\2\2\2\u0717\u071a\3\2\2\2")
        buf.write("\u0718\u0716\3\2\2\2\u0719\u0711\3\2\2\2\u0719\u071a\3")
        buf.write("\2\2\2\u071a\u071b\3\2\2\2\u071b\u071c\7\4\2\2\u071c\u071d")
        buf.write("\5\u0082B\2\u071d\u0081\3\2\2\2\u071e\u0720\7\20\2\2\u071f")
        buf.write("\u071e\3\2\2\2\u071f\u0720\3\2\2\2\u0720\u0721\3\2\2\2")
        buf.write("\u0721\u0723\5\u00c4c\2\u0722\u0724\5p9\2\u0723\u0722")
        buf.write("\3\2\2\2\u0723\u0724\3\2\2\2\u0724\u0726\3\2\2\2\u0725")
        buf.write("\u071f\3\2\2\2\u0725\u0726\3\2\2\2\u0726\u0083\3\2\2\2")
        buf.write("\u0727\u0728\7N\2\2\u0728\u0729\7[\2\2\u0729\u072a\7\u00a2")
        buf.write("\2\2\u072a\u072e\7\u00f2\2\2\u072b\u072c\7O\2\2\u072c")
        buf.write("\u072d\7\u00a3\2\2\u072d\u072f\5.\30\2\u072e\u072b\3\2")
        buf.write("\2\2\u072e\u072f\3\2\2\2\u072f\u0759\3\2\2\2\u0730\u0731")
        buf.write("\7N\2\2\u0731\u0732\7[\2\2\u0732\u073c\7\u00a6\2\2\u0733")
        buf.write("\u0734\7\u00a7\2\2\u0734\u0735\7\u00a8\2\2\u0735\u0736")
        buf.write("\7\26\2\2\u0736\u073a\7\u00f2\2\2\u0737\u0738\7\u00ac")
        buf.write("\2\2\u0738\u0739\7\26\2\2\u0739\u073b\7\u00f2\2\2\u073a")
        buf.write("\u0737\3\2\2\2\u073a\u073b\3\2\2\2\u073b\u073d\3\2\2\2")
        buf.write("\u073c\u0733\3\2\2\2\u073c\u073d\3\2\2\2\u073d\u0743\3")
        buf.write("\2\2\2\u073e\u073f\7\u00a9\2\2\u073f\u0740\7\u00aa\2\2")
        buf.write("\u0740\u0741\7\u00a8\2\2\u0741\u0742\7\26\2\2\u0742\u0744")
        buf.write("\7\u00f2\2\2\u0743\u073e\3\2\2\2\u0743\u0744\3\2\2\2\u0744")
        buf.write("\u074a\3\2\2\2\u0745\u0746\7r\2\2\u0746\u0747\7\u00ab")
        buf.write("\2\2\u0747\u0748\7\u00a8\2\2\u0748\u0749\7\26\2\2\u0749")
        buf.write("\u074b\7\u00f2\2\2\u074a\u0745\3\2\2\2\u074a\u074b\3\2")
        buf.write("\2\2\u074b\u0750\3\2\2\2\u074c\u074d\7\u00ad\2\2\u074d")
        buf.write("\u074e\7\u00a8\2\2\u074e\u074f\7\26\2\2\u074f\u0751\7")
        buf.write("\u00f2\2\2\u0750\u074c\3\2\2\2\u0750\u0751\3\2\2\2\u0751")
        buf.write("\u0756\3\2\2\2\u0752\u0753\7)\2\2\u0753\u0754\7\u00db")
        buf.write("\2\2\u0754\u0755\7\20\2\2\u0755\u0757\7\u00f2\2\2\u0756")
        buf.write("\u0752\3\2\2\2\u0756\u0757\3\2\2\2\u0757\u0759\3\2\2\2")
        buf.write("\u0758\u0727\3\2\2\2\u0758\u0730\3\2\2\2\u0759\u0085\3")
        buf.write("\2\2\2\u075a\u075b\5\u00c2b\2\u075b\u075c\7\6\2\2\u075c")
        buf.write("\u075e\3\2\2\2\u075d\u075a\3\2\2\2\u075d\u075e\3\2\2\2")
        buf.write("\u075e\u075f\3\2\2\2\u075f\u0760\5\u00c2b\2\u0760\u0087")
        buf.write("\3\2\2\2\u0761\u0762\5\u00c2b\2\u0762\u0763\7\6\2\2\u0763")
        buf.write("\u0765\3\2\2\2\u0764\u0761\3\2\2\2\u0764\u0765\3\2\2\2")
        buf.write("\u0765\u0766\3\2\2\2\u0766\u0767\5\u00c2b\2\u0767\u0089")
        buf.write("\3\2\2\2\u0768\u0770\5\u008eH\2\u0769\u076b\7\20\2\2\u076a")
        buf.write("\u0769\3\2\2\2\u076a\u076b\3\2\2\2\u076b\u076e\3\2\2\2")
        buf.write("\u076c\u076f\5\u00c2b\2\u076d\u076f\5p9\2\u076e\u076c")
        buf.write("\3\2\2\2\u076e\u076d\3\2\2\2\u076f\u0771\3\2\2\2\u0770")
        buf.write("\u076a\3\2\2\2\u0770\u0771\3\2\2\2\u0771\u008b\3\2\2\2")
        buf.write("\u0772\u0777\5\u008aF\2\u0773\u0774\7\5\2\2\u0774\u0776")
        buf.write("\5\u008aF\2\u0775\u0773\3\2\2\2\u0776\u0779\3\2\2\2\u0777")
        buf.write("\u0775\3\2\2\2\u0777\u0778\3\2\2\2\u0778\u008d\3\2\2\2")
        buf.write("\u0779\u0777\3\2\2\2\u077a\u077b\5\u0090I\2\u077b\u008f")
        buf.write("\3\2\2\2\u077c\u077d\bI\1\2\u077d\u077e\7\"\2\2\u077e")
        buf.write("\u0789\5\u0090I\7\u077f\u0780\7$\2\2\u0780\u0781\7\3\2")
        buf.write("\2\u0781\u0782\5\32\16\2\u0782\u0783\7\4\2\2\u0783\u0789")
        buf.write("\3\2\2\2\u0784\u0786\5\u0094K\2\u0785\u0787\5\u0092J\2")
        buf.write("\u0786\u0785\3\2\2\2\u0786\u0787\3\2\2\2\u0787\u0789\3")
        buf.write("\2\2\2\u0788\u077c\3\2\2\2\u0788\u077f\3\2\2\2\u0788\u0784")
        buf.write("\3\2\2\2\u0789\u0792\3\2\2\2\u078a\u078b\f\4\2\2\u078b")
        buf.write("\u078c\7 \2\2\u078c\u0791\5\u0090I\5\u078d\u078e\f\3\2")
        buf.write("\2\u078e\u078f\7\37\2\2\u078f\u0791\5\u0090I\4\u0790\u078a")
        buf.write("\3\2\2\2\u0790\u078d\3\2\2\2\u0791\u0794\3\2\2\2\u0792")
        buf.write("\u0790\3\2\2\2\u0792\u0793\3\2\2\2\u0793\u0091\3\2\2\2")
        buf.write("\u0794\u0792\3\2\2\2\u0795\u0797\7\"\2\2\u0796\u0795\3")
        buf.write("\2\2\2\u0796\u0797\3\2\2\2\u0797\u0798\3\2\2\2\u0798\u0799")
        buf.write("\7%\2\2\u0799\u079a\5\u0094K\2\u079a\u079b\7 \2\2\u079b")
        buf.write("\u079c\5\u0094K\2\u079c\u07c6\3\2\2\2\u079d\u079f\7\"")
        buf.write("\2\2\u079e\u079d\3\2\2\2\u079e\u079f\3\2\2\2\u079f\u07a0")
        buf.write("\3\2\2\2\u07a0\u07a1\7!\2\2\u07a1\u07a2\7\3\2\2\u07a2")
        buf.write("\u07a7\5\u008eH\2\u07a3\u07a4\7\5\2\2\u07a4\u07a6\5\u008e")
        buf.write("H\2\u07a5\u07a3\3\2\2\2\u07a6\u07a9\3\2\2\2\u07a7\u07a5")
        buf.write("\3\2\2\2\u07a7\u07a8\3\2\2\2\u07a8\u07aa\3\2\2\2\u07a9")
        buf.write("\u07a7\3\2\2\2\u07aa\u07ab\7\4\2\2\u07ab\u07c6\3\2\2\2")
        buf.write("\u07ac\u07ae\7\"\2\2\u07ad\u07ac\3\2\2\2\u07ad\u07ae\3")
        buf.write("\2\2\2\u07ae\u07af\3\2\2\2\u07af\u07b0\7!\2\2\u07b0\u07b1")
        buf.write("\7\3\2\2\u07b1\u07b2\5\32\16\2\u07b2\u07b3\7\4\2\2\u07b3")
        buf.write("\u07c6\3\2\2\2\u07b4\u07b6\7\"\2\2\u07b5\u07b4\3\2\2\2")
        buf.write("\u07b5\u07b6\3\2\2\2\u07b6\u07b7\3\2\2\2\u07b7\u07b8\t")
        buf.write("\17\2\2\u07b8\u07c6\5\u0094K\2\u07b9\u07bb\7(\2\2\u07ba")
        buf.write("\u07bc\7\"\2\2\u07bb\u07ba\3\2\2\2\u07bb\u07bc\3\2\2\2")
        buf.write("\u07bc\u07bd\3\2\2\2\u07bd\u07c6\7)\2\2\u07be\u07c0\7")
        buf.write("(\2\2\u07bf\u07c1\7\"\2\2\u07c0\u07bf\3\2\2\2\u07c0\u07c1")
        buf.write("\3\2\2\2\u07c1\u07c2\3\2\2\2\u07c2\u07c3\7\23\2\2\u07c3")
        buf.write("\u07c4\7\16\2\2\u07c4\u07c6\5\u0094K\2\u07c5\u0796\3\2")
        buf.write("\2\2\u07c5\u079e\3\2\2\2\u07c5\u07ad\3\2\2\2\u07c5\u07b5")
        buf.write("\3\2\2\2\u07c5\u07b9\3\2\2\2\u07c5\u07be\3\2\2\2\u07c6")
        buf.write("\u0093\3\2\2\2\u07c7\u07c8\bK\1\2\u07c8\u07cc\5\u0096")
        buf.write("L\2\u07c9\u07ca\t\20\2\2\u07ca\u07cc\5\u0094K\t\u07cb")
        buf.write("\u07c7\3\2\2\2\u07cb\u07c9\3\2\2\2\u07cc\u07e2\3\2\2\2")
        buf.write("\u07cd\u07ce\f\b\2\2\u07ce\u07cf\t\21\2\2\u07cf\u07e1")
        buf.write("\5\u0094K\t\u07d0\u07d1\f\7\2\2\u07d1\u07d2\t\22\2\2\u07d2")
        buf.write("\u07e1\5\u0094K\b\u07d3\u07d4\f\6\2\2\u07d4\u07d5\7\u0093")
        buf.write("\2\2\u07d5\u07e1\5\u0094K\7\u07d6\u07d7\f\5\2\2\u07d7")
        buf.write("\u07d8\7\u0096\2\2\u07d8\u07e1\5\u0094K\6\u07d9\u07da")
        buf.write("\f\4\2\2\u07da\u07db\7\u0094\2\2\u07db\u07e1\5\u0094K")
        buf.write("\5\u07dc\u07dd\f\3\2\2\u07dd\u07de\5\u009aN\2\u07de\u07df")
        buf.write("\5\u0094K\4\u07df\u07e1\3\2\2\2\u07e0\u07cd\3\2\2\2\u07e0")
        buf.write("\u07d0\3\2\2\2\u07e0\u07d3\3\2\2\2\u07e0\u07d6\3\2\2\2")
        buf.write("\u07e0\u07d9\3\2\2\2\u07e0\u07dc\3\2\2\2\u07e1\u07e4\3")
        buf.write("\2\2\2\u07e2\u07e0\3\2\2\2\u07e2\u07e3\3\2\2\2\u07e3\u0095")
        buf.write("\3\2\2\2\u07e4\u07e2\3\2\2\2\u07e5\u07e6\bL\1\2\u07e6")
        buf.write("\u07e8\7\61\2\2\u07e7\u07e9\5\u00b4[\2\u07e8\u07e7\3\2")
        buf.write("\2\2\u07e9\u07ea\3\2\2\2\u07ea\u07e8\3\2\2\2\u07ea\u07eb")
        buf.write("\3\2\2\2\u07eb\u07ee\3\2\2\2\u07ec\u07ed\7\64\2\2\u07ed")
        buf.write("\u07ef\5\u008eH\2\u07ee\u07ec\3\2\2\2\u07ee\u07ef\3\2")
        buf.write("\2\2\u07ef\u07f0\3\2\2\2\u07f0\u07f1\7\65\2\2\u07f1\u0877")
        buf.write("\3\2\2\2\u07f2\u07f3\7\61\2\2\u07f3\u07f5\5\u008eH\2\u07f4")
        buf.write("\u07f6\5\u00b4[\2\u07f5\u07f4\3\2\2\2\u07f6\u07f7\3\2")
        buf.write("\2\2\u07f7\u07f5\3\2\2\2\u07f7\u07f8\3\2\2\2\u07f8\u07fb")
        buf.write("\3\2\2\2\u07f9\u07fa\7\64\2\2\u07fa\u07fc\5\u008eH\2\u07fb")
        buf.write("\u07f9\3\2\2\2\u07fb\u07fc\3\2\2\2\u07fc\u07fd\3\2\2\2")
        buf.write("\u07fd\u07fe\7\65\2\2\u07fe\u0877\3\2\2\2\u07ff\u0800")
        buf.write("\7_\2\2\u0800\u0801\7\3\2\2\u0801\u0802\5\u008eH\2\u0802")
        buf.write("\u0803\7\20\2\2\u0803\u0804\5\u00aaV\2\u0804\u0805\7\4")
        buf.write("\2\2\u0805\u0877\3\2\2\2\u0806\u0807\7s\2\2\u0807\u0810")
        buf.write("\7\3\2\2\u0808\u080d\5\u008aF\2\u0809\u080a\7\5\2\2\u080a")
        buf.write("\u080c\5\u008aF\2\u080b\u0809\3\2\2\2\u080c\u080f\3\2")
        buf.write("\2\2\u080d\u080b\3\2\2\2\u080d\u080e\3\2\2\2\u080e\u0811")
        buf.write("\3\2\2\2\u080f\u080d\3\2\2\2\u0810\u0808\3\2\2\2\u0810")
        buf.write("\u0811\3\2\2\2\u0811\u0812\3\2\2\2\u0812\u0877\7\4\2\2")
        buf.write("\u0813\u0814\7K\2\2\u0814\u0815\7\3\2\2\u0815\u0818\5")
        buf.write("\u008eH\2\u0816\u0817\7}\2\2\u0817\u0819\7,\2\2\u0818")
        buf.write("\u0816\3\2\2\2\u0818\u0819\3\2\2\2\u0819\u081a\3\2\2\2")
        buf.write("\u081a\u081b\7\4\2\2\u081b\u0877\3\2\2\2\u081c\u081d\7")
        buf.write("M\2\2\u081d\u081e\7\3\2\2\u081e\u0821\5\u008eH\2\u081f")
        buf.write("\u0820\7}\2\2\u0820\u0822\7,\2\2\u0821\u081f\3\2\2\2\u0821")
        buf.write("\u0822\3\2\2\2\u0822\u0823\3\2\2\2\u0823\u0824\7\4\2\2")
        buf.write("\u0824\u0877\3\2\2\2\u0825\u0826\7\u0082\2\2\u0826\u0827")
        buf.write("\7\3\2\2\u0827\u0828\5\u0094K\2\u0828\u0829\7!\2\2\u0829")
        buf.write("\u082a\5\u0094K\2\u082a\u082b\7\4\2\2\u082b\u0877\3\2")
        buf.write("\2\2\u082c\u0877\5\u0098M\2\u082d\u0877\7\u008e\2\2\u082e")
        buf.write("\u082f\5\u00c0a\2\u082f\u0830\7\6\2\2\u0830\u0831\7\u008e")
        buf.write("\2\2\u0831\u0877\3\2\2\2\u0832\u0833\7\3\2\2\u0833\u0836")
        buf.write("\5\u008aF\2\u0834\u0835\7\5\2\2\u0835\u0837\5\u008aF\2")
        buf.write("\u0836\u0834\3\2\2\2\u0837\u0838\3\2\2\2\u0838\u0836\3")
        buf.write("\2\2\2\u0838\u0839\3\2\2\2\u0839\u083a\3\2\2\2\u083a\u083b")
        buf.write("\7\4\2\2\u083b\u0877\3\2\2\2\u083c\u083d\7\3\2\2\u083d")
        buf.write("\u083e\5\32\16\2\u083e\u083f\7\4\2\2\u083f\u0877\3\2\2")
        buf.write("\2\u0840\u0841\5\u00c0a\2\u0841\u084d\7\3\2\2\u0842\u0844")
        buf.write("\5b\62\2\u0843\u0842\3\2\2\2\u0843\u0844\3\2\2\2\u0844")
        buf.write("\u0845\3\2\2\2\u0845\u084a\5\u008eH\2\u0846\u0847\7\5")
        buf.write("\2\2\u0847\u0849\5\u008eH\2\u0848\u0846\3\2\2\2\u0849")
        buf.write("\u084c\3\2\2\2\u084a\u0848\3\2\2\2\u084a\u084b\3\2\2\2")
        buf.write("\u084b\u084e\3\2\2\2\u084c\u084a\3\2\2\2\u084d\u0843\3")
        buf.write("\2\2\2\u084d\u084e\3\2\2\2\u084e\u084f\3\2\2\2\u084f\u0852")
        buf.write("\7\4\2\2\u0850\u0851\7C\2\2\u0851\u0853\5\u00ba^\2\u0852")
        buf.write("\u0850\3\2\2\2\u0852\u0853\3\2\2\2\u0853\u0877\3\2\2\2")
        buf.write("\u0854\u0855\5\u00c0a\2\u0855\u0856\7\3\2\2\u0856\u0857")
        buf.write("\t\23\2\2\u0857\u0858\5\u008eH\2\u0858\u0859\7\16\2\2")
        buf.write("\u0859\u085a\5\u008eH\2\u085a\u085b\7\4\2\2\u085b\u0877")
        buf.write("\3\2\2\2\u085c\u085d\7\u00fa\2\2\u085d\u085e\7\t\2\2\u085e")
        buf.write("\u0877\5\u008eH\2\u085f\u0860\7\3\2\2\u0860\u0863\7\u00fa")
        buf.write("\2\2\u0861\u0862\7\5\2\2\u0862\u0864\7\u00fa\2\2\u0863")
        buf.write("\u0861\3\2\2\2\u0864\u0865\3\2\2\2\u0865\u0863\3\2\2\2")
        buf.write("\u0865\u0866\3\2\2\2\u0866\u0867\3\2\2\2\u0867\u0868\7")
        buf.write("\4\2\2\u0868\u0869\7\t\2\2\u0869\u0877\5\u008eH\2\u086a")
        buf.write("\u0877\5\u00c2b\2\u086b\u086c\7\3\2\2\u086c\u086d\5\u008e")
        buf.write("H\2\u086d\u086e\7\4\2\2\u086e\u0877\3\2\2\2\u086f\u0870")
        buf.write("\7\u0083\2\2\u0870\u0871\7\3\2\2\u0871\u0872\5\u00c2b")
        buf.write("\2\u0872\u0873\7\16\2\2\u0873\u0874\5\u0094K\2\u0874\u0875")
        buf.write("\7\4\2\2\u0875\u0877\3\2\2\2\u0876\u07e5\3\2\2\2\u0876")
        buf.write("\u07f2\3\2\2\2\u0876\u07ff\3\2\2\2\u0876\u0806\3\2\2\2")
        buf.write("\u0876\u0813\3\2\2\2\u0876\u081c\3\2\2\2\u0876\u0825\3")
        buf.write("\2\2\2\u0876\u082c\3\2\2\2\u0876\u082d\3\2\2\2\u0876\u082e")
        buf.write("\3\2\2\2\u0876\u0832\3\2\2\2\u0876\u083c\3\2\2\2\u0876")
        buf.write("\u0840\3\2\2\2\u0876\u0854\3\2\2\2\u0876\u085c\3\2\2\2")
        buf.write("\u0876\u085f\3\2\2\2\u0876\u086a\3\2\2\2\u0876\u086b\3")
        buf.write("\2\2\2\u0876\u086f\3\2\2\2\u0877\u0882\3\2\2\2\u0878\u0879")
        buf.write("\f\7\2\2\u0879\u087a\7\n\2\2\u087a\u087b\5\u0094K\2\u087b")
        buf.write("\u087c\7\13\2\2\u087c\u0881\3\2\2\2\u087d\u087e\f\5\2")
        buf.write("\2\u087e\u087f\7\6\2\2\u087f\u0881\5\u00c2b\2\u0880\u0878")
        buf.write("\3\2\2\2\u0880\u087d\3\2\2\2\u0881\u0884\3\2\2\2\u0882")
        buf.write("\u0880\3\2\2\2\u0882\u0883\3\2\2\2\u0883\u0097\3\2\2\2")
        buf.write("\u0884\u0882\3\2\2\2\u0885\u0892\7)\2\2\u0886\u0892\5")
        buf.write("\u00a2R\2\u0887\u0888\5\u00c2b\2\u0888\u0889\7\u00f2\2")
        buf.write("\2\u0889\u0892\3\2\2\2\u088a\u0892\5\u00c8e\2\u088b\u0892")
        buf.write("\5\u00a0Q\2\u088c\u088e\7\u00f2\2\2\u088d\u088c\3\2\2")
        buf.write("\2\u088e\u088f\3\2\2\2\u088f\u088d\3\2\2\2\u088f\u0890")
        buf.write("\3\2\2\2\u0890\u0892\3\2\2\2\u0891\u0885\3\2\2\2\u0891")
        buf.write("\u0886\3\2\2\2\u0891\u0887\3\2\2\2\u0891\u088a\3\2\2\2")
        buf.write("\u0891\u088b\3\2\2\2\u0891\u088d\3\2\2\2\u0892\u0099\3")
        buf.write("\2\2\2\u0893\u0894\t\24\2\2\u0894\u009b\3\2\2\2\u0895")
        buf.write("\u0896\t\25\2\2\u0896\u009d\3\2\2\2\u0897\u0898\t\26\2")
        buf.write("\2\u0898\u009f\3\2\2\2\u0899\u089a\t\27\2\2\u089a\u00a1")
        buf.write("\3\2\2\2\u089b\u089f\7\60\2\2\u089c\u089e\5\u00a4S\2\u089d")
        buf.write("\u089c\3\2\2\2\u089e\u08a1\3\2\2\2\u089f\u089d\3\2\2\2")
        buf.write("\u089f\u08a0\3\2\2\2\u08a0\u00a3\3\2\2\2\u08a1\u089f\3")
        buf.write("\2\2\2\u08a2\u08a3\5\u00a6T\2\u08a3\u08a6\5\u00c2b\2\u08a4")
        buf.write("\u08a5\7l\2\2\u08a5\u08a7\5\u00c2b\2\u08a6\u08a4\3\2\2")
        buf.write("\2\u08a6\u08a7\3\2\2\2\u08a7\u00a5\3\2\2\2\u08a8\u08aa")
        buf.write("\t\30\2\2\u08a9\u08a8\3\2\2\2\u08a9\u08aa\3\2\2\2\u08aa")
        buf.write("\u08ab\3\2\2\2\u08ab\u08ae\t\16\2\2\u08ac\u08ae\7\u00f2")
        buf.write("\2\2\u08ad\u08a9\3\2\2\2\u08ad\u08ac\3\2\2\2\u08ae\u00a7")
        buf.write("\3\2\2\2\u08af\u08b3\7K\2\2\u08b0\u08b1\7L\2\2\u08b1\u08b3")
        buf.write("\5\u00c2b\2\u08b2\u08af\3\2\2\2\u08b2\u08b0\3\2\2\2\u08b3")
        buf.write("\u00a9\3\2\2\2\u08b4\u08b5\7q\2\2\u08b5\u08b6\7\u0088")
        buf.write("\2\2\u08b6\u08b7\5\u00aaV\2\u08b7\u08b8\7\u008a\2\2\u08b8")
        buf.write("\u08d7\3\2\2\2\u08b9\u08ba\7r\2\2\u08ba\u08bb\7\u0088")
        buf.write("\2\2\u08bb\u08bc\5\u00aaV\2\u08bc\u08bd\7\5\2\2\u08bd")
        buf.write("\u08be\5\u00aaV\2\u08be\u08bf\7\u008a\2\2\u08bf\u08d7")
        buf.write("\3\2\2\2\u08c0\u08c7\7s\2\2\u08c1\u08c3\7\u0088\2\2\u08c2")
        buf.write("\u08c4\5\u00b0Y\2\u08c3\u08c2\3\2\2\2\u08c3\u08c4\3\2")
        buf.write("\2\2\u08c4\u08c5\3\2\2\2\u08c5\u08c8\7\u008a\2\2\u08c6")
        buf.write("\u08c8\7\u0086\2\2\u08c7\u08c1\3\2\2\2\u08c7\u08c6\3\2")
        buf.write("\2\2\u08c8\u08d7\3\2\2\2\u08c9\u08d4\5\u00c2b\2\u08ca")
        buf.write("\u08cb\7\3\2\2\u08cb\u08d0\7\u00f6\2\2\u08cc\u08cd\7\5")
        buf.write("\2\2\u08cd\u08cf\7\u00f6\2\2\u08ce\u08cc\3\2\2\2\u08cf")
        buf.write("\u08d2\3\2\2\2\u08d0\u08ce\3\2\2\2\u08d0\u08d1\3\2\2\2")
        buf.write("\u08d1\u08d3\3\2\2\2\u08d2\u08d0\3\2\2\2\u08d3\u08d5\7")
        buf.write("\4\2\2\u08d4\u08ca\3\2\2\2\u08d4\u08d5\3\2\2\2\u08d5\u08d7")
        buf.write("\3\2\2\2\u08d6\u08b4\3\2\2\2\u08d6\u08b9\3\2\2\2\u08d6")
        buf.write("\u08c0\3\2\2\2\u08d6\u08c9\3\2\2\2\u08d7\u00ab\3\2\2\2")
        buf.write("\u08d8\u08dd\5\u00aeX\2\u08d9\u08da\7\5\2\2\u08da\u08dc")
        buf.write("\5\u00aeX\2\u08db\u08d9\3\2\2\2\u08dc\u08df\3\2\2\2\u08dd")
        buf.write("\u08db\3\2\2\2\u08dd\u08de\3\2\2\2\u08de\u00ad\3\2\2\2")
        buf.write("\u08df\u08dd\3\2\2\2\u08e0\u08e1\5\u00c2b\2\u08e1\u08e4")
        buf.write("\5\u00aaV\2\u08e2\u08e3\7t\2\2\u08e3\u08e5\7\u00f2\2\2")
        buf.write("\u08e4\u08e2\3\2\2\2\u08e4\u08e5\3\2\2\2\u08e5\u00af\3")
        buf.write("\2\2\2\u08e6\u08eb\5\u00b2Z\2\u08e7\u08e8\7\5\2\2\u08e8")
        buf.write("\u08ea\5\u00b2Z\2\u08e9\u08e7\3\2\2\2\u08ea\u08ed\3\2")
        buf.write("\2\2\u08eb\u08e9\3\2\2\2\u08eb\u08ec\3\2\2\2\u08ec\u00b1")
        buf.write("\3\2\2\2\u08ed\u08eb\3\2\2\2\u08ee\u08ef\5\u00c2b\2\u08ef")
        buf.write("\u08f0\7\f\2\2\u08f0\u08f3\5\u00aaV\2\u08f1\u08f2\7t\2")
        buf.write("\2\u08f2\u08f4\7\u00f2\2\2\u08f3\u08f1\3\2\2\2\u08f3\u08f4")
        buf.write("\3\2\2\2\u08f4\u00b3\3\2\2\2\u08f5\u08f6\7\62\2\2\u08f6")
        buf.write("\u08f7\5\u008eH\2\u08f7\u08f8\7\63\2\2\u08f8\u08f9\5\u008e")
        buf.write("H\2\u08f9\u00b5\3\2\2\2\u08fa\u08fb\7B\2\2\u08fb\u0900")
        buf.write("\5\u00b8]\2\u08fc\u08fd\7\5\2\2\u08fd\u08ff\5\u00b8]\2")
        buf.write("\u08fe\u08fc\3\2\2\2\u08ff\u0902\3\2\2\2\u0900\u08fe\3")
        buf.write("\2\2\2\u0900\u0901\3\2\2\2\u0901\u00b7\3\2\2\2\u0902\u0900")
        buf.write("\3\2\2\2\u0903\u0904\5\u00c2b\2\u0904\u0905\7\20\2\2\u0905")
        buf.write("\u0906\5\u00ba^\2\u0906\u00b9\3\2\2\2\u0907\u0932\5\u00c2")
        buf.write("b\2\u0908\u092b\7\3\2\2\u0909\u090a\7\u009c\2\2\u090a")
        buf.write("\u090b\7\26\2\2\u090b\u0910\5\u008eH\2\u090c\u090d\7\5")
        buf.write("\2\2\u090d\u090f\5\u008eH\2\u090e\u090c\3\2\2\2\u090f")
        buf.write("\u0912\3\2\2\2\u0910\u090e\3\2\2\2\u0910\u0911\3\2\2\2")
        buf.write("\u0911\u092c\3\2\2\2\u0912\u0910\3\2\2\2\u0913\u0914\t")
        buf.write("\31\2\2\u0914\u0915\7\26\2\2\u0915\u091a\5\u008eH\2\u0916")
        buf.write("\u0917\7\5\2\2\u0917\u0919\5\u008eH\2\u0918\u0916\3\2")
        buf.write("\2\2\u0919\u091c\3\2\2\2\u091a\u0918\3\2\2\2\u091a\u091b")
        buf.write("\3\2\2\2\u091b\u091e\3\2\2\2\u091c\u091a\3\2\2\2\u091d")
        buf.write("\u0913\3\2\2\2\u091d\u091e\3\2\2\2\u091e\u0929\3\2\2\2")
        buf.write("\u091f\u0920\t\32\2\2\u0920\u0921\7\26\2\2\u0921\u0926")
        buf.write("\5L\'\2\u0922\u0923\7\5\2\2\u0923\u0925\5L\'\2\u0924\u0922")
        buf.write("\3\2\2\2\u0925\u0928\3\2\2\2\u0926\u0924\3\2\2\2\u0926")
        buf.write("\u0927\3\2\2\2\u0927\u092a\3\2\2\2\u0928\u0926\3\2\2\2")
        buf.write("\u0929\u091f\3\2\2\2\u0929\u092a\3\2\2\2\u092a\u092c\3")
        buf.write("\2\2\2\u092b\u0909\3\2\2\2\u092b\u091d\3\2\2\2\u092c\u092e")
        buf.write("\3\2\2\2\u092d\u092f\5\u00bc_\2\u092e\u092d\3\2\2\2\u092e")
        buf.write("\u092f\3\2\2\2\u092f\u0930\3\2\2\2\u0930\u0932\7\4\2\2")
        buf.write("\u0931\u0907\3\2\2\2\u0931\u0908\3\2\2\2\u0932\u00bb\3")
        buf.write("\2\2\2\u0933\u0934\7E\2\2\u0934\u0944\5\u00be`\2\u0935")
        buf.write("\u0936\7F\2\2\u0936\u0944\5\u00be`\2\u0937\u0938\7E\2")
        buf.write("\2\u0938\u0939\7%\2\2\u0939\u093a\5\u00be`\2\u093a\u093b")
        buf.write("\7 \2\2\u093b\u093c\5\u00be`\2\u093c\u0944\3\2\2\2\u093d")
        buf.write("\u093e\7F\2\2\u093e\u093f\7%\2\2\u093f\u0940\5\u00be`")
        buf.write("\2\u0940\u0941\7 \2\2\u0941\u0942\5\u00be`\2\u0942\u0944")
        buf.write("\3\2\2\2\u0943\u0933\3\2\2\2\u0943\u0935\3\2\2\2\u0943")
        buf.write("\u0937\3\2\2\2\u0943\u093d\3\2\2\2\u0944\u00bd\3\2\2\2")
        buf.write("\u0945\u0946\7G\2\2\u0946\u094d\t\33\2\2\u0947\u0948\7")
        buf.write("J\2\2\u0948\u094d\7N\2\2\u0949\u094a\5\u008eH\2\u094a")
        buf.write("\u094b\t\33\2\2\u094b\u094d\3\2\2\2\u094c\u0945\3\2\2")
        buf.write("\2\u094c\u0947\3\2\2\2\u094c\u0949\3\2\2\2\u094d\u00bf")
        buf.write("\3\2\2\2\u094e\u0953\5\u00c2b\2\u094f\u0950\7\6\2\2\u0950")
        buf.write("\u0952\5\u00c2b\2\u0951\u094f\3\2\2\2\u0952\u0955\3\2")
        buf.write("\2\2\u0953\u0951\3\2\2\2\u0953\u0954\3\2\2\2\u0954\u00c1")
        buf.write("\3\2\2\2\u0955\u0953\3\2\2\2\u0956\u0966\5\u00c4c\2\u0957")
        buf.write("\u0966\7\u00ef\2\2\u0958\u0966\7=\2\2\u0959\u0966\79\2")
        buf.write("\2\u095a\u0966\7:\2\2\u095b\u0966\7;\2\2\u095c\u0966\7")
        buf.write("<\2\2\u095d\u0966\7>\2\2\u095e\u0966\7\66\2\2\u095f\u0966")
        buf.write("\7\67\2\2\u0960\u0966\7?\2\2\u0961\u0966\7h\2\2\u0962")
        buf.write("\u0966\7k\2\2\u0963\u0966\7i\2\2\u0964\u0966\7j\2\2\u0965")
        buf.write("\u0956\3\2\2\2\u0965\u0957\3\2\2\2\u0965\u0958\3\2\2\2")
        buf.write("\u0965\u0959\3\2\2\2\u0965\u095a\3\2\2\2\u0965\u095b\3")
        buf.write("\2\2\2\u0965\u095c\3\2\2\2\u0965\u095d\3\2\2\2\u0965\u095e")
        buf.write("\3\2\2\2\u0965\u095f\3\2\2\2\u0965\u0960\3\2\2\2\u0965")
        buf.write("\u0961\3\2\2\2\u0965\u0962\3\2\2\2\u0965\u0963\3\2\2\2")
        buf.write("\u0965\u0964\3\2\2\2\u0966\u00c3\3\2\2\2\u0967\u096b\7")
        buf.write("\u00fa\2\2\u0968\u096b\5\u00c6d\2\u0969\u096b\5\u00ca")
        buf.write("f\2\u096a\u0967\3\2\2\2\u096a\u0968\3\2\2\2\u096a\u0969")
        buf.write("\3\2\2\2\u096b\u00c5\3\2\2\2\u096c\u096d\7\u00fb\2\2\u096d")
        buf.write("\u00c7\3\2\2\2\u096e\u0970\7\u008d\2\2\u096f\u096e\3\2")
        buf.write("\2\2\u096f\u0970\3\2\2\2\u0970\u0971\3\2\2\2\u0971\u098b")
        buf.write("\7\u00f7\2\2\u0972\u0974\7\u008d\2\2\u0973\u0972\3\2\2")
        buf.write("\2\u0973\u0974\3\2\2\2\u0974\u0975\3\2\2\2\u0975\u098b")
        buf.write("\7\u00f6\2\2\u0976\u0978\7\u008d\2\2\u0977\u0976\3\2\2")
        buf.write("\2\u0977\u0978\3\2\2\2\u0978\u0979\3\2\2\2\u0979\u098b")
        buf.write("\7\u00f3\2\2\u097a\u097c\7\u008d\2\2\u097b\u097a\3\2\2")
        buf.write("\2\u097b\u097c\3\2\2\2\u097c\u097d\3\2\2\2\u097d\u098b")
        buf.write("\7\u00f4\2\2\u097e\u0980\7\u008d\2\2\u097f\u097e\3\2\2")
        buf.write("\2\u097f\u0980\3\2\2\2\u0980\u0981\3\2\2\2\u0981\u098b")
        buf.write("\7\u00f5\2\2\u0982\u0984\7\u008d\2\2\u0983\u0982\3\2\2")
        buf.write("\2\u0983\u0984\3\2\2\2\u0984\u0985\3\2\2\2\u0985\u098b")
        buf.write("\7\u00f8\2\2\u0986\u0988\7\u008d\2\2\u0987\u0986\3\2\2")
        buf.write("\2\u0987\u0988\3\2\2\2\u0988\u0989\3\2\2\2\u0989\u098b")
        buf.write("\7\u00f9\2\2\u098a\u096f\3\2\2\2\u098a\u0973\3\2\2\2\u098a")
        buf.write("\u0977\3\2\2\2\u098a\u097b\3\2\2\2\u098a\u097f\3\2\2\2")
        buf.write("\u098a\u0983\3\2\2\2\u098a\u0987\3\2\2\2\u098b\u00c9\3")
        buf.write("\2\2\2\u098c\u098d\t\34\2\2\u098d\u00cb\3\2\2\2\u0148")
        buf.write("\u00e6\u00eb\u00ee\u00f3\u0100\u0104\u010b\u0119\u011b")
        buf.write("\u011f\u0122\u0129\u013a\u013c\u0140\u0143\u014a\u0150")
        buf.write("\u0156\u0161\u0181\u0189\u018d\u0192\u0198\u01a0\u01a6")
        buf.write("\u01b3\u01b8\u01c1\u01c6\u01d6\u01dd\u01e1\u01e9\u01f0")
        buf.write("\u01f7\u0206\u020a\u0210\u0216\u0219\u021c\u0222\u0226")
        buf.write("\u022a\u022f\u0233\u023b\u023e\u0247\u024c\u0252\u0259")
        buf.write("\u025c\u0262\u026d\u0270\u0274\u0279\u027e\u0285\u0288")
        buf.write("\u028b\u0292\u0297\u029c\u029f\u02a8\u02b0\u02b6\u02ba")
        buf.write("\u02be\u02c2\u02c4\u02cd\u02d3\u02d8\u02db\u02df\u02e2")
        buf.write("\u02ec\u02ef\u02f3\u02f9\u02fc\u02ff\u0305\u030d\u0312")
        buf.write("\u0318\u031e\u0329\u0331\u0338\u0340\u0343\u034b\u034f")
        buf.write("\u0356\u03ca\u03d2\u03da\u03e3\u03ef\u03f3\u03f6\u03fc")
        buf.write("\u0406\u0412\u0417\u041d\u0429\u042b\u0430\u0434\u0439")
        buf.write("\u043e\u0441\u0446\u044a\u044f\u0451\u0455\u045e\u0466")
        buf.write("\u046d\u0474\u047d\u0482\u0491\u0498\u049b\u04a2\u04a6")
        buf.write("\u04ac\u04b4\u04bf\u04ca\u04d1\u04d7\u04dd\u04e6\u04e8")
        buf.write("\u04f1\u04f4\u04fd\u0500\u0509\u050c\u0515\u0518\u051b")
        buf.write("\u0520\u0522\u0525\u0531\u0538\u053f\u0542\u0544\u054f")
        buf.write("\u0553\u0557\u0563\u0566\u056a\u0574\u0578\u057a\u057d")
        buf.write("\u0581\u0584\u0588\u058e\u0592\u0596\u059b\u059e\u05a0")
        buf.write("\u05a5\u05aa\u05ad\u05b1\u05b4\u05b6\u05bb\u05c0\u05cd")
        buf.write("\u05d2\u05da\u05e0\u05e4\u05ed\u05fc\u0601\u060d\u0612")
        buf.write("\u061a\u061d\u0621\u062f\u063c\u0641\u0645\u0648\u064d")
        buf.write("\u0656\u0659\u065e\u0665\u0668\u0670\u0677\u067e\u0681")
        buf.write("\u0686\u068c\u0690\u0693\u0696\u069c\u06a1\u06a6\u06b8")
        buf.write("\u06ba\u06bd\u06c8\u06d1\u06d8\u06e0\u06e8\u06ec\u06f4")
        buf.write("\u06fc\u0702\u070a\u0716\u0719\u071f\u0723\u0725\u072e")
        buf.write("\u073a\u073c\u0743\u074a\u0750\u0756\u0758\u075d\u0764")
        buf.write("\u076a\u076e\u0770\u0777\u0786\u0788\u0790\u0792\u0796")
        buf.write("\u079e\u07a7\u07ad\u07b5\u07bb\u07c0\u07c5\u07cb\u07e0")
        buf.write("\u07e2\u07ea\u07ee\u07f7\u07fb\u080d\u0810\u0818\u0821")
        buf.write("\u0838\u0843\u084a\u084d\u0852\u0865\u0876\u0880\u0882")
        buf.write("\u088f\u0891\u089f\u08a6\u08a9\u08ad\u08b2\u08c3\u08c7")
        buf.write("\u08d0\u08d4\u08d6\u08dd\u08e4\u08eb\u08f3\u0900\u0910")
        buf.write("\u091a\u091d\u0926\u0929\u092b\u092e\u0931\u0943\u094c")
        buf.write("\u0953\u0965\u096a\u096f\u0973\u0977\u097b\u097f\u0983")
        buf.write("\u0987\u098a")
        return buf.getvalue()


class SparkSqlBaseParser ( Parser ):

    grammarFileName = "SparkSqlBase.g4"

    atn = ATNDeserializer().deserialize(serializedATN())

    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]

    sharedContextCache = PredictionContextCache()

    literalNames = [ "<INVALID>", "'('", "')'", "','", "'.'", "'/*+'", "'*/'", 
                     "'->'", "'['", "']'", "':'", "'SELECT'", "'FROM'", 
                     "'ADD'", "'AS'", "'ALL'", "'ANY'", "'DISTINCT'", "'WHERE'", 
                     "'GROUP'", "'BY'", "'GROUPING'", "'SETS'", "'CUBE'", 
                     "'ROLLUP'", "'ORDER'", "'HAVING'", "'LIMIT'", "'AT'", 
                     "'OR'", "'AND'", "'IN'", "<INVALID>", "'NO'", "'EXISTS'", 
                     "'BETWEEN'", "'LIKE'", "<INVALID>", "'IS'", "'NULL'", 
                     "'TRUE'", "'FALSE'", "'NULLS'", "'ASC'", "'DESC'", 
                     "'FOR'", "'INTERVAL'", "'CASE'", "'WHEN'", "'THEN'", 
                     "'ELSE'", "'END'", "'JOIN'", "'CROSS'", "'OUTER'", 
                     "'INNER'", "'LEFT'", "'SEMI'", "'RIGHT'", "'FULL'", 
                     "'NATURAL'", "'ON'", "'PIVOT'", "'LATERAL'", "'WINDOW'", 
                     "'OVER'", "'PARTITION'", "'RANGE'", "'ROWS'", "'UNBOUNDED'", 
                     "'PRECEDING'", "'FOLLOWING'", "'CURRENT'", "'FIRST'", 
                     "'AFTER'", "'LAST'", "'ROW'", "'WITH'", "'VALUES'", 
                     "'CREATE'", "'TABLE'", "'DIRECTORY'", "'VIEW'", "'REPLACE'", 
                     "'INSERT'", "'DELETE'", "'INTO'", "'DESCRIBE'", "'EXPLAIN'", 
                     "'FORMAT'", "'LOGICAL'", "'CODEGEN'", "'COST'", "'CAST'", 
                     "'SHOW'", "'TABLES'", "'COLUMNS'", "'COLUMN'", "'USE'", 
                     "'PARTITIONS'", "'FUNCTIONS'", "'DROP'", "'UNION'", 
                     "'EXCEPT'", "'MINUS'", "'INTERSECT'", "'TO'", "'TABLESAMPLE'", 
                     "'STRATIFY'", "'ALTER'", "'RENAME'", "'ARRAY'", "'MAP'", 
                     "'STRUCT'", "'COMMENT'", "'SET'", "'RESET'", "'DATA'", 
                     "'START'", "'TRANSACTION'", "'COMMIT'", "'ROLLBACK'", 
                     "'MACRO'", "'IGNORE'", "'BOTH'", "'LEADING'", "'TRAILING'", 
                     "'IF'", "'POSITION'", "'EXTRACT'", "<INVALID>", "'<=>'", 
                     "'<>'", "'!='", "'<'", "<INVALID>", "'>'", "<INVALID>", 
                     "'+'", "'-'", "'*'", "'/'", "'%'", "'DIV'", "'~'", 
                     "'&'", "'|'", "'||'", "'^'", "'PERCENT'", "'BUCKET'", 
                     "'OUT'", "'OF'", "'SORT'", "'CLUSTER'", "'DISTRIBUTE'", 
                     "'OVERWRITE'", "'TRANSFORM'", "'REDUCE'", "'USING'", 
                     "'SERDE'", "'SERDEPROPERTIES'", "'RECORDREADER'", "'RECORDWRITER'", 
                     "'DELIMITED'", "'FIELDS'", "'TERMINATED'", "'COLLECTION'", 
                     "'ITEMS'", "'KEYS'", "'ESCAPED'", "'LINES'", "'SEPARATED'", 
                     "'FUNCTION'", "'EXTENDED'", "'REFRESH'", "'CLEAR'", 
                     "'CACHE'", "'UNCACHE'", "'LAZY'", "'FORMATTED'", "'GLOBAL'", 
                     "<INVALID>", "'OPTIONS'", "'UNSET'", "'TBLPROPERTIES'", 
                     "'DBPROPERTIES'", "'BUCKETS'", "'SKEWED'", "'STORED'", 
                     "'DIRECTORIES'", "'LOCATION'", "'EXCHANGE'", "'ARCHIVE'", 
                     "'UNARCHIVE'", "'FILEFORMAT'", "'TOUCH'", "'COMPACT'", 
                     "'CONCATENATE'", "'CHANGE'", "'CASCADE'", "'RESTRICT'", 
                     "'CLUSTERED'", "'SORTED'", "'PURGE'", "'INPUTFORMAT'", 
                     "'OUTPUTFORMAT'", "<INVALID>", "<INVALID>", "'DFS'", 
                     "'TRUNCATE'", "'ANALYZE'", "'COMPUTE'", "'LIST'", "'STATISTICS'", 
                     "'PARTITIONED'", "'EXTERNAL'", "'DEFINED'", "'REVOKE'", 
                     "'GRANT'", "'LOCK'", "'UNLOCK'", "'MSCK'", "'REPAIR'", 
                     "'RECOVER'", "'EXPORT'", "'IMPORT'", "'LOAD'", "'ROLE'", 
                     "'ROLES'", "'COMPACTIONS'", "'PRINCIPALS'", "'TRANSACTIONS'", 
                     "'INDEX'", "'INDEXES'", "'LOCKS'", "'OPTION'", "'ANTI'", 
                     "'LOCAL'", "'INPATH'", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "'/**/'" ]

    symbolicNames = [ "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                      "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                      "<INVALID>", "<INVALID>", "<INVALID>", "SELECT", "FROM", 
                      "ADD", "AS", "ALL", "ANY", "DISTINCT", "WHERE", "GROUP", 
                      "BY", "GROUPING", "SETS", "CUBE", "ROLLUP", "ORDER", 
                      "HAVING", "LIMIT", "AT", "OR", "AND", "IN", "NOT", 
                      "NO", "EXISTS", "BETWEEN", "LIKE", "RLIKE", "IS", 
                      "NULL", "TRUE", "FALSE", "NULLS", "ASC", "DESC", "FOR", 
                      "INTERVAL", "CASE", "WHEN", "THEN", "ELSE", "END", 
                      "JOIN", "CROSS", "OUTER", "INNER", "LEFT", "SEMI", 
                      "RIGHT", "FULL", "NATURAL", "ON", "PIVOT", "LATERAL", 
                      "WINDOW", "OVER", "PARTITION", "RANGE", "ROWS", "UNBOUNDED", 
                      "PRECEDING", "FOLLOWING", "CURRENT", "FIRST", "AFTER", 
                      "LAST", "ROW", "WITH", "VALUES", "CREATE", "TABLE", 
                      "DIRECTORY", "VIEW", "REPLACE", "INSERT", "DELETE", 
                      "INTO", "DESCRIBE", "EXPLAIN", "FORMAT", "LOGICAL", 
                      "CODEGEN", "COST", "CAST", "SHOW", "TABLES", "COLUMNS", 
                      "COLUMN", "USE", "PARTITIONS", "FUNCTIONS", "DROP", 
                      "UNION", "EXCEPT", "SETMINUS", "INTERSECT", "TO", 
                      "TABLESAMPLE", "STRATIFY", "ALTER", "RENAME", "ARRAY", 
                      "MAP", "STRUCT", "COMMENT", "SET", "RESET", "DATA", 
                      "START", "TRANSACTION", "COMMIT", "ROLLBACK", "MACRO", 
                      "IGNORE", "BOTH", "LEADING", "TRAILING", "IF", "POSITION", 
                      "EXTRACT", "EQ", "NSEQ", "NEQ", "NEQJ", "LT", "LTE", 
                      "GT", "GTE", "PLUS", "MINUS", "ASTERISK", "SLASH", 
                      "PERCENT", "DIV", "TILDE", "AMPERSAND", "PIPE", "CONCAT_PIPE", 
                      "HAT", "PERCENTLIT", "BUCKET", "OUT", "OF", "SORT", 
                      "CLUSTER", "DISTRIBUTE", "OVERWRITE", "TRANSFORM", 
                      "REDUCE", "USING", "SERDE", "SERDEPROPERTIES", "RECORDREADER", 
                      "RECORDWRITER", "DELIMITED", "FIELDS", "TERMINATED", 
                      "COLLECTION", "ITEMS", "KEYS", "ESCAPED", "LINES", 
                      "SEPARATED", "FUNCTION", "EXTENDED", "REFRESH", "CLEAR", 
                      "CACHE", "UNCACHE", "LAZY", "FORMATTED", "GLOBAL", 
                      "TEMPORARY", "OPTIONS", "UNSET", "TBLPROPERTIES", 
                      "DBPROPERTIES", "BUCKETS", "SKEWED", "STORED", "DIRECTORIES", 
                      "LOCATION", "EXCHANGE", "ARCHIVE", "UNARCHIVE", "FILEFORMAT", 
                      "TOUCH", "COMPACT", "CONCATENATE", "CHANGE", "CASCADE", 
                      "RESTRICT", "CLUSTERED", "SORTED", "PURGE", "INPUTFORMAT", 
                      "OUTPUTFORMAT", "DATABASE", "DATABASES", "DFS", "TRUNCATE", 
                      "ANALYZE", "COMPUTE", "LIST", "STATISTICS", "PARTITIONED", 
                      "EXTERNAL", "DEFINED", "REVOKE", "GRANT", "LOCK", 
                      "UNLOCK", "MSCK", "REPAIR", "RECOVER", "EXPORT", "IMPORT", 
                      "LOAD", "ROLE", "ROLES", "COMPACTIONS", "PRINCIPALS", 
                      "TRANSACTIONS", "INDEX", "INDEXES", "LOCKS", "OPTION", 
                      "ANTI", "LOCAL", "INPATH", "STRING", "BIGINT_LITERAL", 
                      "SMALLINT_LITERAL", "TINYINT_LITERAL", "INTEGER_VALUE", 
                      "DECIMAL_VALUE", "DOUBLE_LITERAL", "BIGDECIMAL_LITERAL", 
                      "IDENTIFIER", "BACKQUOTED_IDENTIFIER", "SIMPLE_COMMENT", 
                      "BRACKETED_EMPTY_COMMENT", "BRACKETED_COMMENT", "WS", 
                      "UNRECOGNIZED" ]

    RULE_singleStatement = 0
    RULE_singleExpression = 1
    RULE_singleTableIdentifier = 2
    RULE_singleFunctionIdentifier = 3
    RULE_singleDataType = 4
    RULE_singleTableSchema = 5
    RULE_statement = 6
    RULE_unsupportedHiveNativeCommands = 7
    RULE_createTableHeader = 8
    RULE_bucketSpec = 9
    RULE_skewSpec = 10
    RULE_locationSpec = 11
    RULE_query = 12
    RULE_insertInto = 13
    RULE_partitionSpecLocation = 14
    RULE_partitionSpec = 15
    RULE_partitionVal = 16
    RULE_describeFuncName = 17
    RULE_describeColName = 18
    RULE_ctes = 19
    RULE_namedQuery = 20
    RULE_tableProvider = 21
    RULE_tablePropertyList = 22
    RULE_tableProperty = 23
    RULE_tablePropertyKey = 24
    RULE_tablePropertyValue = 25
    RULE_constantList = 26
    RULE_nestedConstantList = 27
    RULE_createFileFormat = 28
    RULE_fileFormat = 29
    RULE_storageHandler = 30
    RULE_resource = 31
    RULE_queryNoWith = 32
    RULE_queryOrganization = 33
    RULE_multiInsertQueryBody = 34
    RULE_queryTerm = 35
    RULE_queryPrimary = 36
    RULE_sortItem = 37
    RULE_querySpecification = 38
    RULE_hint = 39
    RULE_hintStatement = 40
    RULE_fromClause = 41
    RULE_aggregation = 42
    RULE_groupingSet = 43
    RULE_pivotClause = 44
    RULE_pivotColumn = 45
    RULE_pivotValue = 46
    RULE_lateralView = 47
    RULE_setQuantifier = 48
    RULE_relation = 49
    RULE_joinRelation = 50
    RULE_joinType = 51
    RULE_joinCriteria = 52
    RULE_sample = 53
    RULE_sampleMethod = 54
    RULE_identifierList = 55
    RULE_identifierSeq = 56
    RULE_orderedIdentifierList = 57
    RULE_orderedIdentifier = 58
    RULE_identifierCommentList = 59
    RULE_identifierComment = 60
    RULE_relationPrimary = 61
    RULE_inlineTable = 62
    RULE_functionTable = 63
    RULE_tableAlias = 64
    RULE_rowFormat = 65
    RULE_tableIdentifier = 66
    RULE_functionIdentifier = 67
    RULE_namedExpression = 68
    RULE_namedExpressionSeq = 69
    RULE_expression = 70
    RULE_booleanExpression = 71
    RULE_predicate = 72
    RULE_valueExpression = 73
    RULE_primaryExpression = 74
    RULE_constant = 75
    RULE_comparisonOperator = 76
    RULE_arithmeticOperator = 77
    RULE_predicateOperator = 78
    RULE_booleanValue = 79
    RULE_interval = 80
    RULE_intervalField = 81
    RULE_intervalValue = 82
    RULE_colPosition = 83
    RULE_dataType = 84
    RULE_colTypeList = 85
    RULE_colType = 86
    RULE_complex_ColTypeList = 87
    RULE_complex_ColType = 88
    RULE_whenClause = 89
    RULE_windows = 90
    RULE_namedWindow = 91
    RULE_windowSpec = 92
    RULE_windowFrame = 93
    RULE_frameBound = 94
    RULE_qualifiedName = 95
    RULE_identifier = 96
    RULE_strictIdentifier = 97
    RULE_quotedIdentifier = 98
    RULE_number = 99
    RULE_nonReserved = 100

    ruleNames =  [ "singleStatement", "singleExpression", "singleTableIdentifier", 
                   "singleFunctionIdentifier", "singleDataType", "singleTableSchema", 
                   "statement", "unsupportedHiveNativeCommands", "createTableHeader", 
                   "bucketSpec", "skewSpec", "locationSpec", "query", "insertInto", 
                   "partitionSpecLocation", "partitionSpec", "partitionVal", 
                   "describeFuncName", "describeColName", "ctes", "namedQuery", 
                   "tableProvider", "tablePropertyList", "tableProperty", 
                   "tablePropertyKey", "tablePropertyValue", "constantList", 
                   "nestedConstantList", "createFileFormat", "fileFormat", 
                   "storageHandler", "resource", "queryNoWith", "queryOrganization", 
                   "multiInsertQueryBody", "queryTerm", "queryPrimary", 
                   "sortItem", "querySpecification", "hint", "hintStatement", 
                   "fromClause", "aggregation", "groupingSet", "pivotClause", 
                   "pivotColumn", "pivotValue", "lateralView", "setQuantifier", 
                   "relation", "joinRelation", "joinType", "joinCriteria", 
                   "sample", "sampleMethod", "identifierList", "identifierSeq", 
                   "orderedIdentifierList", "orderedIdentifier", "identifierCommentList", 
                   "identifierComment", "relationPrimary", "inlineTable", 
                   "functionTable", "tableAlias", "rowFormat", "tableIdentifier", 
                   "functionIdentifier", "namedExpression", "namedExpressionSeq", 
                   "expression", "booleanExpression", "predicate", "valueExpression", 
                   "primaryExpression", "constant", "comparisonOperator", 
                   "arithmeticOperator", "predicateOperator", "booleanValue", 
                   "interval", "intervalField", "intervalValue", "colPosition", 
                   "dataType", "colTypeList", "colType", "complex_ColTypeList", 
                   "complex_ColType", "whenClause", "windows", "namedWindow", 
                   "windowSpec", "windowFrame", "frameBound", "qualifiedName", 
                   "identifier", "strictIdentifier", "quotedIdentifier", 
                   "number", "nonReserved" ]

    EOF = Token.EOF
    T__0=1
    T__1=2
    T__2=3
    T__3=4
    T__4=5
    T__5=6
    T__6=7
    T__7=8
    T__8=9
    T__9=10
    SELECT=11
    FROM=12
    ADD=13
    AS=14
    ALL=15
    ANY=16
    DISTINCT=17
    WHERE=18
    GROUP=19
    BY=20
    GROUPING=21
    SETS=22
    CUBE=23
    ROLLUP=24
    ORDER=25
    HAVING=26
    LIMIT=27
    AT=28
    OR=29
    AND=30
    IN=31
    NOT=32
    NO=33
    EXISTS=34
    BETWEEN=35
    LIKE=36
    RLIKE=37
    IS=38
    NULL=39
    TRUE=40
    FALSE=41
    NULLS=42
    ASC=43
    DESC=44
    FOR=45
    INTERVAL=46
    CASE=47
    WHEN=48
    THEN=49
    ELSE=50
    END=51
    JOIN=52
    CROSS=53
    OUTER=54
    INNER=55
    LEFT=56
    SEMI=57
    RIGHT=58
    FULL=59
    NATURAL=60
    ON=61
    PIVOT=62
    LATERAL=63
    WINDOW=64
    OVER=65
    PARTITION=66
    RANGE=67
    ROWS=68
    UNBOUNDED=69
    PRECEDING=70
    FOLLOWING=71
    CURRENT=72
    FIRST=73
    AFTER=74
    LAST=75
    ROW=76
    WITH=77
    VALUES=78
    CREATE=79
    TABLE=80
    DIRECTORY=81
    VIEW=82
    REPLACE=83
    INSERT=84
    DELETE=85
    INTO=86
    DESCRIBE=87
    EXPLAIN=88
    FORMAT=89
    LOGICAL=90
    CODEGEN=91
    COST=92
    CAST=93
    SHOW=94
    TABLES=95
    COLUMNS=96
    COLUMN=97
    USE=98
    PARTITIONS=99
    FUNCTIONS=100
    DROP=101
    UNION=102
    EXCEPT=103
    SETMINUS=104
    INTERSECT=105
    TO=106
    TABLESAMPLE=107
    STRATIFY=108
    ALTER=109
    RENAME=110
    ARRAY=111
    MAP=112
    STRUCT=113
    COMMENT=114
    SET=115
    RESET=116
    DATA=117
    START=118
    TRANSACTION=119
    COMMIT=120
    ROLLBACK=121
    MACRO=122
    IGNORE=123
    BOTH=124
    LEADING=125
    TRAILING=126
    IF=127
    POSITION=128
    EXTRACT=129
    EQ=130
    NSEQ=131
    NEQ=132
    NEQJ=133
    LT=134
    LTE=135
    GT=136
    GTE=137
    PLUS=138
    MINUS=139
    ASTERISK=140
    SLASH=141
    PERCENT=142
    DIV=143
    TILDE=144
    AMPERSAND=145
    PIPE=146
    CONCAT_PIPE=147
    HAT=148
    PERCENTLIT=149
    BUCKET=150
    OUT=151
    OF=152
    SORT=153
    CLUSTER=154
    DISTRIBUTE=155
    OVERWRITE=156
    TRANSFORM=157
    REDUCE=158
    USING=159
    SERDE=160
    SERDEPROPERTIES=161
    RECORDREADER=162
    RECORDWRITER=163
    DELIMITED=164
    FIELDS=165
    TERMINATED=166
    COLLECTION=167
    ITEMS=168
    KEYS=169
    ESCAPED=170
    LINES=171
    SEPARATED=172
    FUNCTION=173
    EXTENDED=174
    REFRESH=175
    CLEAR=176
    CACHE=177
    UNCACHE=178
    LAZY=179
    FORMATTED=180
    GLOBAL=181
    TEMPORARY=182
    OPTIONS=183
    UNSET=184
    TBLPROPERTIES=185
    DBPROPERTIES=186
    BUCKETS=187
    SKEWED=188
    STORED=189
    DIRECTORIES=190
    LOCATION=191
    EXCHANGE=192
    ARCHIVE=193
    UNARCHIVE=194
    FILEFORMAT=195
    TOUCH=196
    COMPACT=197
    CONCATENATE=198
    CHANGE=199
    CASCADE=200
    RESTRICT=201
    CLUSTERED=202
    SORTED=203
    PURGE=204
    INPUTFORMAT=205
    OUTPUTFORMAT=206
    DATABASE=207
    DATABASES=208
    DFS=209
    TRUNCATE=210
    ANALYZE=211
    COMPUTE=212
    LIST=213
    STATISTICS=214
    PARTITIONED=215
    EXTERNAL=216
    DEFINED=217
    REVOKE=218
    GRANT=219
    LOCK=220
    UNLOCK=221
    MSCK=222
    REPAIR=223
    RECOVER=224
    EXPORT=225
    IMPORT=226
    LOAD=227
    ROLE=228
    ROLES=229
    COMPACTIONS=230
    PRINCIPALS=231
    TRANSACTIONS=232
    INDEX=233
    INDEXES=234
    LOCKS=235
    OPTION=236
    ANTI=237
    LOCAL=238
    INPATH=239
    STRING=240
    BIGINT_LITERAL=241
    SMALLINT_LITERAL=242
    TINYINT_LITERAL=243
    INTEGER_VALUE=244
    DECIMAL_VALUE=245
    DOUBLE_LITERAL=246
    BIGDECIMAL_LITERAL=247
    IDENTIFIER=248
    BACKQUOTED_IDENTIFIER=249
    SIMPLE_COMMENT=250
    BRACKETED_EMPTY_COMMENT=251
    BRACKETED_COMMENT=252
    WS=253
    UNRECOGNIZED=254

    def __init__(self, input:TokenStream, output:TextIO = sys.stdout):
        super().__init__(input, output)
        self.checkVersion("4.7.1")
        self._interp = ParserATNSimulator(self, self.atn, self.decisionsToDFA, self.sharedContextCache)
        self._predicates = None



        '''
      /**
       * When false, INTERSECT is given the greater precedence over the other set
       * operations (UNION, EXCEPT and MINUS) as per the SQL standard.
       */
      public boolean False = false;

      /**
       * Verify whether current token is a valid decimal token (which contains dot).
       * Returns true if the character that follows the token is not a digit or letter or underscore.
       *
       * For example:
       * For char stream "2.3", "2." is not a valid decimal token, because it is followed by digit '3'.
       * For char stream "2.3_", "2.3" is not a valid decimal token, because it is followed by '_'.
       * For char stream "2.3W", "2.3" is not a valid decimal token, because it is followed by 'W'.
       * For char stream "12.0D 34.E2+0.12 "  12.0D is a valid decimal token because it is followed
       * by a space. 34.E2 is a valid decimal token because it is followed by symbol '+'
       * which is not a digit or letter or underscore.
       */
       '''
    #   public boolean isValidDecimal() {
    #     int nextChar = _input.LA(1);
    #     if (nextChar >= 'A' && nextChar <= 'Z' || nextChar >= '0' && nextChar <= '9' ||
    #       nextChar == '_') {
    #       return false;
    #     } else {
    #       return true;
    #     }
    #   }


    class SingleStatementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def statement(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.StatementContext,0)


        def EOF(self):
            return self.getToken(SparkSqlBaseParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_singleStatement

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleStatement" ):
                listener.enterSingleStatement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleStatement" ):
                listener.exitSingleStatement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleStatement" ):
                return visitor.visitSingleStatement(self)
            else:
                return visitor.visitChildren(self)




    def singleStatement(self):

        localctx = SparkSqlBaseParser.SingleStatementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 0, self.RULE_singleStatement)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 202
            self.statement()
            self.state = 203
            self.match(SparkSqlBaseParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SingleExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def namedExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.NamedExpressionContext,0)


        def EOF(self):
            return self.getToken(SparkSqlBaseParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_singleExpression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleExpression" ):
                listener.enterSingleExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleExpression" ):
                listener.exitSingleExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleExpression" ):
                return visitor.visitSingleExpression(self)
            else:
                return visitor.visitChildren(self)




    def singleExpression(self):

        localctx = SparkSqlBaseParser.SingleExpressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 2, self.RULE_singleExpression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 205
            self.namedExpression()
            self.state = 206
            self.match(SparkSqlBaseParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SingleTableIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)


        def EOF(self):
            return self.getToken(SparkSqlBaseParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_singleTableIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleTableIdentifier" ):
                listener.enterSingleTableIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleTableIdentifier" ):
                listener.exitSingleTableIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleTableIdentifier" ):
                return visitor.visitSingleTableIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def singleTableIdentifier(self):

        localctx = SparkSqlBaseParser.SingleTableIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 4, self.RULE_singleTableIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 208
            self.tableIdentifier()
            self.state = 209
            self.match(SparkSqlBaseParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SingleFunctionIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def functionIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.FunctionIdentifierContext,0)


        def EOF(self):
            return self.getToken(SparkSqlBaseParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_singleFunctionIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleFunctionIdentifier" ):
                listener.enterSingleFunctionIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleFunctionIdentifier" ):
                listener.exitSingleFunctionIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleFunctionIdentifier" ):
                return visitor.visitSingleFunctionIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def singleFunctionIdentifier(self):

        localctx = SparkSqlBaseParser.SingleFunctionIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 6, self.RULE_singleFunctionIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 211
            self.functionIdentifier()
            self.state = 212
            self.match(SparkSqlBaseParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SingleDataTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def dataType(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.DataTypeContext,0)


        def EOF(self):
            return self.getToken(SparkSqlBaseParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_singleDataType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleDataType" ):
                listener.enterSingleDataType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleDataType" ):
                listener.exitSingleDataType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleDataType" ):
                return visitor.visitSingleDataType(self)
            else:
                return visitor.visitChildren(self)




    def singleDataType(self):

        localctx = SparkSqlBaseParser.SingleDataTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 8, self.RULE_singleDataType)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 214
            self.dataType()
            self.state = 215
            self.match(SparkSqlBaseParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SingleTableSchemaContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def colTypeList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeListContext,0)


        def EOF(self):
            return self.getToken(SparkSqlBaseParser.EOF, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_singleTableSchema

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleTableSchema" ):
                listener.enterSingleTableSchema(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleTableSchema" ):
                listener.exitSingleTableSchema(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleTableSchema" ):
                return visitor.visitSingleTableSchema(self)
            else:
                return visitor.visitChildren(self)




    def singleTableSchema(self):

        localctx = SparkSqlBaseParser.SingleTableSchemaContext(self, self._ctx, self.state)
        self.enterRule(localctx, 10, self.RULE_singleTableSchema)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 217
            self.colTypeList()
            self.state = 218
            self.match(SparkSqlBaseParser.EOF)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class StatementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_statement

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class ExplainContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def EXPLAIN(self):
            return self.getToken(SparkSqlBaseParser.EXPLAIN, 0)
        def statement(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.StatementContext,0)

        def LOGICAL(self):
            return self.getToken(SparkSqlBaseParser.LOGICAL, 0)
        def FORMATTED(self):
            return self.getToken(SparkSqlBaseParser.FORMATTED, 0)
        def EXTENDED(self):
            return self.getToken(SparkSqlBaseParser.EXTENDED, 0)
        def CODEGEN(self):
            return self.getToken(SparkSqlBaseParser.CODEGEN, 0)
        def COST(self):
            return self.getToken(SparkSqlBaseParser.COST, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExplain" ):
                listener.enterExplain(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExplain" ):
                listener.exitExplain(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExplain" ):
                return visitor.visitExplain(self)
            else:
                return visitor.visitChildren(self)


    class DropDatabaseContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DROP(self):
            return self.getToken(SparkSqlBaseParser.DROP, 0)
        def DATABASE(self):
            return self.getToken(SparkSqlBaseParser.DATABASE, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def RESTRICT(self):
            return self.getToken(SparkSqlBaseParser.RESTRICT, 0)
        def CASCADE(self):
            return self.getToken(SparkSqlBaseParser.CASCADE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropDatabase" ):
                listener.enterDropDatabase(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropDatabase" ):
                listener.exitDropDatabase(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropDatabase" ):
                return visitor.visitDropDatabase(self)
            else:
                return visitor.visitChildren(self)


    class ResetConfigurationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def RESET(self):
            return self.getToken(SparkSqlBaseParser.RESET, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterResetConfiguration" ):
                listener.enterResetConfiguration(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitResetConfiguration" ):
                listener.exitResetConfiguration(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitResetConfiguration" ):
                return visitor.visitResetConfiguration(self)
            else:
                return visitor.visitChildren(self)


    class DescribeDatabaseContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DATABASE(self):
            return self.getToken(SparkSqlBaseParser.DATABASE, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def DESC(self):
            return self.getToken(SparkSqlBaseParser.DESC, 0)
        def DESCRIBE(self):
            return self.getToken(SparkSqlBaseParser.DESCRIBE, 0)
        def EXTENDED(self):
            return self.getToken(SparkSqlBaseParser.EXTENDED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeDatabase" ):
                listener.enterDescribeDatabase(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeDatabase" ):
                listener.exitDescribeDatabase(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeDatabase" ):
                return visitor.visitDescribeDatabase(self)
            else:
                return visitor.visitChildren(self)


    class AlterViewQueryContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAlterViewQuery" ):
                listener.enterAlterViewQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAlterViewQuery" ):
                listener.exitAlterViewQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAlterViewQuery" ):
                return visitor.visitAlterViewQuery(self)
            else:
                return visitor.visitChildren(self)


    class UseContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.db = None # IdentifierContext
            self.copyFrom(ctx)

        def USE(self):
            return self.getToken(SparkSqlBaseParser.USE, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUse" ):
                listener.enterUse(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUse" ):
                listener.exitUse(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUse" ):
                return visitor.visitUse(self)
            else:
                return visitor.visitChildren(self)


    class CreateTempViewUsingContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)
        def TEMPORARY(self):
            return self.getToken(SparkSqlBaseParser.TEMPORARY, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def tableProvider(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableProviderContext,0)

        def OR(self):
            return self.getToken(SparkSqlBaseParser.OR, 0)
        def REPLACE(self):
            return self.getToken(SparkSqlBaseParser.REPLACE, 0)
        def GLOBAL(self):
            return self.getToken(SparkSqlBaseParser.GLOBAL, 0)
        def colTypeList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeListContext,0)

        def OPTIONS(self):
            return self.getToken(SparkSqlBaseParser.OPTIONS, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTempViewUsing" ):
                listener.enterCreateTempViewUsing(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTempViewUsing" ):
                listener.exitCreateTempViewUsing(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTempViewUsing" ):
                return visitor.visitCreateTempViewUsing(self)
            else:
                return visitor.visitChildren(self)


    class RenameTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.from_ = None # TableIdentifierContext
            self.to = None # TableIdentifierContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def RENAME(self):
            return self.getToken(SparkSqlBaseParser.RENAME, 0)
        def TO(self):
            return self.getToken(SparkSqlBaseParser.TO, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)
        def tableIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.TableIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRenameTable" ):
                listener.enterRenameTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRenameTable" ):
                listener.exitRenameTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRenameTable" ):
                return visitor.visitRenameTable(self)
            else:
                return visitor.visitChildren(self)


    class FailNativeCommandContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)
        def ROLE(self):
            return self.getToken(SparkSqlBaseParser.ROLE, 0)
        def unsupportedHiveNativeCommands(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.UnsupportedHiveNativeCommandsContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFailNativeCommand" ):
                listener.enterFailNativeCommand(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFailNativeCommand" ):
                listener.exitFailNativeCommand(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFailNativeCommand" ):
                return visitor.visitFailNativeCommand(self)
            else:
                return visitor.visitChildren(self)


    class ClearCacheContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CLEAR(self):
            return self.getToken(SparkSqlBaseParser.CLEAR, 0)
        def CACHE(self):
            return self.getToken(SparkSqlBaseParser.CACHE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterClearCache" ):
                listener.enterClearCache(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitClearCache" ):
                listener.exitClearCache(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitClearCache" ):
                return visitor.visitClearCache(self)
            else:
                return visitor.visitChildren(self)


    class ShowTablesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.db = None # IdentifierContext
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def TABLES(self):
            return self.getToken(SparkSqlBaseParser.TABLES, 0)
        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)
        def IN(self):
            return self.getToken(SparkSqlBaseParser.IN, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def LIKE(self):
            return self.getToken(SparkSqlBaseParser.LIKE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowTables" ):
                listener.enterShowTables(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowTables" ):
                listener.exitShowTables(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowTables" ):
                return visitor.visitShowTables(self)
            else:
                return visitor.visitChildren(self)


    class RecoverPartitionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def RECOVER(self):
            return self.getToken(SparkSqlBaseParser.RECOVER, 0)
        def PARTITIONS(self):
            return self.getToken(SparkSqlBaseParser.PARTITIONS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRecoverPartitions" ):
                listener.enterRecoverPartitions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRecoverPartitions" ):
                listener.exitRecoverPartitions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRecoverPartitions" ):
                return visitor.visitRecoverPartitions(self)
            else:
                return visitor.visitChildren(self)


    class RenameTablePartitionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.from_ = None # PartitionSpecContext
            self.to = None # PartitionSpecContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def RENAME(self):
            return self.getToken(SparkSqlBaseParser.RENAME, 0)
        def TO(self):
            return self.getToken(SparkSqlBaseParser.TO, 0)
        def partitionSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.PartitionSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRenameTablePartition" ):
                listener.enterRenameTablePartition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRenameTablePartition" ):
                listener.exitRenameTablePartition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRenameTablePartition" ):
                return visitor.visitRenameTablePartition(self)
            else:
                return visitor.visitChildren(self)


    class RepairTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def MSCK(self):
            return self.getToken(SparkSqlBaseParser.MSCK, 0)
        def REPAIR(self):
            return self.getToken(SparkSqlBaseParser.REPAIR, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRepairTable" ):
                listener.enterRepairTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRepairTable" ):
                listener.exitRepairTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRepairTable" ):
                return visitor.visitRepairTable(self)
            else:
                return visitor.visitChildren(self)


    class RefreshResourceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def REFRESH(self):
            return self.getToken(SparkSqlBaseParser.REFRESH, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRefreshResource" ):
                listener.enterRefreshResource(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRefreshResource" ):
                listener.exitRefreshResource(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRefreshResource" ):
                return visitor.visitRefreshResource(self)
            else:
                return visitor.visitChildren(self)


    class ShowCreateTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowCreateTable" ):
                listener.enterShowCreateTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowCreateTable" ):
                listener.exitShowCreateTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowCreateTable" ):
                return visitor.visitShowCreateTable(self)
            else:
                return visitor.visitChildren(self)


    class ShowColumnsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.db = None # IdentifierContext
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def COLUMNS(self):
            return self.getToken(SparkSqlBaseParser.COLUMNS, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def FROM(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.FROM)
            else:
                return self.getToken(SparkSqlBaseParser.FROM, i)
        def IN(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.IN)
            else:
                return self.getToken(SparkSqlBaseParser.IN, i)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowColumns" ):
                listener.enterShowColumns(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowColumns" ):
                listener.exitShowColumns(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowColumns" ):
                return visitor.visitShowColumns(self)
            else:
                return visitor.visitChildren(self)


    class AddTablePartitionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def ADD(self):
            return self.getToken(SparkSqlBaseParser.ADD, 0)
        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def partitionSpecLocation(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.PartitionSpecLocationContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecLocationContext,i)

        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)
        def partitionSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.PartitionSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAddTablePartition" ):
                listener.enterAddTablePartition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAddTablePartition" ):
                listener.exitAddTablePartition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAddTablePartition" ):
                return visitor.visitAddTablePartition(self)
            else:
                return visitor.visitChildren(self)


    class RefreshTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def REFRESH(self):
            return self.getToken(SparkSqlBaseParser.REFRESH, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRefreshTable" ):
                listener.enterRefreshTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRefreshTable" ):
                listener.exitRefreshTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRefreshTable" ):
                return visitor.visitRefreshTable(self)
            else:
                return visitor.visitChildren(self)


    class ManageResourceContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.op = None # Token
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def ADD(self):
            return self.getToken(SparkSqlBaseParser.ADD, 0)
        def LIST(self):
            return self.getToken(SparkSqlBaseParser.LIST, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterManageResource" ):
                listener.enterManageResource(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitManageResource" ):
                listener.exitManageResource(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitManageResource" ):
                return visitor.visitManageResource(self)
            else:
                return visitor.visitChildren(self)


    class CreateDatabaseContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.comment = None # Token
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)
        def DATABASE(self):
            return self.getToken(SparkSqlBaseParser.DATABASE, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def COMMENT(self):
            return self.getToken(SparkSqlBaseParser.COMMENT, 0)
        def locationSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.LocationSpecContext,0)

        def WITH(self):
            return self.getToken(SparkSqlBaseParser.WITH, 0)
        def DBPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.DBPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateDatabase" ):
                listener.enterCreateDatabase(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateDatabase" ):
                listener.exitCreateDatabase(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateDatabase" ):
                return visitor.visitCreateDatabase(self)
            else:
                return visitor.visitChildren(self)


    class AnalyzeContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ANALYZE(self):
            return self.getToken(SparkSqlBaseParser.ANALYZE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def COMPUTE(self):
            return self.getToken(SparkSqlBaseParser.COMPUTE, 0)
        def STATISTICS(self):
            return self.getToken(SparkSqlBaseParser.STATISTICS, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def FOR(self):
            return self.getToken(SparkSqlBaseParser.FOR, 0)
        def COLUMNS(self):
            return self.getToken(SparkSqlBaseParser.COLUMNS, 0)
        def identifierSeq(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierSeqContext,0)

        def ALL(self):
            return self.getToken(SparkSqlBaseParser.ALL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAnalyze" ):
                listener.enterAnalyze(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAnalyze" ):
                listener.exitAnalyze(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAnalyze" ):
                return visitor.visitAnalyze(self)
            else:
                return visitor.visitChildren(self)


    class CreateHiveTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.columns = None # ColTypeListContext
            self.comment = None # Token
            self.partitionColumns = None # ColTypeListContext
            self.tableProps = None # TablePropertyListContext
            self.copyFrom(ctx)

        def createTableHeader(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.CreateTableHeaderContext,0)

        def bucketSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.BucketSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.BucketSpecContext,i)

        def skewSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.SkewSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.SkewSpecContext,i)

        def rowFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.RowFormatContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.RowFormatContext,i)

        def createFileFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.CreateFileFormatContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.CreateFileFormatContext,i)

        def locationSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.LocationSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.LocationSpecContext,i)

        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)

        def colTypeList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ColTypeListContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeListContext,i)

        def COMMENT(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.COMMENT)
            else:
                return self.getToken(SparkSqlBaseParser.COMMENT, i)
        def PARTITIONED(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.PARTITIONED)
            else:
                return self.getToken(SparkSqlBaseParser.PARTITIONED, i)
        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.BY)
            else:
                return self.getToken(SparkSqlBaseParser.BY, i)
        def TBLPROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.TBLPROPERTIES)
            else:
                return self.getToken(SparkSqlBaseParser.TBLPROPERTIES, i)
        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.STRING)
            else:
                return self.getToken(SparkSqlBaseParser.STRING, i)
        def tablePropertyList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.TablePropertyListContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,i)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateHiveTable" ):
                listener.enterCreateHiveTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateHiveTable" ):
                listener.exitCreateHiveTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateHiveTable" ):
                return visitor.visitCreateHiveTable(self)
            else:
                return visitor.visitChildren(self)


    class CreateFunctionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.className = None # Token
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)
        def FUNCTION(self):
            return self.getToken(SparkSqlBaseParser.FUNCTION, 0)
        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def OR(self):
            return self.getToken(SparkSqlBaseParser.OR, 0)
        def REPLACE(self):
            return self.getToken(SparkSqlBaseParser.REPLACE, 0)
        def TEMPORARY(self):
            return self.getToken(SparkSqlBaseParser.TEMPORARY, 0)
        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def USING(self):
            return self.getToken(SparkSqlBaseParser.USING, 0)
        def resource(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ResourceContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ResourceContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateFunction" ):
                listener.enterCreateFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateFunction" ):
                listener.exitCreateFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateFunction" ):
                return visitor.visitCreateFunction(self)
            else:
                return visitor.visitChildren(self)


    class ShowTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.db = None # IdentifierContext
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def EXTENDED(self):
            return self.getToken(SparkSqlBaseParser.EXTENDED, 0)
        def LIKE(self):
            return self.getToken(SparkSqlBaseParser.LIKE, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)

        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)
        def IN(self):
            return self.getToken(SparkSqlBaseParser.IN, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowTable" ):
                listener.enterShowTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowTable" ):
                listener.exitShowTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowTable" ):
                return visitor.visitShowTable(self)
            else:
                return visitor.visitChildren(self)


    class SetDatabasePropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def DATABASE(self):
            return self.getToken(SparkSqlBaseParser.DATABASE, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)
        def DBPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.DBPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetDatabaseProperties" ):
                listener.enterSetDatabaseProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetDatabaseProperties" ):
                listener.exitSetDatabaseProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetDatabaseProperties" ):
                return visitor.visitSetDatabaseProperties(self)
            else:
                return visitor.visitChildren(self)


    class CreateTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.options = None # TablePropertyListContext
            self.partitionColumnNames = None # IdentifierListContext
            self.comment = None # Token
            self.tableProps = None # TablePropertyListContext
            self.copyFrom(ctx)

        def createTableHeader(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.CreateTableHeaderContext,0)

        def tableProvider(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableProviderContext,0)

        def colTypeList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeListContext,0)

        def bucketSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.BucketSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.BucketSpecContext,i)

        def locationSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.LocationSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.LocationSpecContext,i)

        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)

        def OPTIONS(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.OPTIONS)
            else:
                return self.getToken(SparkSqlBaseParser.OPTIONS, i)
        def PARTITIONED(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.PARTITIONED)
            else:
                return self.getToken(SparkSqlBaseParser.PARTITIONED, i)
        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.BY)
            else:
                return self.getToken(SparkSqlBaseParser.BY, i)
        def COMMENT(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.COMMENT)
            else:
                return self.getToken(SparkSqlBaseParser.COMMENT, i)
        def TBLPROPERTIES(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.TBLPROPERTIES)
            else:
                return self.getToken(SparkSqlBaseParser.TBLPROPERTIES, i)
        def tablePropertyList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.TablePropertyListContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,i)

        def identifierList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierListContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierListContext,i)

        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.STRING)
            else:
                return self.getToken(SparkSqlBaseParser.STRING, i)
        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTable" ):
                listener.enterCreateTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTable" ):
                listener.exitCreateTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTable" ):
                return visitor.visitCreateTable(self)
            else:
                return visitor.visitChildren(self)


    class DescribeTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.option = None # Token
            self.copyFrom(ctx)

        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def DESC(self):
            return self.getToken(SparkSqlBaseParser.DESC, 0)
        def DESCRIBE(self):
            return self.getToken(SparkSqlBaseParser.DESCRIBE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)

        def describeColName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.DescribeColNameContext,0)

        def EXTENDED(self):
            return self.getToken(SparkSqlBaseParser.EXTENDED, 0)
        def FORMATTED(self):
            return self.getToken(SparkSqlBaseParser.FORMATTED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeTable" ):
                listener.enterDescribeTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeTable" ):
                listener.exitDescribeTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeTable" ):
                return visitor.visitDescribeTable(self)
            else:
                return visitor.visitChildren(self)


    class CreateTableLikeContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.target = None # TableIdentifierContext
            self.source = None # TableIdentifierContext
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def LIKE(self):
            return self.getToken(SparkSqlBaseParser.LIKE, 0)
        def tableIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.TableIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,i)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def locationSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.LocationSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTableLike" ):
                listener.enterCreateTableLike(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTableLike" ):
                listener.exitCreateTableLike(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTableLike" ):
                return visitor.visitCreateTableLike(self)
            else:
                return visitor.visitChildren(self)


    class UncacheTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def UNCACHE(self):
            return self.getToken(SparkSqlBaseParser.UNCACHE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUncacheTable" ):
                listener.enterUncacheTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUncacheTable" ):
                listener.exitUncacheTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUncacheTable" ):
                return visitor.visitUncacheTable(self)
            else:
                return visitor.visitChildren(self)


    class DropFunctionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DROP(self):
            return self.getToken(SparkSqlBaseParser.DROP, 0)
        def FUNCTION(self):
            return self.getToken(SparkSqlBaseParser.FUNCTION, 0)
        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)

        def TEMPORARY(self):
            return self.getToken(SparkSqlBaseParser.TEMPORARY, 0)
        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropFunction" ):
                listener.enterDropFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropFunction" ):
                listener.exitDropFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropFunction" ):
                return visitor.visitDropFunction(self)
            else:
                return visitor.visitChildren(self)


    class LoadDataContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.path = None # Token
            self.copyFrom(ctx)

        def LOAD(self):
            return self.getToken(SparkSqlBaseParser.LOAD, 0)
        def DATA(self):
            return self.getToken(SparkSqlBaseParser.DATA, 0)
        def INPATH(self):
            return self.getToken(SparkSqlBaseParser.INPATH, 0)
        def INTO(self):
            return self.getToken(SparkSqlBaseParser.INTO, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def LOCAL(self):
            return self.getToken(SparkSqlBaseParser.LOCAL, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSqlBaseParser.OVERWRITE, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLoadData" ):
                listener.enterLoadData(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLoadData" ):
                listener.exitLoadData(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLoadData" ):
                return visitor.visitLoadData(self)
            else:
                return visitor.visitChildren(self)


    class ShowPartitionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def PARTITIONS(self):
            return self.getToken(SparkSqlBaseParser.PARTITIONS, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowPartitions" ):
                listener.enterShowPartitions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowPartitions" ):
                listener.exitShowPartitions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowPartitions" ):
                return visitor.visitShowPartitions(self)
            else:
                return visitor.visitChildren(self)


    class DescribeFunctionContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def FUNCTION(self):
            return self.getToken(SparkSqlBaseParser.FUNCTION, 0)
        def describeFuncName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.DescribeFuncNameContext,0)

        def DESC(self):
            return self.getToken(SparkSqlBaseParser.DESC, 0)
        def DESCRIBE(self):
            return self.getToken(SparkSqlBaseParser.DESCRIBE, 0)
        def EXTENDED(self):
            return self.getToken(SparkSqlBaseParser.EXTENDED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeFunction" ):
                listener.enterDescribeFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeFunction" ):
                listener.exitDescribeFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeFunction" ):
                return visitor.visitDescribeFunction(self)
            else:
                return visitor.visitChildren(self)


    class ChangeColumnContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def CHANGE(self):
            return self.getToken(SparkSqlBaseParser.CHANGE, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def colType(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)

        def COLUMN(self):
            return self.getToken(SparkSqlBaseParser.COLUMN, 0)
        def colPosition(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ColPositionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterChangeColumn" ):
                listener.enterChangeColumn(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitChangeColumn" ):
                listener.exitChangeColumn(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitChangeColumn" ):
                return visitor.visitChangeColumn(self)
            else:
                return visitor.visitChildren(self)


    class StatementDefaultContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStatementDefault" ):
                listener.enterStatementDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStatementDefault" ):
                listener.exitStatementDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStatementDefault" ):
                return visitor.visitStatementDefault(self)
            else:
                return visitor.visitChildren(self)


    class TruncateTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def TRUNCATE(self):
            return self.getToken(SparkSqlBaseParser.TRUNCATE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTruncateTable" ):
                listener.enterTruncateTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTruncateTable" ):
                listener.exitTruncateTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTruncateTable" ):
                return visitor.visitTruncateTable(self)
            else:
                return visitor.visitChildren(self)


    class SetTableSerDeContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)
        def SERDE(self):
            return self.getToken(SparkSqlBaseParser.SERDE, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)

        def WITH(self):
            return self.getToken(SparkSqlBaseParser.WITH, 0)
        def SERDEPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.SERDEPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetTableSerDe" ):
                listener.enterSetTableSerDe(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetTableSerDe" ):
                listener.exitSetTableSerDe(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetTableSerDe" ):
                return visitor.visitSetTableSerDe(self)
            else:
                return visitor.visitChildren(self)


    class CreateViewContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)
        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)

        def OR(self):
            return self.getToken(SparkSqlBaseParser.OR, 0)
        def REPLACE(self):
            return self.getToken(SparkSqlBaseParser.REPLACE, 0)
        def TEMPORARY(self):
            return self.getToken(SparkSqlBaseParser.TEMPORARY, 0)
        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def identifierCommentList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierCommentListContext,0)

        def COMMENT(self):
            return self.getToken(SparkSqlBaseParser.COMMENT, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def PARTITIONED(self):
            return self.getToken(SparkSqlBaseParser.PARTITIONED, 0)
        def ON(self):
            return self.getToken(SparkSqlBaseParser.ON, 0)
        def identifierList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierListContext,0)

        def TBLPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.TBLPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)

        def GLOBAL(self):
            return self.getToken(SparkSqlBaseParser.GLOBAL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateView" ):
                listener.enterCreateView(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateView" ):
                listener.exitCreateView(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateView" ):
                return visitor.visitCreateView(self)
            else:
                return visitor.visitChildren(self)


    class DropTablePartitionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def DROP(self):
            return self.getToken(SparkSqlBaseParser.DROP, 0)
        def partitionSpec(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.PartitionSpecContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,i)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def PURGE(self):
            return self.getToken(SparkSqlBaseParser.PURGE, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropTablePartitions" ):
                listener.enterDropTablePartitions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropTablePartitions" ):
                listener.exitDropTablePartitions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropTablePartitions" ):
                return visitor.visitDropTablePartitions(self)
            else:
                return visitor.visitChildren(self)


    class SetConfigurationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetConfiguration" ):
                listener.enterSetConfiguration(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetConfiguration" ):
                listener.exitSetConfiguration(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetConfiguration" ):
                return visitor.visitSetConfiguration(self)
            else:
                return visitor.visitChildren(self)


    class DropTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DROP(self):
            return self.getToken(SparkSqlBaseParser.DROP, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def PURGE(self):
            return self.getToken(SparkSqlBaseParser.PURGE, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDropTable" ):
                listener.enterDropTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDropTable" ):
                listener.exitDropTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDropTable" ):
                return visitor.visitDropTable(self)
            else:
                return visitor.visitChildren(self)


    class ShowDatabasesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def DATABASES(self):
            return self.getToken(SparkSqlBaseParser.DATABASES, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def LIKE(self):
            return self.getToken(SparkSqlBaseParser.LIKE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowDatabases" ):
                listener.enterShowDatabases(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowDatabases" ):
                listener.exitShowDatabases(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowDatabases" ):
                return visitor.visitShowDatabases(self)
            else:
                return visitor.visitChildren(self)


    class ShowTblPropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.table = None # TableIdentifierContext
            self.key = None # TablePropertyKeyContext
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def TBLPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.TBLPROPERTIES, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def tablePropertyKey(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyKeyContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowTblProperties" ):
                listener.enterShowTblProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowTblProperties" ):
                listener.exitShowTblProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowTblProperties" ):
                return visitor.visitShowTblProperties(self)
            else:
                return visitor.visitChildren(self)


    class UnsetTablePropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def UNSET(self):
            return self.getToken(SparkSqlBaseParser.UNSET, 0)
        def TBLPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.TBLPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)

        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)
        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnsetTableProperties" ):
                listener.enterUnsetTableProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnsetTableProperties" ):
                listener.exitUnsetTableProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnsetTableProperties" ):
                return visitor.visitUnsetTableProperties(self)
            else:
                return visitor.visitChildren(self)


    class SetTableLocationContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)
        def locationSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.LocationSpecContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetTableLocation" ):
                listener.enterSetTableLocation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetTableLocation" ):
                listener.exitSetTableLocation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetTableLocation" ):
                return visitor.visitSetTableLocation(self)
            else:
                return visitor.visitChildren(self)


    class ShowFunctionsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.pattern = None # Token
            self.copyFrom(ctx)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)
        def FUNCTIONS(self):
            return self.getToken(SparkSqlBaseParser.FUNCTIONS, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)

        def LIKE(self):
            return self.getToken(SparkSqlBaseParser.LIKE, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterShowFunctions" ):
                listener.enterShowFunctions(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitShowFunctions" ):
                listener.exitShowFunctions(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitShowFunctions" ):
                return visitor.visitShowFunctions(self)
            else:
                return visitor.visitChildren(self)


    class CacheTableContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.options = None # TablePropertyListContext
            self.copyFrom(ctx)

        def CACHE(self):
            return self.getToken(SparkSqlBaseParser.CACHE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def LAZY(self):
            return self.getToken(SparkSqlBaseParser.LAZY, 0)
        def OPTIONS(self):
            return self.getToken(SparkSqlBaseParser.OPTIONS, 0)
        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)

        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCacheTable" ):
                listener.enterCacheTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCacheTable" ):
                listener.exitCacheTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCacheTable" ):
                return visitor.visitCacheTable(self)
            else:
                return visitor.visitChildren(self)


    class AddTableColumnsContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.columns = None # ColTypeListContext
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def ADD(self):
            return self.getToken(SparkSqlBaseParser.ADD, 0)
        def COLUMNS(self):
            return self.getToken(SparkSqlBaseParser.COLUMNS, 0)
        def colTypeList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAddTableColumns" ):
                listener.enterAddTableColumns(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAddTableColumns" ):
                listener.exitAddTableColumns(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAddTableColumns" ):
                return visitor.visitAddTableColumns(self)
            else:
                return visitor.visitChildren(self)


    class SetTablePropertiesContext(StatementContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StatementContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)
        def TBLPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.TBLPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)

        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetTableProperties" ):
                listener.enterSetTableProperties(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetTableProperties" ):
                listener.exitSetTableProperties(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetTableProperties" ):
                return visitor.visitSetTableProperties(self)
            else:
                return visitor.visitChildren(self)



    def statement(self):

        localctx = SparkSqlBaseParser.StatementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 12, self.RULE_statement)
        self._la = 0 # Token type
        try:
            self.state = 833
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,96,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.StatementDefaultContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 220
                self.query()
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.UseContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 221
                self.match(SparkSqlBaseParser.USE)
                self.state = 222
                localctx.db = self.identifier()
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.CreateDatabaseContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 223
                self.match(SparkSqlBaseParser.CREATE)
                self.state = 224
                self.match(SparkSqlBaseParser.DATABASE)
                self.state = 228
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,0,self._ctx)
                if la_ == 1:
                    self.state = 225
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 226
                    self.match(SparkSqlBaseParser.NOT)
                    self.state = 227
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 230
                self.identifier()
                self.state = 233
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.COMMENT:
                    self.state = 231
                    self.match(SparkSqlBaseParser.COMMENT)
                    self.state = 232
                    localctx.comment = self.match(SparkSqlBaseParser.STRING)


                self.state = 236
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LOCATION:
                    self.state = 235
                    self.locationSpec()


                self.state = 241
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.WITH:
                    self.state = 238
                    self.match(SparkSqlBaseParser.WITH)
                    self.state = 239
                    self.match(SparkSqlBaseParser.DBPROPERTIES)
                    self.state = 240
                    self.tablePropertyList()


                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.SetDatabasePropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 243
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 244
                self.match(SparkSqlBaseParser.DATABASE)
                self.state = 245
                self.identifier()
                self.state = 246
                self.match(SparkSqlBaseParser.SET)
                self.state = 247
                self.match(SparkSqlBaseParser.DBPROPERTIES)
                self.state = 248
                self.tablePropertyList()
                pass

            elif la_ == 5:
                localctx = SparkSqlBaseParser.DropDatabaseContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 250
                self.match(SparkSqlBaseParser.DROP)
                self.state = 251
                self.match(SparkSqlBaseParser.DATABASE)
                self.state = 254
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,4,self._ctx)
                if la_ == 1:
                    self.state = 252
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 253
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 256
                self.identifier()
                self.state = 258
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.CASCADE or _la==SparkSqlBaseParser.RESTRICT:
                    self.state = 257
                    _la = self._input.LA(1)
                    if not(_la==SparkSqlBaseParser.CASCADE or _la==SparkSqlBaseParser.RESTRICT):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                pass

            elif la_ == 6:
                localctx = SparkSqlBaseParser.CreateTableContext(self, localctx)
                self.enterOuterAlt(localctx, 6)
                self.state = 260
                self.createTableHeader()
                self.state = 265
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.T__0:
                    self.state = 261
                    self.match(SparkSqlBaseParser.T__0)
                    self.state = 262
                    self.colTypeList()
                    self.state = 263
                    self.match(SparkSqlBaseParser.T__1)


                self.state = 267
                self.tableProvider()
                self.state = 281
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.COMMENT or ((((_la - 183)) & ~0x3f) == 0 and ((1 << (_la - 183)) & ((1 << (SparkSqlBaseParser.OPTIONS - 183)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 183)) | (1 << (SparkSqlBaseParser.LOCATION - 183)) | (1 << (SparkSqlBaseParser.CLUSTERED - 183)) | (1 << (SparkSqlBaseParser.PARTITIONED - 183)))) != 0):
                    self.state = 279
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSqlBaseParser.OPTIONS]:
                        self.state = 268
                        self.match(SparkSqlBaseParser.OPTIONS)
                        self.state = 269
                        localctx.options = self.tablePropertyList()
                        pass
                    elif token in [SparkSqlBaseParser.PARTITIONED]:
                        self.state = 270
                        self.match(SparkSqlBaseParser.PARTITIONED)
                        self.state = 271
                        self.match(SparkSqlBaseParser.BY)
                        self.state = 272
                        localctx.partitionColumnNames = self.identifierList()
                        pass
                    elif token in [SparkSqlBaseParser.CLUSTERED]:
                        self.state = 273
                        self.bucketSpec()
                        pass
                    elif token in [SparkSqlBaseParser.LOCATION]:
                        self.state = 274
                        self.locationSpec()
                        pass
                    elif token in [SparkSqlBaseParser.COMMENT]:
                        self.state = 275
                        self.match(SparkSqlBaseParser.COMMENT)
                        self.state = 276
                        localctx.comment = self.match(SparkSqlBaseParser.STRING)
                        pass
                    elif token in [SparkSqlBaseParser.TBLPROPERTIES]:
                        self.state = 277
                        self.match(SparkSqlBaseParser.TBLPROPERTIES)
                        self.state = 278
                        localctx.tableProps = self.tablePropertyList()
                        pass
                    else:
                        raise NoViableAltException(self)

                    self.state = 283
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 288
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.AS))) != 0) or ((((_la - 77)) & ~0x3f) == 0 and ((1 << (_la - 77)) & ((1 << (SparkSqlBaseParser.WITH - 77)) | (1 << (SparkSqlBaseParser.VALUES - 77)) | (1 << (SparkSqlBaseParser.TABLE - 77)) | (1 << (SparkSqlBaseParser.INSERT - 77)) | (1 << (SparkSqlBaseParser.MAP - 77)))) != 0) or _la==SparkSqlBaseParser.REDUCE:
                    self.state = 285
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.AS:
                        self.state = 284
                        self.match(SparkSqlBaseParser.AS)


                    self.state = 287
                    self.query()


                pass

            elif la_ == 7:
                localctx = SparkSqlBaseParser.CreateHiveTableContext(self, localctx)
                self.enterOuterAlt(localctx, 7)
                self.state = 290
                self.createTableHeader()
                self.state = 295
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,11,self._ctx)
                if la_ == 1:
                    self.state = 291
                    self.match(SparkSqlBaseParser.T__0)
                    self.state = 292
                    localctx.columns = self.colTypeList()
                    self.state = 293
                    self.match(SparkSqlBaseParser.T__1)


                self.state = 314
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.ROW or _la==SparkSqlBaseParser.COMMENT or ((((_la - 185)) & ~0x3f) == 0 and ((1 << (_la - 185)) & ((1 << (SparkSqlBaseParser.TBLPROPERTIES - 185)) | (1 << (SparkSqlBaseParser.SKEWED - 185)) | (1 << (SparkSqlBaseParser.STORED - 185)) | (1 << (SparkSqlBaseParser.LOCATION - 185)) | (1 << (SparkSqlBaseParser.CLUSTERED - 185)) | (1 << (SparkSqlBaseParser.PARTITIONED - 185)))) != 0):
                    self.state = 312
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSqlBaseParser.COMMENT]:
                        self.state = 297
                        self.match(SparkSqlBaseParser.COMMENT)
                        self.state = 298
                        localctx.comment = self.match(SparkSqlBaseParser.STRING)
                        pass
                    elif token in [SparkSqlBaseParser.PARTITIONED]:
                        self.state = 299
                        self.match(SparkSqlBaseParser.PARTITIONED)
                        self.state = 300
                        self.match(SparkSqlBaseParser.BY)
                        self.state = 301
                        self.match(SparkSqlBaseParser.T__0)
                        self.state = 302
                        localctx.partitionColumns = self.colTypeList()
                        self.state = 303
                        self.match(SparkSqlBaseParser.T__1)
                        pass
                    elif token in [SparkSqlBaseParser.CLUSTERED]:
                        self.state = 305
                        self.bucketSpec()
                        pass
                    elif token in [SparkSqlBaseParser.SKEWED]:
                        self.state = 306
                        self.skewSpec()
                        pass
                    elif token in [SparkSqlBaseParser.ROW]:
                        self.state = 307
                        self.rowFormat()
                        pass
                    elif token in [SparkSqlBaseParser.STORED]:
                        self.state = 308
                        self.createFileFormat()
                        pass
                    elif token in [SparkSqlBaseParser.LOCATION]:
                        self.state = 309
                        self.locationSpec()
                        pass
                    elif token in [SparkSqlBaseParser.TBLPROPERTIES]:
                        self.state = 310
                        self.match(SparkSqlBaseParser.TBLPROPERTIES)
                        self.state = 311
                        localctx.tableProps = self.tablePropertyList()
                        pass
                    else:
                        raise NoViableAltException(self)

                    self.state = 316
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 321
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.AS))) != 0) or ((((_la - 77)) & ~0x3f) == 0 and ((1 << (_la - 77)) & ((1 << (SparkSqlBaseParser.WITH - 77)) | (1 << (SparkSqlBaseParser.VALUES - 77)) | (1 << (SparkSqlBaseParser.TABLE - 77)) | (1 << (SparkSqlBaseParser.INSERT - 77)) | (1 << (SparkSqlBaseParser.MAP - 77)))) != 0) or _la==SparkSqlBaseParser.REDUCE:
                    self.state = 318
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.AS:
                        self.state = 317
                        self.match(SparkSqlBaseParser.AS)


                    self.state = 320
                    self.query()


                pass

            elif la_ == 8:
                localctx = SparkSqlBaseParser.CreateTableLikeContext(self, localctx)
                self.enterOuterAlt(localctx, 8)
                self.state = 323
                self.match(SparkSqlBaseParser.CREATE)
                self.state = 324
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 328
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,16,self._ctx)
                if la_ == 1:
                    self.state = 325
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 326
                    self.match(SparkSqlBaseParser.NOT)
                    self.state = 327
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 330
                localctx.target = self.tableIdentifier()
                self.state = 331
                self.match(SparkSqlBaseParser.LIKE)
                self.state = 332
                localctx.source = self.tableIdentifier()
                self.state = 334
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LOCATION:
                    self.state = 333
                    self.locationSpec()


                pass

            elif la_ == 9:
                localctx = SparkSqlBaseParser.AnalyzeContext(self, localctx)
                self.enterOuterAlt(localctx, 9)
                self.state = 336
                self.match(SparkSqlBaseParser.ANALYZE)
                self.state = 337
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 338
                self.tableIdentifier()
                self.state = 340
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 339
                    self.partitionSpec()


                self.state = 342
                self.match(SparkSqlBaseParser.COMPUTE)
                self.state = 343
                self.match(SparkSqlBaseParser.STATISTICS)
                self.state = 351
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,19,self._ctx)
                if la_ == 1:
                    self.state = 344
                    self.identifier()

                elif la_ == 2:
                    self.state = 345
                    self.match(SparkSqlBaseParser.FOR)
                    self.state = 346
                    self.match(SparkSqlBaseParser.COLUMNS)
                    self.state = 347
                    self.identifierSeq()

                elif la_ == 3:
                    self.state = 348
                    self.match(SparkSqlBaseParser.FOR)
                    self.state = 349
                    self.match(SparkSqlBaseParser.ALL)
                    self.state = 350
                    self.match(SparkSqlBaseParser.COLUMNS)


                pass

            elif la_ == 10:
                localctx = SparkSqlBaseParser.AddTableColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 10)
                self.state = 353
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 354
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 355
                self.tableIdentifier()
                self.state = 356
                self.match(SparkSqlBaseParser.ADD)
                self.state = 357
                self.match(SparkSqlBaseParser.COLUMNS)
                self.state = 358
                self.match(SparkSqlBaseParser.T__0)
                self.state = 359
                localctx.columns = self.colTypeList()
                self.state = 360
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 11:
                localctx = SparkSqlBaseParser.RenameTableContext(self, localctx)
                self.enterOuterAlt(localctx, 11)
                self.state = 362
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 363
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.TABLE or _la==SparkSqlBaseParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 364
                localctx.from_ = self.tableIdentifier()
                self.state = 365
                self.match(SparkSqlBaseParser.RENAME)
                self.state = 366
                self.match(SparkSqlBaseParser.TO)
                self.state = 367
                localctx.to = self.tableIdentifier()
                pass

            elif la_ == 12:
                localctx = SparkSqlBaseParser.SetTablePropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 12)
                self.state = 369
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 370
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.TABLE or _la==SparkSqlBaseParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 371
                self.tableIdentifier()
                self.state = 372
                self.match(SparkSqlBaseParser.SET)
                self.state = 373
                self.match(SparkSqlBaseParser.TBLPROPERTIES)
                self.state = 374
                self.tablePropertyList()
                pass

            elif la_ == 13:
                localctx = SparkSqlBaseParser.UnsetTablePropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 13)
                self.state = 376
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 377
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.TABLE or _la==SparkSqlBaseParser.VIEW):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 378
                self.tableIdentifier()
                self.state = 379
                self.match(SparkSqlBaseParser.UNSET)
                self.state = 380
                self.match(SparkSqlBaseParser.TBLPROPERTIES)
                self.state = 383
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.IF:
                    self.state = 381
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 382
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 385
                self.tablePropertyList()
                pass

            elif la_ == 14:
                localctx = SparkSqlBaseParser.ChangeColumnContext(self, localctx)
                self.enterOuterAlt(localctx, 14)
                self.state = 387
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 388
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 389
                self.tableIdentifier()
                self.state = 391
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 390
                    self.partitionSpec()


                self.state = 393
                self.match(SparkSqlBaseParser.CHANGE)
                self.state = 395
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,22,self._ctx)
                if la_ == 1:
                    self.state = 394
                    self.match(SparkSqlBaseParser.COLUMN)


                self.state = 397
                self.identifier()
                self.state = 398
                self.colType()
                self.state = 400
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.FIRST or _la==SparkSqlBaseParser.AFTER:
                    self.state = 399
                    self.colPosition()


                pass

            elif la_ == 15:
                localctx = SparkSqlBaseParser.SetTableSerDeContext(self, localctx)
                self.enterOuterAlt(localctx, 15)
                self.state = 402
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 403
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 404
                self.tableIdentifier()
                self.state = 406
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 405
                    self.partitionSpec()


                self.state = 408
                self.match(SparkSqlBaseParser.SET)
                self.state = 409
                self.match(SparkSqlBaseParser.SERDE)
                self.state = 410
                self.match(SparkSqlBaseParser.STRING)
                self.state = 414
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.WITH:
                    self.state = 411
                    self.match(SparkSqlBaseParser.WITH)
                    self.state = 412
                    self.match(SparkSqlBaseParser.SERDEPROPERTIES)
                    self.state = 413
                    self.tablePropertyList()


                pass

            elif la_ == 16:
                localctx = SparkSqlBaseParser.SetTableSerDeContext(self, localctx)
                self.enterOuterAlt(localctx, 16)
                self.state = 416
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 417
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 418
                self.tableIdentifier()
                self.state = 420
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 419
                    self.partitionSpec()


                self.state = 422
                self.match(SparkSqlBaseParser.SET)
                self.state = 423
                self.match(SparkSqlBaseParser.SERDEPROPERTIES)
                self.state = 424
                self.tablePropertyList()
                pass

            elif la_ == 17:
                localctx = SparkSqlBaseParser.AddTablePartitionContext(self, localctx)
                self.enterOuterAlt(localctx, 17)
                self.state = 426
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 427
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 428
                self.tableIdentifier()
                self.state = 429
                self.match(SparkSqlBaseParser.ADD)
                self.state = 433
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.IF:
                    self.state = 430
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 431
                    self.match(SparkSqlBaseParser.NOT)
                    self.state = 432
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 436 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 435
                    self.partitionSpecLocation()
                    self.state = 438 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSqlBaseParser.PARTITION):
                        break

                pass

            elif la_ == 18:
                localctx = SparkSqlBaseParser.AddTablePartitionContext(self, localctx)
                self.enterOuterAlt(localctx, 18)
                self.state = 440
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 441
                self.match(SparkSqlBaseParser.VIEW)
                self.state = 442
                self.tableIdentifier()
                self.state = 443
                self.match(SparkSqlBaseParser.ADD)
                self.state = 447
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.IF:
                    self.state = 444
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 445
                    self.match(SparkSqlBaseParser.NOT)
                    self.state = 446
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 450 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 449
                    self.partitionSpec()
                    self.state = 452 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSqlBaseParser.PARTITION):
                        break

                pass

            elif la_ == 19:
                localctx = SparkSqlBaseParser.RenameTablePartitionContext(self, localctx)
                self.enterOuterAlt(localctx, 19)
                self.state = 454
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 455
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 456
                self.tableIdentifier()
                self.state = 457
                localctx.from_ = self.partitionSpec()
                self.state = 458
                self.match(SparkSqlBaseParser.RENAME)
                self.state = 459
                self.match(SparkSqlBaseParser.TO)
                self.state = 460
                localctx.to = self.partitionSpec()
                pass

            elif la_ == 20:
                localctx = SparkSqlBaseParser.DropTablePartitionsContext(self, localctx)
                self.enterOuterAlt(localctx, 20)
                self.state = 462
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 463
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 464
                self.tableIdentifier()
                self.state = 465
                self.match(SparkSqlBaseParser.DROP)
                self.state = 468
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.IF:
                    self.state = 466
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 467
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 470
                self.partitionSpec()
                self.state = 475
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 471
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 472
                    self.partitionSpec()
                    self.state = 477
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 479
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PURGE:
                    self.state = 478
                    self.match(SparkSqlBaseParser.PURGE)


                pass

            elif la_ == 21:
                localctx = SparkSqlBaseParser.DropTablePartitionsContext(self, localctx)
                self.enterOuterAlt(localctx, 21)
                self.state = 481
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 482
                self.match(SparkSqlBaseParser.VIEW)
                self.state = 483
                self.tableIdentifier()
                self.state = 484
                self.match(SparkSqlBaseParser.DROP)
                self.state = 487
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.IF:
                    self.state = 485
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 486
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 489
                self.partitionSpec()
                self.state = 494
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 490
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 491
                    self.partitionSpec()
                    self.state = 496
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                pass

            elif la_ == 22:
                localctx = SparkSqlBaseParser.SetTableLocationContext(self, localctx)
                self.enterOuterAlt(localctx, 22)
                self.state = 497
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 498
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 499
                self.tableIdentifier()
                self.state = 501
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 500
                    self.partitionSpec()


                self.state = 503
                self.match(SparkSqlBaseParser.SET)
                self.state = 504
                self.locationSpec()
                pass

            elif la_ == 23:
                localctx = SparkSqlBaseParser.RecoverPartitionsContext(self, localctx)
                self.enterOuterAlt(localctx, 23)
                self.state = 506
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 507
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 508
                self.tableIdentifier()
                self.state = 509
                self.match(SparkSqlBaseParser.RECOVER)
                self.state = 510
                self.match(SparkSqlBaseParser.PARTITIONS)
                pass

            elif la_ == 24:
                localctx = SparkSqlBaseParser.DropTableContext(self, localctx)
                self.enterOuterAlt(localctx, 24)
                self.state = 512
                self.match(SparkSqlBaseParser.DROP)
                self.state = 513
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 516
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,37,self._ctx)
                if la_ == 1:
                    self.state = 514
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 515
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 518
                self.tableIdentifier()
                self.state = 520
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PURGE:
                    self.state = 519
                    self.match(SparkSqlBaseParser.PURGE)


                pass

            elif la_ == 25:
                localctx = SparkSqlBaseParser.DropTableContext(self, localctx)
                self.enterOuterAlt(localctx, 25)
                self.state = 522
                self.match(SparkSqlBaseParser.DROP)
                self.state = 523
                self.match(SparkSqlBaseParser.VIEW)
                self.state = 526
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,39,self._ctx)
                if la_ == 1:
                    self.state = 524
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 525
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 528
                self.tableIdentifier()
                pass

            elif la_ == 26:
                localctx = SparkSqlBaseParser.CreateViewContext(self, localctx)
                self.enterOuterAlt(localctx, 26)
                self.state = 529
                self.match(SparkSqlBaseParser.CREATE)
                self.state = 532
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OR:
                    self.state = 530
                    self.match(SparkSqlBaseParser.OR)
                    self.state = 531
                    self.match(SparkSqlBaseParser.REPLACE)


                self.state = 538
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.GLOBAL or _la==SparkSqlBaseParser.TEMPORARY:
                    self.state = 535
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.GLOBAL:
                        self.state = 534
                        self.match(SparkSqlBaseParser.GLOBAL)


                    self.state = 537
                    self.match(SparkSqlBaseParser.TEMPORARY)


                self.state = 540
                self.match(SparkSqlBaseParser.VIEW)
                self.state = 544
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,43,self._ctx)
                if la_ == 1:
                    self.state = 541
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 542
                    self.match(SparkSqlBaseParser.NOT)
                    self.state = 543
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 546
                self.tableIdentifier()
                self.state = 548
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.T__0:
                    self.state = 547
                    self.identifierCommentList()


                self.state = 552
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.COMMENT:
                    self.state = 550
                    self.match(SparkSqlBaseParser.COMMENT)
                    self.state = 551
                    self.match(SparkSqlBaseParser.STRING)


                self.state = 557
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITIONED:
                    self.state = 554
                    self.match(SparkSqlBaseParser.PARTITIONED)
                    self.state = 555
                    self.match(SparkSqlBaseParser.ON)
                    self.state = 556
                    self.identifierList()


                self.state = 561
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.TBLPROPERTIES:
                    self.state = 559
                    self.match(SparkSqlBaseParser.TBLPROPERTIES)
                    self.state = 560
                    self.tablePropertyList()


                self.state = 563
                self.match(SparkSqlBaseParser.AS)
                self.state = 564
                self.query()
                pass

            elif la_ == 27:
                localctx = SparkSqlBaseParser.CreateTempViewUsingContext(self, localctx)
                self.enterOuterAlt(localctx, 27)
                self.state = 566
                self.match(SparkSqlBaseParser.CREATE)
                self.state = 569
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OR:
                    self.state = 567
                    self.match(SparkSqlBaseParser.OR)
                    self.state = 568
                    self.match(SparkSqlBaseParser.REPLACE)


                self.state = 572
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.GLOBAL:
                    self.state = 571
                    self.match(SparkSqlBaseParser.GLOBAL)


                self.state = 574
                self.match(SparkSqlBaseParser.TEMPORARY)
                self.state = 575
                self.match(SparkSqlBaseParser.VIEW)
                self.state = 576
                self.tableIdentifier()
                self.state = 581
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.T__0:
                    self.state = 577
                    self.match(SparkSqlBaseParser.T__0)
                    self.state = 578
                    self.colTypeList()
                    self.state = 579
                    self.match(SparkSqlBaseParser.T__1)


                self.state = 583
                self.tableProvider()
                self.state = 586
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OPTIONS:
                    self.state = 584
                    self.match(SparkSqlBaseParser.OPTIONS)
                    self.state = 585
                    self.tablePropertyList()


                pass

            elif la_ == 28:
                localctx = SparkSqlBaseParser.AlterViewQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 28)
                self.state = 588
                self.match(SparkSqlBaseParser.ALTER)
                self.state = 589
                self.match(SparkSqlBaseParser.VIEW)
                self.state = 590
                self.tableIdentifier()
                self.state = 592
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.AS:
                    self.state = 591
                    self.match(SparkSqlBaseParser.AS)


                self.state = 594
                self.query()
                pass

            elif la_ == 29:
                localctx = SparkSqlBaseParser.CreateFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 29)
                self.state = 596
                self.match(SparkSqlBaseParser.CREATE)
                self.state = 599
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OR:
                    self.state = 597
                    self.match(SparkSqlBaseParser.OR)
                    self.state = 598
                    self.match(SparkSqlBaseParser.REPLACE)


                self.state = 602
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.TEMPORARY:
                    self.state = 601
                    self.match(SparkSqlBaseParser.TEMPORARY)


                self.state = 604
                self.match(SparkSqlBaseParser.FUNCTION)
                self.state = 608
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,55,self._ctx)
                if la_ == 1:
                    self.state = 605
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 606
                    self.match(SparkSqlBaseParser.NOT)
                    self.state = 607
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 610
                self.qualifiedName()
                self.state = 611
                self.match(SparkSqlBaseParser.AS)
                self.state = 612
                localctx.className = self.match(SparkSqlBaseParser.STRING)
                self.state = 622
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.USING:
                    self.state = 613
                    self.match(SparkSqlBaseParser.USING)
                    self.state = 614
                    self.resource()
                    self.state = 619
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__2:
                        self.state = 615
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 616
                        self.resource()
                        self.state = 621
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                pass

            elif la_ == 30:
                localctx = SparkSqlBaseParser.DropFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 30)
                self.state = 624
                self.match(SparkSqlBaseParser.DROP)
                self.state = 626
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.TEMPORARY:
                    self.state = 625
                    self.match(SparkSqlBaseParser.TEMPORARY)


                self.state = 628
                self.match(SparkSqlBaseParser.FUNCTION)
                self.state = 631
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,59,self._ctx)
                if la_ == 1:
                    self.state = 629
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 630
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 633
                self.qualifiedName()
                pass

            elif la_ == 31:
                localctx = SparkSqlBaseParser.ExplainContext(self, localctx)
                self.enterOuterAlt(localctx, 31)
                self.state = 634
                self.match(SparkSqlBaseParser.EXPLAIN)
                self.state = 636
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if ((((_la - 90)) & ~0x3f) == 0 and ((1 << (_la - 90)) & ((1 << (SparkSqlBaseParser.LOGICAL - 90)) | (1 << (SparkSqlBaseParser.CODEGEN - 90)) | (1 << (SparkSqlBaseParser.COST - 90)))) != 0) or _la==SparkSqlBaseParser.EXTENDED or _la==SparkSqlBaseParser.FORMATTED:
                    self.state = 635
                    _la = self._input.LA(1)
                    if not(((((_la - 90)) & ~0x3f) == 0 and ((1 << (_la - 90)) & ((1 << (SparkSqlBaseParser.LOGICAL - 90)) | (1 << (SparkSqlBaseParser.CODEGEN - 90)) | (1 << (SparkSqlBaseParser.COST - 90)))) != 0) or _la==SparkSqlBaseParser.EXTENDED or _la==SparkSqlBaseParser.FORMATTED):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                self.state = 638
                self.statement()
                pass

            elif la_ == 32:
                localctx = SparkSqlBaseParser.ShowTablesContext(self, localctx)
                self.enterOuterAlt(localctx, 32)
                self.state = 639
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 640
                self.match(SparkSqlBaseParser.TABLES)
                self.state = 643
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.IN:
                    self.state = 641
                    _la = self._input.LA(1)
                    if not(_la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 642
                    localctx.db = self.identifier()


                self.state = 649
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LIKE or _la==SparkSqlBaseParser.STRING:
                    self.state = 646
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.LIKE:
                        self.state = 645
                        self.match(SparkSqlBaseParser.LIKE)


                    self.state = 648
                    localctx.pattern = self.match(SparkSqlBaseParser.STRING)


                pass

            elif la_ == 33:
                localctx = SparkSqlBaseParser.ShowTableContext(self, localctx)
                self.enterOuterAlt(localctx, 33)
                self.state = 651
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 652
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 653
                self.match(SparkSqlBaseParser.EXTENDED)
                self.state = 656
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.IN:
                    self.state = 654
                    _la = self._input.LA(1)
                    if not(_la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 655
                    localctx.db = self.identifier()


                self.state = 658
                self.match(SparkSqlBaseParser.LIKE)
                self.state = 659
                localctx.pattern = self.match(SparkSqlBaseParser.STRING)
                self.state = 661
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 660
                    self.partitionSpec()


                pass

            elif la_ == 34:
                localctx = SparkSqlBaseParser.ShowDatabasesContext(self, localctx)
                self.enterOuterAlt(localctx, 34)
                self.state = 663
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 664
                self.match(SparkSqlBaseParser.DATABASES)
                self.state = 669
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LIKE or _la==SparkSqlBaseParser.STRING:
                    self.state = 666
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.LIKE:
                        self.state = 665
                        self.match(SparkSqlBaseParser.LIKE)


                    self.state = 668
                    localctx.pattern = self.match(SparkSqlBaseParser.STRING)


                pass

            elif la_ == 35:
                localctx = SparkSqlBaseParser.ShowTblPropertiesContext(self, localctx)
                self.enterOuterAlt(localctx, 35)
                self.state = 671
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 672
                self.match(SparkSqlBaseParser.TBLPROPERTIES)
                self.state = 673
                localctx.table = self.tableIdentifier()
                self.state = 678
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.T__0:
                    self.state = 674
                    self.match(SparkSqlBaseParser.T__0)
                    self.state = 675
                    localctx.key = self.tablePropertyKey()
                    self.state = 676
                    self.match(SparkSqlBaseParser.T__1)


                pass

            elif la_ == 36:
                localctx = SparkSqlBaseParser.ShowColumnsContext(self, localctx)
                self.enterOuterAlt(localctx, 36)
                self.state = 680
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 681
                self.match(SparkSqlBaseParser.COLUMNS)
                self.state = 682
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.IN):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 683
                self.tableIdentifier()
                self.state = 686
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.IN:
                    self.state = 684
                    _la = self._input.LA(1)
                    if not(_la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.IN):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()
                    self.state = 685
                    localctx.db = self.identifier()


                pass

            elif la_ == 37:
                localctx = SparkSqlBaseParser.ShowPartitionsContext(self, localctx)
                self.enterOuterAlt(localctx, 37)
                self.state = 688
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 689
                self.match(SparkSqlBaseParser.PARTITIONS)
                self.state = 690
                self.tableIdentifier()
                self.state = 692
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 691
                    self.partitionSpec()


                pass

            elif la_ == 38:
                localctx = SparkSqlBaseParser.ShowFunctionsContext(self, localctx)
                self.enterOuterAlt(localctx, 38)
                self.state = 694
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 696
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,71,self._ctx)
                if la_ == 1:
                    self.state = 695
                    self.identifier()


                self.state = 698
                self.match(SparkSqlBaseParser.FUNCTIONS)
                self.state = 706
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.STRING - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                    self.state = 700
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,72,self._ctx)
                    if la_ == 1:
                        self.state = 699
                        self.match(SparkSqlBaseParser.LIKE)


                    self.state = 704
                    self._errHandler.sync(self)
                    token = self._input.LA(1)
                    if token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.ADD, SparkSqlBaseParser.AS, SparkSqlBaseParser.ALL, SparkSqlBaseParser.ANY, SparkSqlBaseParser.DISTINCT, SparkSqlBaseParser.WHERE, SparkSqlBaseParser.GROUP, SparkSqlBaseParser.BY, SparkSqlBaseParser.GROUPING, SparkSqlBaseParser.SETS, SparkSqlBaseParser.CUBE, SparkSqlBaseParser.ROLLUP, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.HAVING, SparkSqlBaseParser.LIMIT, SparkSqlBaseParser.AT, SparkSqlBaseParser.OR, SparkSqlBaseParser.AND, SparkSqlBaseParser.IN, SparkSqlBaseParser.NOT, SparkSqlBaseParser.NO, SparkSqlBaseParser.EXISTS, SparkSqlBaseParser.BETWEEN, SparkSqlBaseParser.LIKE, SparkSqlBaseParser.RLIKE, SparkSqlBaseParser.IS, SparkSqlBaseParser.NULL, SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE, SparkSqlBaseParser.NULLS, SparkSqlBaseParser.ASC, SparkSqlBaseParser.DESC, SparkSqlBaseParser.FOR, SparkSqlBaseParser.INTERVAL, SparkSqlBaseParser.CASE, SparkSqlBaseParser.WHEN, SparkSqlBaseParser.THEN, SparkSqlBaseParser.ELSE, SparkSqlBaseParser.END, SparkSqlBaseParser.JOIN, SparkSqlBaseParser.CROSS, SparkSqlBaseParser.OUTER, SparkSqlBaseParser.INNER, SparkSqlBaseParser.LEFT, SparkSqlBaseParser.SEMI, SparkSqlBaseParser.RIGHT, SparkSqlBaseParser.FULL, SparkSqlBaseParser.NATURAL, SparkSqlBaseParser.ON, SparkSqlBaseParser.PIVOT, SparkSqlBaseParser.LATERAL, SparkSqlBaseParser.WINDOW, SparkSqlBaseParser.OVER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.UNBOUNDED, SparkSqlBaseParser.PRECEDING, SparkSqlBaseParser.FOLLOWING, SparkSqlBaseParser.CURRENT, SparkSqlBaseParser.FIRST, SparkSqlBaseParser.AFTER, SparkSqlBaseParser.LAST, SparkSqlBaseParser.ROW, SparkSqlBaseParser.WITH, SparkSqlBaseParser.VALUES, SparkSqlBaseParser.CREATE, SparkSqlBaseParser.TABLE, SparkSqlBaseParser.DIRECTORY, SparkSqlBaseParser.VIEW, SparkSqlBaseParser.REPLACE, SparkSqlBaseParser.INSERT, SparkSqlBaseParser.DELETE, SparkSqlBaseParser.INTO, SparkSqlBaseParser.DESCRIBE, SparkSqlBaseParser.EXPLAIN, SparkSqlBaseParser.FORMAT, SparkSqlBaseParser.LOGICAL, SparkSqlBaseParser.CODEGEN, SparkSqlBaseParser.COST, SparkSqlBaseParser.CAST, SparkSqlBaseParser.SHOW, SparkSqlBaseParser.TABLES, SparkSqlBaseParser.COLUMNS, SparkSqlBaseParser.COLUMN, SparkSqlBaseParser.USE, SparkSqlBaseParser.PARTITIONS, SparkSqlBaseParser.FUNCTIONS, SparkSqlBaseParser.DROP, SparkSqlBaseParser.UNION, SparkSqlBaseParser.EXCEPT, SparkSqlBaseParser.SETMINUS, SparkSqlBaseParser.INTERSECT, SparkSqlBaseParser.TO, SparkSqlBaseParser.TABLESAMPLE, SparkSqlBaseParser.STRATIFY, SparkSqlBaseParser.ALTER, SparkSqlBaseParser.RENAME, SparkSqlBaseParser.ARRAY, SparkSqlBaseParser.MAP, SparkSqlBaseParser.STRUCT, SparkSqlBaseParser.COMMENT, SparkSqlBaseParser.SET, SparkSqlBaseParser.RESET, SparkSqlBaseParser.DATA, SparkSqlBaseParser.START, SparkSqlBaseParser.TRANSACTION, SparkSqlBaseParser.COMMIT, SparkSqlBaseParser.ROLLBACK, SparkSqlBaseParser.MACRO, SparkSqlBaseParser.IGNORE, SparkSqlBaseParser.BOTH, SparkSqlBaseParser.LEADING, SparkSqlBaseParser.TRAILING, SparkSqlBaseParser.IF, SparkSqlBaseParser.POSITION, SparkSqlBaseParser.EXTRACT, SparkSqlBaseParser.DIV, SparkSqlBaseParser.PERCENTLIT, SparkSqlBaseParser.BUCKET, SparkSqlBaseParser.OUT, SparkSqlBaseParser.OF, SparkSqlBaseParser.SORT, SparkSqlBaseParser.CLUSTER, SparkSqlBaseParser.DISTRIBUTE, SparkSqlBaseParser.OVERWRITE, SparkSqlBaseParser.TRANSFORM, SparkSqlBaseParser.REDUCE, SparkSqlBaseParser.SERDE, SparkSqlBaseParser.SERDEPROPERTIES, SparkSqlBaseParser.RECORDREADER, SparkSqlBaseParser.RECORDWRITER, SparkSqlBaseParser.DELIMITED, SparkSqlBaseParser.FIELDS, SparkSqlBaseParser.TERMINATED, SparkSqlBaseParser.COLLECTION, SparkSqlBaseParser.ITEMS, SparkSqlBaseParser.KEYS, SparkSqlBaseParser.ESCAPED, SparkSqlBaseParser.LINES, SparkSqlBaseParser.SEPARATED, SparkSqlBaseParser.FUNCTION, SparkSqlBaseParser.EXTENDED, SparkSqlBaseParser.REFRESH, SparkSqlBaseParser.CLEAR, SparkSqlBaseParser.CACHE, SparkSqlBaseParser.UNCACHE, SparkSqlBaseParser.LAZY, SparkSqlBaseParser.FORMATTED, SparkSqlBaseParser.GLOBAL, SparkSqlBaseParser.TEMPORARY, SparkSqlBaseParser.OPTIONS, SparkSqlBaseParser.UNSET, SparkSqlBaseParser.TBLPROPERTIES, SparkSqlBaseParser.DBPROPERTIES, SparkSqlBaseParser.BUCKETS, SparkSqlBaseParser.SKEWED, SparkSqlBaseParser.STORED, SparkSqlBaseParser.DIRECTORIES, SparkSqlBaseParser.LOCATION, SparkSqlBaseParser.EXCHANGE, SparkSqlBaseParser.ARCHIVE, SparkSqlBaseParser.UNARCHIVE, SparkSqlBaseParser.FILEFORMAT, SparkSqlBaseParser.TOUCH, SparkSqlBaseParser.COMPACT, SparkSqlBaseParser.CONCATENATE, SparkSqlBaseParser.CHANGE, SparkSqlBaseParser.CASCADE, SparkSqlBaseParser.RESTRICT, SparkSqlBaseParser.CLUSTERED, SparkSqlBaseParser.SORTED, SparkSqlBaseParser.PURGE, SparkSqlBaseParser.INPUTFORMAT, SparkSqlBaseParser.OUTPUTFORMAT, SparkSqlBaseParser.DATABASE, SparkSqlBaseParser.DATABASES, SparkSqlBaseParser.DFS, SparkSqlBaseParser.TRUNCATE, SparkSqlBaseParser.ANALYZE, SparkSqlBaseParser.COMPUTE, SparkSqlBaseParser.LIST, SparkSqlBaseParser.STATISTICS, SparkSqlBaseParser.PARTITIONED, SparkSqlBaseParser.EXTERNAL, SparkSqlBaseParser.DEFINED, SparkSqlBaseParser.REVOKE, SparkSqlBaseParser.GRANT, SparkSqlBaseParser.LOCK, SparkSqlBaseParser.UNLOCK, SparkSqlBaseParser.MSCK, SparkSqlBaseParser.REPAIR, SparkSqlBaseParser.RECOVER, SparkSqlBaseParser.EXPORT, SparkSqlBaseParser.IMPORT, SparkSqlBaseParser.LOAD, SparkSqlBaseParser.ROLE, SparkSqlBaseParser.ROLES, SparkSqlBaseParser.COMPACTIONS, SparkSqlBaseParser.PRINCIPALS, SparkSqlBaseParser.TRANSACTIONS, SparkSqlBaseParser.INDEX, SparkSqlBaseParser.INDEXES, SparkSqlBaseParser.LOCKS, SparkSqlBaseParser.OPTION, SparkSqlBaseParser.ANTI, SparkSqlBaseParser.LOCAL, SparkSqlBaseParser.INPATH, SparkSqlBaseParser.IDENTIFIER, SparkSqlBaseParser.BACKQUOTED_IDENTIFIER]:
                        self.state = 702
                        self.qualifiedName()
                        pass
                    elif token in [SparkSqlBaseParser.STRING]:
                        self.state = 703
                        localctx.pattern = self.match(SparkSqlBaseParser.STRING)
                        pass
                    else:
                        raise NoViableAltException(self)



                pass

            elif la_ == 39:
                localctx = SparkSqlBaseParser.ShowCreateTableContext(self, localctx)
                self.enterOuterAlt(localctx, 39)
                self.state = 708
                self.match(SparkSqlBaseParser.SHOW)
                self.state = 709
                self.match(SparkSqlBaseParser.CREATE)
                self.state = 710
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 711
                self.tableIdentifier()
                pass

            elif la_ == 40:
                localctx = SparkSqlBaseParser.DescribeFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 40)
                self.state = 712
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.DESC or _la==SparkSqlBaseParser.DESCRIBE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 713
                self.match(SparkSqlBaseParser.FUNCTION)
                self.state = 715
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,75,self._ctx)
                if la_ == 1:
                    self.state = 714
                    self.match(SparkSqlBaseParser.EXTENDED)


                self.state = 717
                self.describeFuncName()
                pass

            elif la_ == 41:
                localctx = SparkSqlBaseParser.DescribeDatabaseContext(self, localctx)
                self.enterOuterAlt(localctx, 41)
                self.state = 718
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.DESC or _la==SparkSqlBaseParser.DESCRIBE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 719
                self.match(SparkSqlBaseParser.DATABASE)
                self.state = 721
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,76,self._ctx)
                if la_ == 1:
                    self.state = 720
                    self.match(SparkSqlBaseParser.EXTENDED)


                self.state = 723
                self.identifier()
                pass

            elif la_ == 42:
                localctx = SparkSqlBaseParser.DescribeTableContext(self, localctx)
                self.enterOuterAlt(localctx, 42)
                self.state = 724
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.DESC or _la==SparkSqlBaseParser.DESCRIBE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 726
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,77,self._ctx)
                if la_ == 1:
                    self.state = 725
                    self.match(SparkSqlBaseParser.TABLE)


                self.state = 729
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,78,self._ctx)
                if la_ == 1:
                    self.state = 728
                    localctx.option = self._input.LT(1)
                    _la = self._input.LA(1)
                    if not(_la==SparkSqlBaseParser.EXTENDED or _la==SparkSqlBaseParser.FORMATTED):
                        localctx.option = self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                self.state = 731
                self.tableIdentifier()
                self.state = 733
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,79,self._ctx)
                if la_ == 1:
                    self.state = 732
                    self.partitionSpec()


                self.state = 736
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                    self.state = 735
                    self.describeColName()


                pass

            elif la_ == 43:
                localctx = SparkSqlBaseParser.RefreshTableContext(self, localctx)
                self.enterOuterAlt(localctx, 43)
                self.state = 738
                self.match(SparkSqlBaseParser.REFRESH)
                self.state = 739
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 740
                self.tableIdentifier()
                pass

            elif la_ == 44:
                localctx = SparkSqlBaseParser.RefreshResourceContext(self, localctx)
                self.enterOuterAlt(localctx, 44)
                self.state = 741
                self.match(SparkSqlBaseParser.REFRESH)
                self.state = 749
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,82,self._ctx)
                if la_ == 1:
                    self.state = 742
                    self.match(SparkSqlBaseParser.STRING)
                    pass

                elif la_ == 2:
                    self.state = 746
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,81,self._ctx)
                    while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                        if _alt==1+1:
                            self.state = 743
                            self.matchWildcard() 
                        self.state = 748
                        self._errHandler.sync(self)
                        _alt = self._interp.adaptivePredict(self._input,81,self._ctx)

                    pass


                pass

            elif la_ == 45:
                localctx = SparkSqlBaseParser.CacheTableContext(self, localctx)
                self.enterOuterAlt(localctx, 45)
                self.state = 751
                self.match(SparkSqlBaseParser.CACHE)
                self.state = 753
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LAZY:
                    self.state = 752
                    self.match(SparkSqlBaseParser.LAZY)


                self.state = 755
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 756
                self.tableIdentifier()
                self.state = 759
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OPTIONS:
                    self.state = 757
                    self.match(SparkSqlBaseParser.OPTIONS)
                    self.state = 758
                    localctx.options = self.tablePropertyList()


                self.state = 765
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.AS))) != 0) or ((((_la - 77)) & ~0x3f) == 0 and ((1 << (_la - 77)) & ((1 << (SparkSqlBaseParser.WITH - 77)) | (1 << (SparkSqlBaseParser.VALUES - 77)) | (1 << (SparkSqlBaseParser.TABLE - 77)) | (1 << (SparkSqlBaseParser.INSERT - 77)) | (1 << (SparkSqlBaseParser.MAP - 77)))) != 0) or _la==SparkSqlBaseParser.REDUCE:
                    self.state = 762
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.AS:
                        self.state = 761
                        self.match(SparkSqlBaseParser.AS)


                    self.state = 764
                    self.query()


                pass

            elif la_ == 46:
                localctx = SparkSqlBaseParser.UncacheTableContext(self, localctx)
                self.enterOuterAlt(localctx, 46)
                self.state = 767
                self.match(SparkSqlBaseParser.UNCACHE)
                self.state = 768
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 771
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,87,self._ctx)
                if la_ == 1:
                    self.state = 769
                    self.match(SparkSqlBaseParser.IF)
                    self.state = 770
                    self.match(SparkSqlBaseParser.EXISTS)


                self.state = 773
                self.tableIdentifier()
                pass

            elif la_ == 47:
                localctx = SparkSqlBaseParser.ClearCacheContext(self, localctx)
                self.enterOuterAlt(localctx, 47)
                self.state = 774
                self.match(SparkSqlBaseParser.CLEAR)
                self.state = 775
                self.match(SparkSqlBaseParser.CACHE)
                pass

            elif la_ == 48:
                localctx = SparkSqlBaseParser.LoadDataContext(self, localctx)
                self.enterOuterAlt(localctx, 48)
                self.state = 776
                self.match(SparkSqlBaseParser.LOAD)
                self.state = 777
                self.match(SparkSqlBaseParser.DATA)
                self.state = 779
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LOCAL:
                    self.state = 778
                    self.match(SparkSqlBaseParser.LOCAL)


                self.state = 781
                self.match(SparkSqlBaseParser.INPATH)
                self.state = 782
                localctx.path = self.match(SparkSqlBaseParser.STRING)
                self.state = 784
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OVERWRITE:
                    self.state = 783
                    self.match(SparkSqlBaseParser.OVERWRITE)


                self.state = 786
                self.match(SparkSqlBaseParser.INTO)
                self.state = 787
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 788
                self.tableIdentifier()
                self.state = 790
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 789
                    self.partitionSpec()


                pass

            elif la_ == 49:
                localctx = SparkSqlBaseParser.TruncateTableContext(self, localctx)
                self.enterOuterAlt(localctx, 49)
                self.state = 792
                self.match(SparkSqlBaseParser.TRUNCATE)
                self.state = 793
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 794
                self.tableIdentifier()
                self.state = 796
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 795
                    self.partitionSpec()


                pass

            elif la_ == 50:
                localctx = SparkSqlBaseParser.RepairTableContext(self, localctx)
                self.enterOuterAlt(localctx, 50)
                self.state = 798
                self.match(SparkSqlBaseParser.MSCK)
                self.state = 799
                self.match(SparkSqlBaseParser.REPAIR)
                self.state = 800
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 801
                self.tableIdentifier()
                pass

            elif la_ == 51:
                localctx = SparkSqlBaseParser.ManageResourceContext(self, localctx)
                self.enterOuterAlt(localctx, 51)
                self.state = 802
                localctx.op = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.ADD or _la==SparkSqlBaseParser.LIST):
                    localctx.op = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 803
                self.identifier()
                self.state = 807
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,92,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 804
                        self.matchWildcard() 
                    self.state = 809
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,92,self._ctx)

                pass

            elif la_ == 52:
                localctx = SparkSqlBaseParser.FailNativeCommandContext(self, localctx)
                self.enterOuterAlt(localctx, 52)
                self.state = 810
                self.match(SparkSqlBaseParser.SET)
                self.state = 811
                self.match(SparkSqlBaseParser.ROLE)
                self.state = 815
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,93,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 812
                        self.matchWildcard() 
                    self.state = 817
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,93,self._ctx)

                pass

            elif la_ == 53:
                localctx = SparkSqlBaseParser.SetConfigurationContext(self, localctx)
                self.enterOuterAlt(localctx, 53)
                self.state = 818
                self.match(SparkSqlBaseParser.SET)
                self.state = 822
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,94,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 819
                        self.matchWildcard() 
                    self.state = 824
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,94,self._ctx)

                pass

            elif la_ == 54:
                localctx = SparkSqlBaseParser.ResetConfigurationContext(self, localctx)
                self.enterOuterAlt(localctx, 54)
                self.state = 825
                self.match(SparkSqlBaseParser.RESET)
                pass

            elif la_ == 55:
                localctx = SparkSqlBaseParser.FailNativeCommandContext(self, localctx)
                self.enterOuterAlt(localctx, 55)
                self.state = 826
                self.unsupportedHiveNativeCommands()
                self.state = 830
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,95,self._ctx)
                while _alt!=1 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1+1:
                        self.state = 827
                        self.matchWildcard() 
                    self.state = 832
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,95,self._ctx)

                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class UnsupportedHiveNativeCommandsContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.kw1 = None # Token
            self.kw2 = None # Token
            self.kw3 = None # Token
            self.kw4 = None # Token
            self.kw5 = None # Token
            self.kw6 = None # Token

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)

        def ROLE(self):
            return self.getToken(SparkSqlBaseParser.ROLE, 0)

        def DROP(self):
            return self.getToken(SparkSqlBaseParser.DROP, 0)

        def GRANT(self):
            return self.getToken(SparkSqlBaseParser.GRANT, 0)

        def REVOKE(self):
            return self.getToken(SparkSqlBaseParser.REVOKE, 0)

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)

        def PRINCIPALS(self):
            return self.getToken(SparkSqlBaseParser.PRINCIPALS, 0)

        def ROLES(self):
            return self.getToken(SparkSqlBaseParser.ROLES, 0)

        def CURRENT(self):
            return self.getToken(SparkSqlBaseParser.CURRENT, 0)

        def EXPORT(self):
            return self.getToken(SparkSqlBaseParser.EXPORT, 0)

        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)

        def IMPORT(self):
            return self.getToken(SparkSqlBaseParser.IMPORT, 0)

        def COMPACTIONS(self):
            return self.getToken(SparkSqlBaseParser.COMPACTIONS, 0)

        def TRANSACTIONS(self):
            return self.getToken(SparkSqlBaseParser.TRANSACTIONS, 0)

        def INDEXES(self):
            return self.getToken(SparkSqlBaseParser.INDEXES, 0)

        def LOCKS(self):
            return self.getToken(SparkSqlBaseParser.LOCKS, 0)

        def INDEX(self):
            return self.getToken(SparkSqlBaseParser.INDEX, 0)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)

        def LOCK(self):
            return self.getToken(SparkSqlBaseParser.LOCK, 0)

        def DATABASE(self):
            return self.getToken(SparkSqlBaseParser.DATABASE, 0)

        def UNLOCK(self):
            return self.getToken(SparkSqlBaseParser.UNLOCK, 0)

        def TEMPORARY(self):
            return self.getToken(SparkSqlBaseParser.TEMPORARY, 0)

        def MACRO(self):
            return self.getToken(SparkSqlBaseParser.MACRO, 0)

        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)


        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)

        def CLUSTERED(self):
            return self.getToken(SparkSqlBaseParser.CLUSTERED, 0)

        def BY(self):
            return self.getToken(SparkSqlBaseParser.BY, 0)

        def SORTED(self):
            return self.getToken(SparkSqlBaseParser.SORTED, 0)

        def SKEWED(self):
            return self.getToken(SparkSqlBaseParser.SKEWED, 0)

        def STORED(self):
            return self.getToken(SparkSqlBaseParser.STORED, 0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def DIRECTORIES(self):
            return self.getToken(SparkSqlBaseParser.DIRECTORIES, 0)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)

        def LOCATION(self):
            return self.getToken(SparkSqlBaseParser.LOCATION, 0)

        def EXCHANGE(self):
            return self.getToken(SparkSqlBaseParser.EXCHANGE, 0)

        def PARTITION(self):
            return self.getToken(SparkSqlBaseParser.PARTITION, 0)

        def ARCHIVE(self):
            return self.getToken(SparkSqlBaseParser.ARCHIVE, 0)

        def UNARCHIVE(self):
            return self.getToken(SparkSqlBaseParser.UNARCHIVE, 0)

        def TOUCH(self):
            return self.getToken(SparkSqlBaseParser.TOUCH, 0)

        def COMPACT(self):
            return self.getToken(SparkSqlBaseParser.COMPACT, 0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)


        def CONCATENATE(self):
            return self.getToken(SparkSqlBaseParser.CONCATENATE, 0)

        def FILEFORMAT(self):
            return self.getToken(SparkSqlBaseParser.FILEFORMAT, 0)

        def REPLACE(self):
            return self.getToken(SparkSqlBaseParser.REPLACE, 0)

        def COLUMNS(self):
            return self.getToken(SparkSqlBaseParser.COLUMNS, 0)

        def START(self):
            return self.getToken(SparkSqlBaseParser.START, 0)

        def TRANSACTION(self):
            return self.getToken(SparkSqlBaseParser.TRANSACTION, 0)

        def COMMIT(self):
            return self.getToken(SparkSqlBaseParser.COMMIT, 0)

        def ROLLBACK(self):
            return self.getToken(SparkSqlBaseParser.ROLLBACK, 0)

        def DFS(self):
            return self.getToken(SparkSqlBaseParser.DFS, 0)

        def DELETE(self):
            return self.getToken(SparkSqlBaseParser.DELETE, 0)

        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_unsupportedHiveNativeCommands

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnsupportedHiveNativeCommands" ):
                listener.enterUnsupportedHiveNativeCommands(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnsupportedHiveNativeCommands" ):
                listener.exitUnsupportedHiveNativeCommands(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnsupportedHiveNativeCommands" ):
                return visitor.visitUnsupportedHiveNativeCommands(self)
            else:
                return visitor.visitChildren(self)




    def unsupportedHiveNativeCommands(self):

        localctx = SparkSqlBaseParser.UnsupportedHiveNativeCommandsContext(self, self._ctx, self.state)
        self.enterRule(localctx, 14, self.RULE_unsupportedHiveNativeCommands)
        self._la = 0 # Token type
        try:
            self.state = 1005
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,104,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 835
                localctx.kw1 = self.match(SparkSqlBaseParser.CREATE)
                self.state = 836
                localctx.kw2 = self.match(SparkSqlBaseParser.ROLE)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 837
                localctx.kw1 = self.match(SparkSqlBaseParser.DROP)
                self.state = 838
                localctx.kw2 = self.match(SparkSqlBaseParser.ROLE)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 839
                localctx.kw1 = self.match(SparkSqlBaseParser.GRANT)
                self.state = 841
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,97,self._ctx)
                if la_ == 1:
                    self.state = 840
                    localctx.kw2 = self.match(SparkSqlBaseParser.ROLE)


                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 843
                localctx.kw1 = self.match(SparkSqlBaseParser.REVOKE)
                self.state = 845
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,98,self._ctx)
                if la_ == 1:
                    self.state = 844
                    localctx.kw2 = self.match(SparkSqlBaseParser.ROLE)


                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 847
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 848
                localctx.kw2 = self.match(SparkSqlBaseParser.GRANT)
                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 849
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 850
                localctx.kw2 = self.match(SparkSqlBaseParser.ROLE)
                self.state = 852
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,99,self._ctx)
                if la_ == 1:
                    self.state = 851
                    localctx.kw3 = self.match(SparkSqlBaseParser.GRANT)


                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 854
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 855
                localctx.kw2 = self.match(SparkSqlBaseParser.PRINCIPALS)
                pass

            elif la_ == 8:
                self.enterOuterAlt(localctx, 8)
                self.state = 856
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 857
                localctx.kw2 = self.match(SparkSqlBaseParser.ROLES)
                pass

            elif la_ == 9:
                self.enterOuterAlt(localctx, 9)
                self.state = 858
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 859
                localctx.kw2 = self.match(SparkSqlBaseParser.CURRENT)
                self.state = 860
                localctx.kw3 = self.match(SparkSqlBaseParser.ROLES)
                pass

            elif la_ == 10:
                self.enterOuterAlt(localctx, 10)
                self.state = 861
                localctx.kw1 = self.match(SparkSqlBaseParser.EXPORT)
                self.state = 862
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                pass

            elif la_ == 11:
                self.enterOuterAlt(localctx, 11)
                self.state = 863
                localctx.kw1 = self.match(SparkSqlBaseParser.IMPORT)
                self.state = 864
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                pass

            elif la_ == 12:
                self.enterOuterAlt(localctx, 12)
                self.state = 865
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 866
                localctx.kw2 = self.match(SparkSqlBaseParser.COMPACTIONS)
                pass

            elif la_ == 13:
                self.enterOuterAlt(localctx, 13)
                self.state = 867
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 868
                localctx.kw2 = self.match(SparkSqlBaseParser.CREATE)
                self.state = 869
                localctx.kw3 = self.match(SparkSqlBaseParser.TABLE)
                pass

            elif la_ == 14:
                self.enterOuterAlt(localctx, 14)
                self.state = 870
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 871
                localctx.kw2 = self.match(SparkSqlBaseParser.TRANSACTIONS)
                pass

            elif la_ == 15:
                self.enterOuterAlt(localctx, 15)
                self.state = 872
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 873
                localctx.kw2 = self.match(SparkSqlBaseParser.INDEXES)
                pass

            elif la_ == 16:
                self.enterOuterAlt(localctx, 16)
                self.state = 874
                localctx.kw1 = self.match(SparkSqlBaseParser.SHOW)
                self.state = 875
                localctx.kw2 = self.match(SparkSqlBaseParser.LOCKS)
                pass

            elif la_ == 17:
                self.enterOuterAlt(localctx, 17)
                self.state = 876
                localctx.kw1 = self.match(SparkSqlBaseParser.CREATE)
                self.state = 877
                localctx.kw2 = self.match(SparkSqlBaseParser.INDEX)
                pass

            elif la_ == 18:
                self.enterOuterAlt(localctx, 18)
                self.state = 878
                localctx.kw1 = self.match(SparkSqlBaseParser.DROP)
                self.state = 879
                localctx.kw2 = self.match(SparkSqlBaseParser.INDEX)
                pass

            elif la_ == 19:
                self.enterOuterAlt(localctx, 19)
                self.state = 880
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 881
                localctx.kw2 = self.match(SparkSqlBaseParser.INDEX)
                pass

            elif la_ == 20:
                self.enterOuterAlt(localctx, 20)
                self.state = 882
                localctx.kw1 = self.match(SparkSqlBaseParser.LOCK)
                self.state = 883
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                pass

            elif la_ == 21:
                self.enterOuterAlt(localctx, 21)
                self.state = 884
                localctx.kw1 = self.match(SparkSqlBaseParser.LOCK)
                self.state = 885
                localctx.kw2 = self.match(SparkSqlBaseParser.DATABASE)
                pass

            elif la_ == 22:
                self.enterOuterAlt(localctx, 22)
                self.state = 886
                localctx.kw1 = self.match(SparkSqlBaseParser.UNLOCK)
                self.state = 887
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                pass

            elif la_ == 23:
                self.enterOuterAlt(localctx, 23)
                self.state = 888
                localctx.kw1 = self.match(SparkSqlBaseParser.UNLOCK)
                self.state = 889
                localctx.kw2 = self.match(SparkSqlBaseParser.DATABASE)
                pass

            elif la_ == 24:
                self.enterOuterAlt(localctx, 24)
                self.state = 890
                localctx.kw1 = self.match(SparkSqlBaseParser.CREATE)
                self.state = 891
                localctx.kw2 = self.match(SparkSqlBaseParser.TEMPORARY)
                self.state = 892
                localctx.kw3 = self.match(SparkSqlBaseParser.MACRO)
                pass

            elif la_ == 25:
                self.enterOuterAlt(localctx, 25)
                self.state = 893
                localctx.kw1 = self.match(SparkSqlBaseParser.DROP)
                self.state = 894
                localctx.kw2 = self.match(SparkSqlBaseParser.TEMPORARY)
                self.state = 895
                localctx.kw3 = self.match(SparkSqlBaseParser.MACRO)
                pass

            elif la_ == 26:
                self.enterOuterAlt(localctx, 26)
                self.state = 896
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 897
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 898
                self.tableIdentifier()
                self.state = 899
                localctx.kw3 = self.match(SparkSqlBaseParser.NOT)
                self.state = 900
                localctx.kw4 = self.match(SparkSqlBaseParser.CLUSTERED)
                pass

            elif la_ == 27:
                self.enterOuterAlt(localctx, 27)
                self.state = 902
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 903
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 904
                self.tableIdentifier()
                self.state = 905
                localctx.kw3 = self.match(SparkSqlBaseParser.CLUSTERED)
                self.state = 906
                localctx.kw4 = self.match(SparkSqlBaseParser.BY)
                pass

            elif la_ == 28:
                self.enterOuterAlt(localctx, 28)
                self.state = 908
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 909
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 910
                self.tableIdentifier()
                self.state = 911
                localctx.kw3 = self.match(SparkSqlBaseParser.NOT)
                self.state = 912
                localctx.kw4 = self.match(SparkSqlBaseParser.SORTED)
                pass

            elif la_ == 29:
                self.enterOuterAlt(localctx, 29)
                self.state = 914
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 915
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 916
                self.tableIdentifier()
                self.state = 917
                localctx.kw3 = self.match(SparkSqlBaseParser.SKEWED)
                self.state = 918
                localctx.kw4 = self.match(SparkSqlBaseParser.BY)
                pass

            elif la_ == 30:
                self.enterOuterAlt(localctx, 30)
                self.state = 920
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 921
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 922
                self.tableIdentifier()
                self.state = 923
                localctx.kw3 = self.match(SparkSqlBaseParser.NOT)
                self.state = 924
                localctx.kw4 = self.match(SparkSqlBaseParser.SKEWED)
                pass

            elif la_ == 31:
                self.enterOuterAlt(localctx, 31)
                self.state = 926
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 927
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 928
                self.tableIdentifier()
                self.state = 929
                localctx.kw3 = self.match(SparkSqlBaseParser.NOT)
                self.state = 930
                localctx.kw4 = self.match(SparkSqlBaseParser.STORED)
                self.state = 931
                localctx.kw5 = self.match(SparkSqlBaseParser.AS)
                self.state = 932
                localctx.kw6 = self.match(SparkSqlBaseParser.DIRECTORIES)
                pass

            elif la_ == 32:
                self.enterOuterAlt(localctx, 32)
                self.state = 934
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 935
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 936
                self.tableIdentifier()
                self.state = 937
                localctx.kw3 = self.match(SparkSqlBaseParser.SET)
                self.state = 938
                localctx.kw4 = self.match(SparkSqlBaseParser.SKEWED)
                self.state = 939
                localctx.kw5 = self.match(SparkSqlBaseParser.LOCATION)
                pass

            elif la_ == 33:
                self.enterOuterAlt(localctx, 33)
                self.state = 941
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 942
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 943
                self.tableIdentifier()
                self.state = 944
                localctx.kw3 = self.match(SparkSqlBaseParser.EXCHANGE)
                self.state = 945
                localctx.kw4 = self.match(SparkSqlBaseParser.PARTITION)
                pass

            elif la_ == 34:
                self.enterOuterAlt(localctx, 34)
                self.state = 947
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 948
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 949
                self.tableIdentifier()
                self.state = 950
                localctx.kw3 = self.match(SparkSqlBaseParser.ARCHIVE)
                self.state = 951
                localctx.kw4 = self.match(SparkSqlBaseParser.PARTITION)
                pass

            elif la_ == 35:
                self.enterOuterAlt(localctx, 35)
                self.state = 953
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 954
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 955
                self.tableIdentifier()
                self.state = 956
                localctx.kw3 = self.match(SparkSqlBaseParser.UNARCHIVE)
                self.state = 957
                localctx.kw4 = self.match(SparkSqlBaseParser.PARTITION)
                pass

            elif la_ == 36:
                self.enterOuterAlt(localctx, 36)
                self.state = 959
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 960
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 961
                self.tableIdentifier()
                self.state = 962
                localctx.kw3 = self.match(SparkSqlBaseParser.TOUCH)
                pass

            elif la_ == 37:
                self.enterOuterAlt(localctx, 37)
                self.state = 964
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 965
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 966
                self.tableIdentifier()
                self.state = 968
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 967
                    self.partitionSpec()


                self.state = 970
                localctx.kw3 = self.match(SparkSqlBaseParser.COMPACT)
                pass

            elif la_ == 38:
                self.enterOuterAlt(localctx, 38)
                self.state = 972
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 973
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 974
                self.tableIdentifier()
                self.state = 976
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 975
                    self.partitionSpec()


                self.state = 978
                localctx.kw3 = self.match(SparkSqlBaseParser.CONCATENATE)
                pass

            elif la_ == 39:
                self.enterOuterAlt(localctx, 39)
                self.state = 980
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 981
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 982
                self.tableIdentifier()
                self.state = 984
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 983
                    self.partitionSpec()


                self.state = 986
                localctx.kw3 = self.match(SparkSqlBaseParser.SET)
                self.state = 987
                localctx.kw4 = self.match(SparkSqlBaseParser.FILEFORMAT)
                pass

            elif la_ == 40:
                self.enterOuterAlt(localctx, 40)
                self.state = 989
                localctx.kw1 = self.match(SparkSqlBaseParser.ALTER)
                self.state = 990
                localctx.kw2 = self.match(SparkSqlBaseParser.TABLE)
                self.state = 991
                self.tableIdentifier()
                self.state = 993
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 992
                    self.partitionSpec()


                self.state = 995
                localctx.kw3 = self.match(SparkSqlBaseParser.REPLACE)
                self.state = 996
                localctx.kw4 = self.match(SparkSqlBaseParser.COLUMNS)
                pass

            elif la_ == 41:
                self.enterOuterAlt(localctx, 41)
                self.state = 998
                localctx.kw1 = self.match(SparkSqlBaseParser.START)
                self.state = 999
                localctx.kw2 = self.match(SparkSqlBaseParser.TRANSACTION)
                pass

            elif la_ == 42:
                self.enterOuterAlt(localctx, 42)
                self.state = 1000
                localctx.kw1 = self.match(SparkSqlBaseParser.COMMIT)
                pass

            elif la_ == 43:
                self.enterOuterAlt(localctx, 43)
                self.state = 1001
                localctx.kw1 = self.match(SparkSqlBaseParser.ROLLBACK)
                pass

            elif la_ == 44:
                self.enterOuterAlt(localctx, 44)
                self.state = 1002
                localctx.kw1 = self.match(SparkSqlBaseParser.DFS)
                pass

            elif la_ == 45:
                self.enterOuterAlt(localctx, 45)
                self.state = 1003
                localctx.kw1 = self.match(SparkSqlBaseParser.DELETE)
                self.state = 1004
                localctx.kw2 = self.match(SparkSqlBaseParser.FROM)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class CreateTableHeaderContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)

        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)

        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)


        def TEMPORARY(self):
            return self.getToken(SparkSqlBaseParser.TEMPORARY, 0)

        def EXTERNAL(self):
            return self.getToken(SparkSqlBaseParser.EXTERNAL, 0)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)

        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)

        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_createTableHeader

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateTableHeader" ):
                listener.enterCreateTableHeader(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateTableHeader" ):
                listener.exitCreateTableHeader(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateTableHeader" ):
                return visitor.visitCreateTableHeader(self)
            else:
                return visitor.visitChildren(self)




    def createTableHeader(self):

        localctx = SparkSqlBaseParser.CreateTableHeaderContext(self, self._ctx, self.state)
        self.enterRule(localctx, 16, self.RULE_createTableHeader)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1007
            self.match(SparkSqlBaseParser.CREATE)
            self.state = 1009
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.TEMPORARY:
                self.state = 1008
                self.match(SparkSqlBaseParser.TEMPORARY)


            self.state = 1012
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.EXTERNAL:
                self.state = 1011
                self.match(SparkSqlBaseParser.EXTERNAL)


            self.state = 1014
            self.match(SparkSqlBaseParser.TABLE)
            self.state = 1018
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,107,self._ctx)
            if la_ == 1:
                self.state = 1015
                self.match(SparkSqlBaseParser.IF)
                self.state = 1016
                self.match(SparkSqlBaseParser.NOT)
                self.state = 1017
                self.match(SparkSqlBaseParser.EXISTS)


            self.state = 1020
            self.tableIdentifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class BucketSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def CLUSTERED(self):
            return self.getToken(SparkSqlBaseParser.CLUSTERED, 0)

        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.BY)
            else:
                return self.getToken(SparkSqlBaseParser.BY, i)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierListContext,0)


        def INTO(self):
            return self.getToken(SparkSqlBaseParser.INTO, 0)

        def INTEGER_VALUE(self):
            return self.getToken(SparkSqlBaseParser.INTEGER_VALUE, 0)

        def BUCKETS(self):
            return self.getToken(SparkSqlBaseParser.BUCKETS, 0)

        def SORTED(self):
            return self.getToken(SparkSqlBaseParser.SORTED, 0)

        def orderedIdentifierList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.OrderedIdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_bucketSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBucketSpec" ):
                listener.enterBucketSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBucketSpec" ):
                listener.exitBucketSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBucketSpec" ):
                return visitor.visitBucketSpec(self)
            else:
                return visitor.visitChildren(self)




    def bucketSpec(self):

        localctx = SparkSqlBaseParser.BucketSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 18, self.RULE_bucketSpec)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1022
            self.match(SparkSqlBaseParser.CLUSTERED)
            self.state = 1023
            self.match(SparkSqlBaseParser.BY)
            self.state = 1024
            self.identifierList()
            self.state = 1028
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.SORTED:
                self.state = 1025
                self.match(SparkSqlBaseParser.SORTED)
                self.state = 1026
                self.match(SparkSqlBaseParser.BY)
                self.state = 1027
                self.orderedIdentifierList()


            self.state = 1030
            self.match(SparkSqlBaseParser.INTO)
            self.state = 1031
            self.match(SparkSqlBaseParser.INTEGER_VALUE)
            self.state = 1032
            self.match(SparkSqlBaseParser.BUCKETS)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SkewSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def SKEWED(self):
            return self.getToken(SparkSqlBaseParser.SKEWED, 0)

        def BY(self):
            return self.getToken(SparkSqlBaseParser.BY, 0)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierListContext,0)


        def ON(self):
            return self.getToken(SparkSqlBaseParser.ON, 0)

        def constantList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ConstantListContext,0)


        def nestedConstantList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.NestedConstantListContext,0)


        def STORED(self):
            return self.getToken(SparkSqlBaseParser.STORED, 0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def DIRECTORIES(self):
            return self.getToken(SparkSqlBaseParser.DIRECTORIES, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_skewSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSkewSpec" ):
                listener.enterSkewSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSkewSpec" ):
                listener.exitSkewSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSkewSpec" ):
                return visitor.visitSkewSpec(self)
            else:
                return visitor.visitChildren(self)




    def skewSpec(self):

        localctx = SparkSqlBaseParser.SkewSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 20, self.RULE_skewSpec)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1034
            self.match(SparkSqlBaseParser.SKEWED)
            self.state = 1035
            self.match(SparkSqlBaseParser.BY)
            self.state = 1036
            self.identifierList()
            self.state = 1037
            self.match(SparkSqlBaseParser.ON)
            self.state = 1040
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,109,self._ctx)
            if la_ == 1:
                self.state = 1038
                self.constantList()
                pass

            elif la_ == 2:
                self.state = 1039
                self.nestedConstantList()
                pass


            self.state = 1045
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,110,self._ctx)
            if la_ == 1:
                self.state = 1042
                self.match(SparkSqlBaseParser.STORED)
                self.state = 1043
                self.match(SparkSqlBaseParser.AS)
                self.state = 1044
                self.match(SparkSqlBaseParser.DIRECTORIES)


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class LocationSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def LOCATION(self):
            return self.getToken(SparkSqlBaseParser.LOCATION, 0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_locationSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLocationSpec" ):
                listener.enterLocationSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLocationSpec" ):
                listener.exitLocationSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLocationSpec" ):
                return visitor.visitLocationSpec(self)
            else:
                return visitor.visitChildren(self)




    def locationSpec(self):

        localctx = SparkSqlBaseParser.LocationSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 22, self.RULE_locationSpec)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1047
            self.match(SparkSqlBaseParser.LOCATION)
            self.state = 1048
            self.match(SparkSqlBaseParser.STRING)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class QueryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def queryNoWith(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryNoWithContext,0)


        def ctes(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.CtesContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_query

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuery" ):
                listener.enterQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuery" ):
                listener.exitQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuery" ):
                return visitor.visitQuery(self)
            else:
                return visitor.visitChildren(self)




    def query(self):

        localctx = SparkSqlBaseParser.QueryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 24, self.RULE_query)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1051
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.WITH:
                self.state = 1050
                self.ctes()


            self.state = 1053
            self.queryNoWith()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class InsertIntoContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_insertInto

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class InsertOverwriteHiveDirContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.InsertIntoContext
            super().__init__(parser)
            self.path = None # Token
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSqlBaseParser.INSERT, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSqlBaseParser.OVERWRITE, 0)
        def DIRECTORY(self):
            return self.getToken(SparkSqlBaseParser.DIRECTORY, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def LOCAL(self):
            return self.getToken(SparkSqlBaseParser.LOCAL, 0)
        def rowFormat(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.RowFormatContext,0)

        def createFileFormat(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.CreateFileFormatContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertOverwriteHiveDir" ):
                listener.enterInsertOverwriteHiveDir(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertOverwriteHiveDir" ):
                listener.exitInsertOverwriteHiveDir(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertOverwriteHiveDir" ):
                return visitor.visitInsertOverwriteHiveDir(self)
            else:
                return visitor.visitChildren(self)


    class InsertOverwriteDirContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.InsertIntoContext
            super().__init__(parser)
            self.path = None # Token
            self.options = None # TablePropertyListContext
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSqlBaseParser.INSERT, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSqlBaseParser.OVERWRITE, 0)
        def DIRECTORY(self):
            return self.getToken(SparkSqlBaseParser.DIRECTORY, 0)
        def tableProvider(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableProviderContext,0)

        def LOCAL(self):
            return self.getToken(SparkSqlBaseParser.LOCAL, 0)
        def OPTIONS(self):
            return self.getToken(SparkSqlBaseParser.OPTIONS, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertOverwriteDir" ):
                listener.enterInsertOverwriteDir(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertOverwriteDir" ):
                listener.exitInsertOverwriteDir(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertOverwriteDir" ):
                return visitor.visitInsertOverwriteDir(self)
            else:
                return visitor.visitChildren(self)


    class InsertOverwriteTableContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.InsertIntoContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSqlBaseParser.INSERT, 0)
        def OVERWRITE(self):
            return self.getToken(SparkSqlBaseParser.OVERWRITE, 0)
        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)
        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)
        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertOverwriteTable" ):
                listener.enterInsertOverwriteTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertOverwriteTable" ):
                listener.exitInsertOverwriteTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertOverwriteTable" ):
                return visitor.visitInsertOverwriteTable(self)
            else:
                return visitor.visitChildren(self)


    class InsertIntoTableContext(InsertIntoContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.InsertIntoContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def INSERT(self):
            return self.getToken(SparkSqlBaseParser.INSERT, 0)
        def INTO(self):
            return self.getToken(SparkSqlBaseParser.INTO, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInsertIntoTable" ):
                listener.enterInsertIntoTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInsertIntoTable" ):
                listener.exitInsertIntoTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInsertIntoTable" ):
                return visitor.visitInsertIntoTable(self)
            else:
                return visitor.visitChildren(self)



    def insertInto(self):

        localctx = SparkSqlBaseParser.InsertIntoContext(self, self._ctx, self.state)
        self.enterRule(localctx, 26, self.RULE_insertInto)
        self._la = 0 # Token type
        try:
            self.state = 1103
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,122,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.InsertOverwriteTableContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1055
                self.match(SparkSqlBaseParser.INSERT)
                self.state = 1056
                self.match(SparkSqlBaseParser.OVERWRITE)
                self.state = 1057
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 1058
                self.tableIdentifier()
                self.state = 1065
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 1059
                    self.partitionSpec()
                    self.state = 1063
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.IF:
                        self.state = 1060
                        self.match(SparkSqlBaseParser.IF)
                        self.state = 1061
                        self.match(SparkSqlBaseParser.NOT)
                        self.state = 1062
                        self.match(SparkSqlBaseParser.EXISTS)




                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.InsertIntoTableContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1067
                self.match(SparkSqlBaseParser.INSERT)
                self.state = 1068
                self.match(SparkSqlBaseParser.INTO)
                self.state = 1070
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,114,self._ctx)
                if la_ == 1:
                    self.state = 1069
                    self.match(SparkSqlBaseParser.TABLE)


                self.state = 1072
                self.tableIdentifier()
                self.state = 1074
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PARTITION:
                    self.state = 1073
                    self.partitionSpec()


                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.InsertOverwriteHiveDirContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 1076
                self.match(SparkSqlBaseParser.INSERT)
                self.state = 1077
                self.match(SparkSqlBaseParser.OVERWRITE)
                self.state = 1079
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LOCAL:
                    self.state = 1078
                    self.match(SparkSqlBaseParser.LOCAL)


                self.state = 1081
                self.match(SparkSqlBaseParser.DIRECTORY)
                self.state = 1082
                localctx.path = self.match(SparkSqlBaseParser.STRING)
                self.state = 1084
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.ROW:
                    self.state = 1083
                    self.rowFormat()


                self.state = 1087
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.STORED:
                    self.state = 1086
                    self.createFileFormat()


                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.InsertOverwriteDirContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 1089
                self.match(SparkSqlBaseParser.INSERT)
                self.state = 1090
                self.match(SparkSqlBaseParser.OVERWRITE)
                self.state = 1092
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LOCAL:
                    self.state = 1091
                    self.match(SparkSqlBaseParser.LOCAL)


                self.state = 1094
                self.match(SparkSqlBaseParser.DIRECTORY)
                self.state = 1096
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.STRING:
                    self.state = 1095
                    localctx.path = self.match(SparkSqlBaseParser.STRING)


                self.state = 1098
                self.tableProvider()
                self.state = 1101
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OPTIONS:
                    self.state = 1099
                    self.match(SparkSqlBaseParser.OPTIONS)
                    self.state = 1100
                    localctx.options = self.tablePropertyList()


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PartitionSpecLocationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def partitionSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PartitionSpecContext,0)


        def locationSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.LocationSpecContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_partitionSpecLocation

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPartitionSpecLocation" ):
                listener.enterPartitionSpecLocation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPartitionSpecLocation" ):
                listener.exitPartitionSpecLocation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPartitionSpecLocation" ):
                return visitor.visitPartitionSpecLocation(self)
            else:
                return visitor.visitChildren(self)




    def partitionSpecLocation(self):

        localctx = SparkSqlBaseParser.PartitionSpecLocationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 28, self.RULE_partitionSpecLocation)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1105
            self.partitionSpec()
            self.state = 1107
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.LOCATION:
                self.state = 1106
                self.locationSpec()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PartitionSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def PARTITION(self):
            return self.getToken(SparkSqlBaseParser.PARTITION, 0)

        def partitionVal(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.PartitionValContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.PartitionValContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_partitionSpec

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPartitionSpec" ):
                listener.enterPartitionSpec(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPartitionSpec" ):
                listener.exitPartitionSpec(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPartitionSpec" ):
                return visitor.visitPartitionSpec(self)
            else:
                return visitor.visitChildren(self)




    def partitionSpec(self):

        localctx = SparkSqlBaseParser.PartitionSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 30, self.RULE_partitionSpec)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1109
            self.match(SparkSqlBaseParser.PARTITION)
            self.state = 1110
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1111
            self.partitionVal()
            self.state = 1116
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1112
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1113
                self.partitionVal()
                self.state = 1118
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1119
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PartitionValContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def EQ(self):
            return self.getToken(SparkSqlBaseParser.EQ, 0)

        def constant(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ConstantContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_partitionVal

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPartitionVal" ):
                listener.enterPartitionVal(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPartitionVal" ):
                listener.exitPartitionVal(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPartitionVal" ):
                return visitor.visitPartitionVal(self)
            else:
                return visitor.visitChildren(self)




    def partitionVal(self):

        localctx = SparkSqlBaseParser.PartitionValContext(self, self._ctx, self.state)
        self.enterRule(localctx, 32, self.RULE_partitionVal)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1121
            self.identifier()
            self.state = 1124
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.EQ:
                self.state = 1122
                self.match(SparkSqlBaseParser.EQ)
                self.state = 1123
                self.constant()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class DescribeFuncNameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)


        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def comparisonOperator(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ComparisonOperatorContext,0)


        def arithmeticOperator(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ArithmeticOperatorContext,0)


        def predicateOperator(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PredicateOperatorContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_describeFuncName

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeFuncName" ):
                listener.enterDescribeFuncName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeFuncName" ):
                listener.exitDescribeFuncName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeFuncName" ):
                return visitor.visitDescribeFuncName(self)
            else:
                return visitor.visitChildren(self)




    def describeFuncName(self):

        localctx = SparkSqlBaseParser.DescribeFuncNameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 34, self.RULE_describeFuncName)
        try:
            self.state = 1131
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,126,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1126
                self.qualifiedName()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1127
                self.match(SparkSqlBaseParser.STRING)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 1128
                self.comparisonOperator()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 1129
                self.arithmeticOperator()
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 1130
                self.predicateOperator()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class DescribeColNameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._identifier = None # IdentifierContext
            self.nameParts = list() # of IdentifierContexts

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_describeColName

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDescribeColName" ):
                listener.enterDescribeColName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDescribeColName" ):
                listener.exitDescribeColName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDescribeColName" ):
                return visitor.visitDescribeColName(self)
            else:
                return visitor.visitChildren(self)




    def describeColName(self):

        localctx = SparkSqlBaseParser.DescribeColNameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 36, self.RULE_describeColName)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1133
            localctx._identifier = self.identifier()
            localctx.nameParts.append(localctx._identifier)
            self.state = 1138
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__3:
                self.state = 1134
                self.match(SparkSqlBaseParser.T__3)
                self.state = 1135
                localctx._identifier = self.identifier()
                localctx.nameParts.append(localctx._identifier)
                self.state = 1140
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class CtesContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def WITH(self):
            return self.getToken(SparkSqlBaseParser.WITH, 0)

        def namedQuery(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.NamedQueryContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.NamedQueryContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_ctes

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCtes" ):
                listener.enterCtes(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCtes" ):
                listener.exitCtes(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCtes" ):
                return visitor.visitCtes(self)
            else:
                return visitor.visitChildren(self)




    def ctes(self):

        localctx = SparkSqlBaseParser.CtesContext(self, self._ctx, self.state)
        self.enterRule(localctx, 38, self.RULE_ctes)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1141
            self.match(SparkSqlBaseParser.WITH)
            self.state = 1142
            self.namedQuery()
            self.state = 1147
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1143
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1144
                self.namedQuery()
                self.state = 1149
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NamedQueryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.name = None # IdentifierContext

        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)


        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_namedQuery

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedQuery" ):
                listener.enterNamedQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedQuery" ):
                listener.exitNamedQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedQuery" ):
                return visitor.visitNamedQuery(self)
            else:
                return visitor.visitChildren(self)




    def namedQuery(self):

        localctx = SparkSqlBaseParser.NamedQueryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 40, self.RULE_namedQuery)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1150
            localctx.name = self.identifier()
            self.state = 1152
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.AS:
                self.state = 1151
                self.match(SparkSqlBaseParser.AS)


            self.state = 1154
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1155
            self.query()
            self.state = 1156
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class TableProviderContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def USING(self):
            return self.getToken(SparkSqlBaseParser.USING, 0)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_tableProvider

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableProvider" ):
                listener.enterTableProvider(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableProvider" ):
                listener.exitTableProvider(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableProvider" ):
                return visitor.visitTableProvider(self)
            else:
                return visitor.visitChildren(self)




    def tableProvider(self):

        localctx = SparkSqlBaseParser.TableProviderContext(self, self._ctx, self.state)
        self.enterRule(localctx, 42, self.RULE_tableProvider)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1158
            self.match(SparkSqlBaseParser.USING)
            self.state = 1159
            self.qualifiedName()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class TablePropertyListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def tableProperty(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.TablePropertyContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_tablePropertyList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTablePropertyList" ):
                listener.enterTablePropertyList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTablePropertyList" ):
                listener.exitTablePropertyList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTablePropertyList" ):
                return visitor.visitTablePropertyList(self)
            else:
                return visitor.visitChildren(self)




    def tablePropertyList(self):

        localctx = SparkSqlBaseParser.TablePropertyListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 44, self.RULE_tablePropertyList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1161
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1162
            self.tableProperty()
            self.state = 1167
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1163
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1164
                self.tableProperty()
                self.state = 1169
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1170
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class TablePropertyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.key = None # TablePropertyKeyContext
            self.value = None # TablePropertyValueContext

        def tablePropertyKey(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyKeyContext,0)


        def tablePropertyValue(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyValueContext,0)


        def EQ(self):
            return self.getToken(SparkSqlBaseParser.EQ, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_tableProperty

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableProperty" ):
                listener.enterTableProperty(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableProperty" ):
                listener.exitTableProperty(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableProperty" ):
                return visitor.visitTableProperty(self)
            else:
                return visitor.visitChildren(self)




    def tableProperty(self):

        localctx = SparkSqlBaseParser.TablePropertyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 46, self.RULE_tableProperty)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1172
            localctx.key = self.tablePropertyKey()
            self.state = 1177
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.TRUE or _la==SparkSqlBaseParser.FALSE or _la==SparkSqlBaseParser.EQ or ((((_la - 240)) & ~0x3f) == 0 and ((1 << (_la - 240)) & ((1 << (SparkSqlBaseParser.STRING - 240)) | (1 << (SparkSqlBaseParser.INTEGER_VALUE - 240)) | (1 << (SparkSqlBaseParser.DECIMAL_VALUE - 240)))) != 0):
                self.state = 1174
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.EQ:
                    self.state = 1173
                    self.match(SparkSqlBaseParser.EQ)


                self.state = 1176
                localctx.value = self.tablePropertyValue()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class TablePropertyKeyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_tablePropertyKey

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTablePropertyKey" ):
                listener.enterTablePropertyKey(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTablePropertyKey" ):
                listener.exitTablePropertyKey(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTablePropertyKey" ):
                return visitor.visitTablePropertyKey(self)
            else:
                return visitor.visitChildren(self)




    def tablePropertyKey(self):

        localctx = SparkSqlBaseParser.TablePropertyKeyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 48, self.RULE_tablePropertyKey)
        self._la = 0 # Token type
        try:
            self.state = 1188
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.ADD, SparkSqlBaseParser.AS, SparkSqlBaseParser.ALL, SparkSqlBaseParser.ANY, SparkSqlBaseParser.DISTINCT, SparkSqlBaseParser.WHERE, SparkSqlBaseParser.GROUP, SparkSqlBaseParser.BY, SparkSqlBaseParser.GROUPING, SparkSqlBaseParser.SETS, SparkSqlBaseParser.CUBE, SparkSqlBaseParser.ROLLUP, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.HAVING, SparkSqlBaseParser.LIMIT, SparkSqlBaseParser.AT, SparkSqlBaseParser.OR, SparkSqlBaseParser.AND, SparkSqlBaseParser.IN, SparkSqlBaseParser.NOT, SparkSqlBaseParser.NO, SparkSqlBaseParser.EXISTS, SparkSqlBaseParser.BETWEEN, SparkSqlBaseParser.LIKE, SparkSqlBaseParser.RLIKE, SparkSqlBaseParser.IS, SparkSqlBaseParser.NULL, SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE, SparkSqlBaseParser.NULLS, SparkSqlBaseParser.ASC, SparkSqlBaseParser.DESC, SparkSqlBaseParser.FOR, SparkSqlBaseParser.INTERVAL, SparkSqlBaseParser.CASE, SparkSqlBaseParser.WHEN, SparkSqlBaseParser.THEN, SparkSqlBaseParser.ELSE, SparkSqlBaseParser.END, SparkSqlBaseParser.JOIN, SparkSqlBaseParser.CROSS, SparkSqlBaseParser.OUTER, SparkSqlBaseParser.INNER, SparkSqlBaseParser.LEFT, SparkSqlBaseParser.SEMI, SparkSqlBaseParser.RIGHT, SparkSqlBaseParser.FULL, SparkSqlBaseParser.NATURAL, SparkSqlBaseParser.ON, SparkSqlBaseParser.PIVOT, SparkSqlBaseParser.LATERAL, SparkSqlBaseParser.WINDOW, SparkSqlBaseParser.OVER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.UNBOUNDED, SparkSqlBaseParser.PRECEDING, SparkSqlBaseParser.FOLLOWING, SparkSqlBaseParser.CURRENT, SparkSqlBaseParser.FIRST, SparkSqlBaseParser.AFTER, SparkSqlBaseParser.LAST, SparkSqlBaseParser.ROW, SparkSqlBaseParser.WITH, SparkSqlBaseParser.VALUES, SparkSqlBaseParser.CREATE, SparkSqlBaseParser.TABLE, SparkSqlBaseParser.DIRECTORY, SparkSqlBaseParser.VIEW, SparkSqlBaseParser.REPLACE, SparkSqlBaseParser.INSERT, SparkSqlBaseParser.DELETE, SparkSqlBaseParser.INTO, SparkSqlBaseParser.DESCRIBE, SparkSqlBaseParser.EXPLAIN, SparkSqlBaseParser.FORMAT, SparkSqlBaseParser.LOGICAL, SparkSqlBaseParser.CODEGEN, SparkSqlBaseParser.COST, SparkSqlBaseParser.CAST, SparkSqlBaseParser.SHOW, SparkSqlBaseParser.TABLES, SparkSqlBaseParser.COLUMNS, SparkSqlBaseParser.COLUMN, SparkSqlBaseParser.USE, SparkSqlBaseParser.PARTITIONS, SparkSqlBaseParser.FUNCTIONS, SparkSqlBaseParser.DROP, SparkSqlBaseParser.UNION, SparkSqlBaseParser.EXCEPT, SparkSqlBaseParser.SETMINUS, SparkSqlBaseParser.INTERSECT, SparkSqlBaseParser.TO, SparkSqlBaseParser.TABLESAMPLE, SparkSqlBaseParser.STRATIFY, SparkSqlBaseParser.ALTER, SparkSqlBaseParser.RENAME, SparkSqlBaseParser.ARRAY, SparkSqlBaseParser.MAP, SparkSqlBaseParser.STRUCT, SparkSqlBaseParser.COMMENT, SparkSqlBaseParser.SET, SparkSqlBaseParser.RESET, SparkSqlBaseParser.DATA, SparkSqlBaseParser.START, SparkSqlBaseParser.TRANSACTION, SparkSqlBaseParser.COMMIT, SparkSqlBaseParser.ROLLBACK, SparkSqlBaseParser.MACRO, SparkSqlBaseParser.IGNORE, SparkSqlBaseParser.BOTH, SparkSqlBaseParser.LEADING, SparkSqlBaseParser.TRAILING, SparkSqlBaseParser.IF, SparkSqlBaseParser.POSITION, SparkSqlBaseParser.EXTRACT, SparkSqlBaseParser.DIV, SparkSqlBaseParser.PERCENTLIT, SparkSqlBaseParser.BUCKET, SparkSqlBaseParser.OUT, SparkSqlBaseParser.OF, SparkSqlBaseParser.SORT, SparkSqlBaseParser.CLUSTER, SparkSqlBaseParser.DISTRIBUTE, SparkSqlBaseParser.OVERWRITE, SparkSqlBaseParser.TRANSFORM, SparkSqlBaseParser.REDUCE, SparkSqlBaseParser.SERDE, SparkSqlBaseParser.SERDEPROPERTIES, SparkSqlBaseParser.RECORDREADER, SparkSqlBaseParser.RECORDWRITER, SparkSqlBaseParser.DELIMITED, SparkSqlBaseParser.FIELDS, SparkSqlBaseParser.TERMINATED, SparkSqlBaseParser.COLLECTION, SparkSqlBaseParser.ITEMS, SparkSqlBaseParser.KEYS, SparkSqlBaseParser.ESCAPED, SparkSqlBaseParser.LINES, SparkSqlBaseParser.SEPARATED, SparkSqlBaseParser.FUNCTION, SparkSqlBaseParser.EXTENDED, SparkSqlBaseParser.REFRESH, SparkSqlBaseParser.CLEAR, SparkSqlBaseParser.CACHE, SparkSqlBaseParser.UNCACHE, SparkSqlBaseParser.LAZY, SparkSqlBaseParser.FORMATTED, SparkSqlBaseParser.GLOBAL, SparkSqlBaseParser.TEMPORARY, SparkSqlBaseParser.OPTIONS, SparkSqlBaseParser.UNSET, SparkSqlBaseParser.TBLPROPERTIES, SparkSqlBaseParser.DBPROPERTIES, SparkSqlBaseParser.BUCKETS, SparkSqlBaseParser.SKEWED, SparkSqlBaseParser.STORED, SparkSqlBaseParser.DIRECTORIES, SparkSqlBaseParser.LOCATION, SparkSqlBaseParser.EXCHANGE, SparkSqlBaseParser.ARCHIVE, SparkSqlBaseParser.UNARCHIVE, SparkSqlBaseParser.FILEFORMAT, SparkSqlBaseParser.TOUCH, SparkSqlBaseParser.COMPACT, SparkSqlBaseParser.CONCATENATE, SparkSqlBaseParser.CHANGE, SparkSqlBaseParser.CASCADE, SparkSqlBaseParser.RESTRICT, SparkSqlBaseParser.CLUSTERED, SparkSqlBaseParser.SORTED, SparkSqlBaseParser.PURGE, SparkSqlBaseParser.INPUTFORMAT, SparkSqlBaseParser.OUTPUTFORMAT, SparkSqlBaseParser.DATABASE, SparkSqlBaseParser.DATABASES, SparkSqlBaseParser.DFS, SparkSqlBaseParser.TRUNCATE, SparkSqlBaseParser.ANALYZE, SparkSqlBaseParser.COMPUTE, SparkSqlBaseParser.LIST, SparkSqlBaseParser.STATISTICS, SparkSqlBaseParser.PARTITIONED, SparkSqlBaseParser.EXTERNAL, SparkSqlBaseParser.DEFINED, SparkSqlBaseParser.REVOKE, SparkSqlBaseParser.GRANT, SparkSqlBaseParser.LOCK, SparkSqlBaseParser.UNLOCK, SparkSqlBaseParser.MSCK, SparkSqlBaseParser.REPAIR, SparkSqlBaseParser.RECOVER, SparkSqlBaseParser.EXPORT, SparkSqlBaseParser.IMPORT, SparkSqlBaseParser.LOAD, SparkSqlBaseParser.ROLE, SparkSqlBaseParser.ROLES, SparkSqlBaseParser.COMPACTIONS, SparkSqlBaseParser.PRINCIPALS, SparkSqlBaseParser.TRANSACTIONS, SparkSqlBaseParser.INDEX, SparkSqlBaseParser.INDEXES, SparkSqlBaseParser.LOCKS, SparkSqlBaseParser.OPTION, SparkSqlBaseParser.ANTI, SparkSqlBaseParser.LOCAL, SparkSqlBaseParser.INPATH, SparkSqlBaseParser.IDENTIFIER, SparkSqlBaseParser.BACKQUOTED_IDENTIFIER]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1179
                self.identifier()
                self.state = 1184
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__3:
                    self.state = 1180
                    self.match(SparkSqlBaseParser.T__3)
                    self.state = 1181
                    self.identifier()
                    self.state = 1186
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                pass
            elif token in [SparkSqlBaseParser.STRING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1187
                self.match(SparkSqlBaseParser.STRING)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class TablePropertyValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INTEGER_VALUE(self):
            return self.getToken(SparkSqlBaseParser.INTEGER_VALUE, 0)

        def DECIMAL_VALUE(self):
            return self.getToken(SparkSqlBaseParser.DECIMAL_VALUE, 0)

        def booleanValue(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.BooleanValueContext,0)


        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_tablePropertyValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTablePropertyValue" ):
                listener.enterTablePropertyValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTablePropertyValue" ):
                listener.exitTablePropertyValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTablePropertyValue" ):
                return visitor.visitTablePropertyValue(self)
            else:
                return visitor.visitChildren(self)




    def tablePropertyValue(self):

        localctx = SparkSqlBaseParser.TablePropertyValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 50, self.RULE_tablePropertyValue)
        try:
            self.state = 1194
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.INTEGER_VALUE]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1190
                self.match(SparkSqlBaseParser.INTEGER_VALUE)
                pass
            elif token in [SparkSqlBaseParser.DECIMAL_VALUE]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1191
                self.match(SparkSqlBaseParser.DECIMAL_VALUE)
                pass
            elif token in [SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE]:
                self.enterOuterAlt(localctx, 3)
                self.state = 1192
                self.booleanValue()
                pass
            elif token in [SparkSqlBaseParser.STRING]:
                self.enterOuterAlt(localctx, 4)
                self.state = 1193
                self.match(SparkSqlBaseParser.STRING)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ConstantListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def constant(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ConstantContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ConstantContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_constantList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterConstantList" ):
                listener.enterConstantList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitConstantList" ):
                listener.exitConstantList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitConstantList" ):
                return visitor.visitConstantList(self)
            else:
                return visitor.visitChildren(self)




    def constantList(self):

        localctx = SparkSqlBaseParser.ConstantListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 52, self.RULE_constantList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1196
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1197
            self.constant()
            self.state = 1202
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1198
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1199
                self.constant()
                self.state = 1204
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1205
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NestedConstantListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def constantList(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ConstantListContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ConstantListContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_nestedConstantList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNestedConstantList" ):
                listener.enterNestedConstantList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNestedConstantList" ):
                listener.exitNestedConstantList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNestedConstantList" ):
                return visitor.visitNestedConstantList(self)
            else:
                return visitor.visitChildren(self)




    def nestedConstantList(self):

        localctx = SparkSqlBaseParser.NestedConstantListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 54, self.RULE_nestedConstantList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1207
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1208
            self.constantList()
            self.state = 1213
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1209
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1210
                self.constantList()
                self.state = 1215
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1216
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class CreateFileFormatContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def STORED(self):
            return self.getToken(SparkSqlBaseParser.STORED, 0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def fileFormat(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.FileFormatContext,0)


        def BY(self):
            return self.getToken(SparkSqlBaseParser.BY, 0)

        def storageHandler(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.StorageHandlerContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_createFileFormat

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCreateFileFormat" ):
                listener.enterCreateFileFormat(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCreateFileFormat" ):
                listener.exitCreateFileFormat(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCreateFileFormat" ):
                return visitor.visitCreateFileFormat(self)
            else:
                return visitor.visitChildren(self)




    def createFileFormat(self):

        localctx = SparkSqlBaseParser.CreateFileFormatContext(self, self._ctx, self.state)
        self.enterRule(localctx, 56, self.RULE_createFileFormat)
        try:
            self.state = 1224
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,138,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1218
                self.match(SparkSqlBaseParser.STORED)
                self.state = 1219
                self.match(SparkSqlBaseParser.AS)
                self.state = 1220
                self.fileFormat()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1221
                self.match(SparkSqlBaseParser.STORED)
                self.state = 1222
                self.match(SparkSqlBaseParser.BY)
                self.state = 1223
                self.storageHandler()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class FileFormatContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_fileFormat

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class TableFileFormatContext(FileFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.FileFormatContext
            super().__init__(parser)
            self.inFmt = None # Token
            self.outFmt = None # Token
            self.copyFrom(ctx)

        def INPUTFORMAT(self):
            return self.getToken(SparkSqlBaseParser.INPUTFORMAT, 0)
        def OUTPUTFORMAT(self):
            return self.getToken(SparkSqlBaseParser.OUTPUTFORMAT, 0)
        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.STRING)
            else:
                return self.getToken(SparkSqlBaseParser.STRING, i)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableFileFormat" ):
                listener.enterTableFileFormat(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableFileFormat" ):
                listener.exitTableFileFormat(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableFileFormat" ):
                return visitor.visitTableFileFormat(self)
            else:
                return visitor.visitChildren(self)


    class GenericFileFormatContext(FileFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.FileFormatContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterGenericFileFormat" ):
                listener.enterGenericFileFormat(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitGenericFileFormat" ):
                listener.exitGenericFileFormat(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitGenericFileFormat" ):
                return visitor.visitGenericFileFormat(self)
            else:
                return visitor.visitChildren(self)



    def fileFormat(self):

        localctx = SparkSqlBaseParser.FileFormatContext(self, self._ctx, self.state)
        self.enterRule(localctx, 58, self.RULE_fileFormat)
        try:
            self.state = 1231
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,139,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.TableFileFormatContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1226
                self.match(SparkSqlBaseParser.INPUTFORMAT)
                self.state = 1227
                localctx.inFmt = self.match(SparkSqlBaseParser.STRING)
                self.state = 1228
                self.match(SparkSqlBaseParser.OUTPUTFORMAT)
                self.state = 1229
                localctx.outFmt = self.match(SparkSqlBaseParser.STRING)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.GenericFileFormatContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1230
                self.identifier()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class StorageHandlerContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def WITH(self):
            return self.getToken(SparkSqlBaseParser.WITH, 0)

        def SERDEPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.SERDEPROPERTIES, 0)

        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_storageHandler

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStorageHandler" ):
                listener.enterStorageHandler(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStorageHandler" ):
                listener.exitStorageHandler(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStorageHandler" ):
                return visitor.visitStorageHandler(self)
            else:
                return visitor.visitChildren(self)




    def storageHandler(self):

        localctx = SparkSqlBaseParser.StorageHandlerContext(self, self._ctx, self.state)
        self.enterRule(localctx, 60, self.RULE_storageHandler)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1233
            self.match(SparkSqlBaseParser.STRING)
            self.state = 1237
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,140,self._ctx)
            if la_ == 1:
                self.state = 1234
                self.match(SparkSqlBaseParser.WITH)
                self.state = 1235
                self.match(SparkSqlBaseParser.SERDEPROPERTIES)
                self.state = 1236
                self.tablePropertyList()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ResourceContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_resource

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterResource" ):
                listener.enterResource(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitResource" ):
                listener.exitResource(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitResource" ):
                return visitor.visitResource(self)
            else:
                return visitor.visitChildren(self)




    def resource(self):

        localctx = SparkSqlBaseParser.ResourceContext(self, self._ctx, self.state)
        self.enterRule(localctx, 62, self.RULE_resource)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1239
            self.identifier()
            self.state = 1240
            self.match(SparkSqlBaseParser.STRING)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class QueryNoWithContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_queryNoWith

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class SingleInsertQueryContext(QueryNoWithContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryNoWithContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def queryTerm(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryTermContext,0)

        def queryOrganization(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryOrganizationContext,0)

        def insertInto(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.InsertIntoContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSingleInsertQuery" ):
                listener.enterSingleInsertQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSingleInsertQuery" ):
                listener.exitSingleInsertQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSingleInsertQuery" ):
                return visitor.visitSingleInsertQuery(self)
            else:
                return visitor.visitChildren(self)


    class MultiInsertQueryContext(QueryNoWithContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryNoWithContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def fromClause(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.FromClauseContext,0)

        def multiInsertQueryBody(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.MultiInsertQueryBodyContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.MultiInsertQueryBodyContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMultiInsertQuery" ):
                listener.enterMultiInsertQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMultiInsertQuery" ):
                listener.exitMultiInsertQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMultiInsertQuery" ):
                return visitor.visitMultiInsertQuery(self)
            else:
                return visitor.visitChildren(self)



    def queryNoWith(self):

        localctx = SparkSqlBaseParser.QueryNoWithContext(self, self._ctx, self.state)
        self.enterRule(localctx, 64, self.RULE_queryNoWith)
        self._la = 0 # Token type
        try:
            self.state = 1254
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,143,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.SingleInsertQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1243
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.INSERT:
                    self.state = 1242
                    self.insertInto()


                self.state = 1245
                self.queryTerm(0)
                self.state = 1246
                self.queryOrganization()
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.MultiInsertQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1248
                self.fromClause()
                self.state = 1250 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 1249
                    self.multiInsertQueryBody()
                    self.state = 1252 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSqlBaseParser.SELECT or _la==SparkSqlBaseParser.FROM or _la==SparkSqlBaseParser.INSERT or _la==SparkSqlBaseParser.MAP or _la==SparkSqlBaseParser.REDUCE):
                        break

                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class QueryOrganizationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._sortItem = None # SortItemContext
            self.order = list() # of SortItemContexts
            self._expression = None # ExpressionContext
            self.clusterBy = list() # of ExpressionContexts
            self.distributeBy = list() # of ExpressionContexts
            self.sort = list() # of SortItemContexts
            self.limit = None # ExpressionContext

        def ORDER(self):
            return self.getToken(SparkSqlBaseParser.ORDER, 0)

        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.BY)
            else:
                return self.getToken(SparkSqlBaseParser.BY, i)

        def CLUSTER(self):
            return self.getToken(SparkSqlBaseParser.CLUSTER, 0)

        def DISTRIBUTE(self):
            return self.getToken(SparkSqlBaseParser.DISTRIBUTE, 0)

        def SORT(self):
            return self.getToken(SparkSqlBaseParser.SORT, 0)

        def windows(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.WindowsContext,0)


        def LIMIT(self):
            return self.getToken(SparkSqlBaseParser.LIMIT, 0)

        def sortItem(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.SortItemContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.SortItemContext,i)


        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def ALL(self):
            return self.getToken(SparkSqlBaseParser.ALL, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_queryOrganization

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQueryOrganization" ):
                listener.enterQueryOrganization(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQueryOrganization" ):
                listener.exitQueryOrganization(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQueryOrganization" ):
                return visitor.visitQueryOrganization(self)
            else:
                return visitor.visitChildren(self)




    def queryOrganization(self):

        localctx = SparkSqlBaseParser.QueryOrganizationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 66, self.RULE_queryOrganization)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1266
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.ORDER:
                self.state = 1256
                self.match(SparkSqlBaseParser.ORDER)
                self.state = 1257
                self.match(SparkSqlBaseParser.BY)
                self.state = 1258
                localctx._sortItem = self.sortItem()
                localctx.order.append(localctx._sortItem)
                self.state = 1263
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1259
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1260
                    localctx._sortItem = self.sortItem()
                    localctx.order.append(localctx._sortItem)
                    self.state = 1265
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 1278
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.CLUSTER:
                self.state = 1268
                self.match(SparkSqlBaseParser.CLUSTER)
                self.state = 1269
                self.match(SparkSqlBaseParser.BY)
                self.state = 1270
                localctx._expression = self.expression()
                localctx.clusterBy.append(localctx._expression)
                self.state = 1275
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1271
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1272
                    localctx._expression = self.expression()
                    localctx.clusterBy.append(localctx._expression)
                    self.state = 1277
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 1290
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.DISTRIBUTE:
                self.state = 1280
                self.match(SparkSqlBaseParser.DISTRIBUTE)
                self.state = 1281
                self.match(SparkSqlBaseParser.BY)
                self.state = 1282
                localctx._expression = self.expression()
                localctx.distributeBy.append(localctx._expression)
                self.state = 1287
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1283
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1284
                    localctx._expression = self.expression()
                    localctx.distributeBy.append(localctx._expression)
                    self.state = 1289
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 1302
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.SORT:
                self.state = 1292
                self.match(SparkSqlBaseParser.SORT)
                self.state = 1293
                self.match(SparkSqlBaseParser.BY)
                self.state = 1294
                localctx._sortItem = self.sortItem()
                localctx.sort.append(localctx._sortItem)
                self.state = 1299
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1295
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1296
                    localctx._sortItem = self.sortItem()
                    localctx.sort.append(localctx._sortItem)
                    self.state = 1301
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 1305
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.WINDOW:
                self.state = 1304
                self.windows()


            self.state = 1312
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.LIMIT:
                self.state = 1307
                self.match(SparkSqlBaseParser.LIMIT)
                self.state = 1310
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,153,self._ctx)
                if la_ == 1:
                    self.state = 1308
                    self.match(SparkSqlBaseParser.ALL)
                    pass

                elif la_ == 2:
                    self.state = 1309
                    localctx.limit = self.expression()
                    pass




        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class MultiInsertQueryBodyContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def querySpecification(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QuerySpecificationContext,0)


        def queryOrganization(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryOrganizationContext,0)


        def insertInto(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.InsertIntoContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_multiInsertQueryBody

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterMultiInsertQueryBody" ):
                listener.enterMultiInsertQueryBody(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitMultiInsertQueryBody" ):
                listener.exitMultiInsertQueryBody(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitMultiInsertQueryBody" ):
                return visitor.visitMultiInsertQueryBody(self)
            else:
                return visitor.visitChildren(self)




    def multiInsertQueryBody(self):

        localctx = SparkSqlBaseParser.MultiInsertQueryBodyContext(self, self._ctx, self.state)
        self.enterRule(localctx, 68, self.RULE_multiInsertQueryBody)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1315
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.INSERT:
                self.state = 1314
                self.insertInto()


            self.state = 1317
            self.querySpecification()
            self.state = 1318
            self.queryOrganization()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class QueryTermContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_queryTerm

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class QueryTermDefaultContext(QueryTermContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryTermContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def queryPrimary(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryPrimaryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQueryTermDefault" ):
                listener.enterQueryTermDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQueryTermDefault" ):
                listener.exitQueryTermDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQueryTermDefault" ):
                return visitor.visitQueryTermDefault(self)
            else:
                return visitor.visitChildren(self)


    class SetOperationContext(QueryTermContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryTermContext
            super().__init__(parser)
            self.left = None # QueryTermContext
            self.operator = None # Token
            self.right = None # QueryTermContext
            self.copyFrom(ctx)

        def queryTerm(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.QueryTermContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.QueryTermContext,i)

        def INTERSECT(self):
            return self.getToken(SparkSqlBaseParser.INTERSECT, 0)
        def UNION(self):
            return self.getToken(SparkSqlBaseParser.UNION, 0)
        def EXCEPT(self):
            return self.getToken(SparkSqlBaseParser.EXCEPT, 0)
        def SETMINUS(self):
            return self.getToken(SparkSqlBaseParser.SETMINUS, 0)
        def setQuantifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.SetQuantifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetOperation" ):
                listener.enterSetOperation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetOperation" ):
                listener.exitSetOperation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetOperation" ):
                return visitor.visitSetOperation(self)
            else:
                return visitor.visitChildren(self)



    def queryTerm(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSqlBaseParser.QueryTermContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 70
        self.enterRecursionRule(localctx, 70, self.RULE_queryTerm, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            localctx = SparkSqlBaseParser.QueryTermDefaultContext(self, localctx)
            self._ctx = localctx
            _prevctx = localctx

            self.state = 1321
            self.queryPrimary()
            self._ctx.stop = self._input.LT(-1)
            self.state = 1346
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,160,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 1344
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,159,self._ctx)
                    if la_ == 1:
                        localctx = SparkSqlBaseParser.SetOperationContext(self, SparkSqlBaseParser.QueryTermContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_queryTerm)
                        self.state = 1323
                        if not self.precpred(self._ctx, 3):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 3)")
                        self.state = 1324
                        if not False:
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "False")
                        self.state = 1325
                        localctx.operator = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 102)) & ~0x3f) == 0 and ((1 << (_la - 102)) & ((1 << (SparkSqlBaseParser.UNION - 102)) | (1 << (SparkSqlBaseParser.EXCEPT - 102)) | (1 << (SparkSqlBaseParser.SETMINUS - 102)) | (1 << (SparkSqlBaseParser.INTERSECT - 102)))) != 0)):
                            localctx.operator = self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 1327
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        if _la==SparkSqlBaseParser.ALL or _la==SparkSqlBaseParser.DISTINCT:
                            self.state = 1326
                            self.setQuantifier()


                        self.state = 1329
                        localctx.right = self.queryTerm(4)
                        pass

                    elif la_ == 2:
                        localctx = SparkSqlBaseParser.SetOperationContext(self, SparkSqlBaseParser.QueryTermContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_queryTerm)
                        self.state = 1330
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 1331
                        if False:
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "!False")
                        self.state = 1332
                        localctx.operator = self.match(SparkSqlBaseParser.INTERSECT)
                        self.state = 1334
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        if _la==SparkSqlBaseParser.ALL or _la==SparkSqlBaseParser.DISTINCT:
                            self.state = 1333
                            self.setQuantifier()


                        self.state = 1336
                        localctx.right = self.queryTerm(3)
                        pass

                    elif la_ == 3:
                        localctx = SparkSqlBaseParser.SetOperationContext(self, SparkSqlBaseParser.QueryTermContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_queryTerm)
                        self.state = 1337
                        if not self.precpred(self._ctx, 1):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
                        self.state = 1338
                        if False:
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "!False")
                        self.state = 1339
                        localctx.operator = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 102)) & ~0x3f) == 0 and ((1 << (_la - 102)) & ((1 << (SparkSqlBaseParser.UNION - 102)) | (1 << (SparkSqlBaseParser.EXCEPT - 102)) | (1 << (SparkSqlBaseParser.SETMINUS - 102)))) != 0)):
                            localctx.operator = self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 1341
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        if _la==SparkSqlBaseParser.ALL or _la==SparkSqlBaseParser.DISTINCT:
                            self.state = 1340
                            self.setQuantifier()


                        self.state = 1343
                        localctx.right = self.queryTerm(2)
                        pass

             
                self.state = 1348
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,160,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx

    class QueryPrimaryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_queryPrimary

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class SubqueryContext(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def queryNoWith(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryNoWithContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubquery" ):
                listener.enterSubquery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubquery" ):
                listener.exitSubquery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubquery" ):
                return visitor.visitSubquery(self)
            else:
                return visitor.visitChildren(self)


    class QueryPrimaryDefaultContext(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def querySpecification(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QuerySpecificationContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQueryPrimaryDefault" ):
                listener.enterQueryPrimaryDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQueryPrimaryDefault" ):
                listener.exitQueryPrimaryDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQueryPrimaryDefault" ):
                return visitor.visitQueryPrimaryDefault(self)
            else:
                return visitor.visitChildren(self)


    class InlineTableDefault1Context(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def inlineTable(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.InlineTableContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInlineTableDefault1" ):
                listener.enterInlineTableDefault1(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInlineTableDefault1" ):
                listener.exitInlineTableDefault1(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInlineTableDefault1" ):
                return visitor.visitInlineTableDefault1(self)
            else:
                return visitor.visitChildren(self)


    class TableContext(QueryPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.QueryPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)
        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable" ):
                listener.enterTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable" ):
                listener.exitTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable" ):
                return visitor.visitTable(self)
            else:
                return visitor.visitChildren(self)



    def queryPrimary(self):

        localctx = SparkSqlBaseParser.QueryPrimaryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 72, self.RULE_queryPrimary)
        try:
            self.state = 1357
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.MAP, SparkSqlBaseParser.REDUCE]:
                localctx = SparkSqlBaseParser.QueryPrimaryDefaultContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1349
                self.querySpecification()
                pass
            elif token in [SparkSqlBaseParser.TABLE]:
                localctx = SparkSqlBaseParser.TableContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1350
                self.match(SparkSqlBaseParser.TABLE)
                self.state = 1351
                self.tableIdentifier()
                pass
            elif token in [SparkSqlBaseParser.VALUES]:
                localctx = SparkSqlBaseParser.InlineTableDefault1Context(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 1352
                self.inlineTable()
                pass
            elif token in [SparkSqlBaseParser.T__0]:
                localctx = SparkSqlBaseParser.SubqueryContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 1353
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1354
                self.queryNoWith()
                self.state = 1355
                self.match(SparkSqlBaseParser.T__1)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SortItemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.ordering = None # Token
            self.nullOrder = None # Token

        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def NULLS(self):
            return self.getToken(SparkSqlBaseParser.NULLS, 0)

        def ASC(self):
            return self.getToken(SparkSqlBaseParser.ASC, 0)

        def DESC(self):
            return self.getToken(SparkSqlBaseParser.DESC, 0)

        def LAST(self):
            return self.getToken(SparkSqlBaseParser.LAST, 0)

        def FIRST(self):
            return self.getToken(SparkSqlBaseParser.FIRST, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_sortItem

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSortItem" ):
                listener.enterSortItem(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSortItem" ):
                listener.exitSortItem(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSortItem" ):
                return visitor.visitSortItem(self)
            else:
                return visitor.visitChildren(self)




    def sortItem(self):

        localctx = SparkSqlBaseParser.SortItemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 74, self.RULE_sortItem)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1359
            self.expression()
            self.state = 1361
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.ASC or _la==SparkSqlBaseParser.DESC:
                self.state = 1360
                localctx.ordering = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.ASC or _la==SparkSqlBaseParser.DESC):
                    localctx.ordering = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()


            self.state = 1365
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.NULLS:
                self.state = 1363
                self.match(SparkSqlBaseParser.NULLS)
                self.state = 1364
                localctx.nullOrder = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.FIRST or _la==SparkSqlBaseParser.LAST):
                    localctx.nullOrder = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class QuerySpecificationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.kind = None # Token
            self.inRowFormat = None # RowFormatContext
            self.recordWriter = None # Token
            self.script = None # Token
            self.outRowFormat = None # RowFormatContext
            self.recordReader = None # Token
            self.where = None # BooleanExpressionContext
            self._hint = None # HintContext
            self.hints = list() # of HintContexts
            self.having = None # BooleanExpressionContext

        def USING(self):
            return self.getToken(SparkSqlBaseParser.USING, 0)

        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.STRING)
            else:
                return self.getToken(SparkSqlBaseParser.STRING, i)

        def RECORDWRITER(self):
            return self.getToken(SparkSqlBaseParser.RECORDWRITER, 0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def RECORDREADER(self):
            return self.getToken(SparkSqlBaseParser.RECORDREADER, 0)

        def fromClause(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.FromClauseContext,0)


        def WHERE(self):
            return self.getToken(SparkSqlBaseParser.WHERE, 0)

        def SELECT(self):
            return self.getToken(SparkSqlBaseParser.SELECT, 0)

        def namedExpressionSeq(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.NamedExpressionSeqContext,0)


        def rowFormat(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.RowFormatContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.RowFormatContext,i)


        def booleanExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.BooleanExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.BooleanExpressionContext,i)


        def TRANSFORM(self):
            return self.getToken(SparkSqlBaseParser.TRANSFORM, 0)

        def MAP(self):
            return self.getToken(SparkSqlBaseParser.MAP, 0)

        def REDUCE(self):
            return self.getToken(SparkSqlBaseParser.REDUCE, 0)

        def identifierSeq(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierSeqContext,0)


        def colTypeList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeListContext,0)


        def lateralView(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.LateralViewContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.LateralViewContext,i)


        def aggregation(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.AggregationContext,0)


        def HAVING(self):
            return self.getToken(SparkSqlBaseParser.HAVING, 0)

        def windows(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.WindowsContext,0)


        def setQuantifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.SetQuantifierContext,0)


        def hint(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.HintContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.HintContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_querySpecification

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuerySpecification" ):
                listener.enterQuerySpecification(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuerySpecification" ):
                listener.exitQuerySpecification(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuerySpecification" ):
                return visitor.visitQuerySpecification(self)
            else:
                return visitor.visitChildren(self)




    def querySpecification(self):

        localctx = SparkSqlBaseParser.QuerySpecificationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 76, self.RULE_querySpecification)
        self._la = 0 # Token type
        try:
            self.state = 1460
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,185,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1377
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSqlBaseParser.SELECT]:
                    self.state = 1367
                    self.match(SparkSqlBaseParser.SELECT)
                    self.state = 1368
                    localctx.kind = self.match(SparkSqlBaseParser.TRANSFORM)
                    self.state = 1369
                    self.match(SparkSqlBaseParser.T__0)
                    self.state = 1370
                    self.namedExpressionSeq()
                    self.state = 1371
                    self.match(SparkSqlBaseParser.T__1)
                    pass
                elif token in [SparkSqlBaseParser.MAP]:
                    self.state = 1373
                    localctx.kind = self.match(SparkSqlBaseParser.MAP)
                    self.state = 1374
                    self.namedExpressionSeq()
                    pass
                elif token in [SparkSqlBaseParser.REDUCE]:
                    self.state = 1375
                    localctx.kind = self.match(SparkSqlBaseParser.REDUCE)
                    self.state = 1376
                    self.namedExpressionSeq()
                    pass
                else:
                    raise NoViableAltException(self)

                self.state = 1380
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.ROW:
                    self.state = 1379
                    localctx.inRowFormat = self.rowFormat()


                self.state = 1384
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.RECORDWRITER:
                    self.state = 1382
                    self.match(SparkSqlBaseParser.RECORDWRITER)
                    self.state = 1383
                    localctx.recordWriter = self.match(SparkSqlBaseParser.STRING)


                self.state = 1386
                self.match(SparkSqlBaseParser.USING)
                self.state = 1387
                localctx.script = self.match(SparkSqlBaseParser.STRING)
                self.state = 1400
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,169,self._ctx)
                if la_ == 1:
                    self.state = 1388
                    self.match(SparkSqlBaseParser.AS)
                    self.state = 1398
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,168,self._ctx)
                    if la_ == 1:
                        self.state = 1389
                        self.identifierSeq()
                        pass

                    elif la_ == 2:
                        self.state = 1390
                        self.colTypeList()
                        pass

                    elif la_ == 3:
                        self.state = 1391
                        self.match(SparkSqlBaseParser.T__0)
                        self.state = 1394
                        self._errHandler.sync(self)
                        la_ = self._interp.adaptivePredict(self._input,167,self._ctx)
                        if la_ == 1:
                            self.state = 1392
                            self.identifierSeq()
                            pass

                        elif la_ == 2:
                            self.state = 1393
                            self.colTypeList()
                            pass


                        self.state = 1396
                        self.match(SparkSqlBaseParser.T__1)
                        pass




                self.state = 1403
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,170,self._ctx)
                if la_ == 1:
                    self.state = 1402
                    localctx.outRowFormat = self.rowFormat()


                self.state = 1407
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,171,self._ctx)
                if la_ == 1:
                    self.state = 1405
                    self.match(SparkSqlBaseParser.RECORDREADER)
                    self.state = 1406
                    localctx.recordReader = self.match(SparkSqlBaseParser.STRING)


                self.state = 1410
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,172,self._ctx)
                if la_ == 1:
                    self.state = 1409
                    self.fromClause()


                self.state = 1414
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,173,self._ctx)
                if la_ == 1:
                    self.state = 1412
                    self.match(SparkSqlBaseParser.WHERE)
                    self.state = 1413
                    localctx.where = self.booleanExpression(0)


                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1438
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSqlBaseParser.SELECT]:
                    self.state = 1416
                    localctx.kind = self.match(SparkSqlBaseParser.SELECT)
                    self.state = 1420
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__4:
                        self.state = 1417
                        localctx._hint = self.hint()
                        localctx.hints.append(localctx._hint)
                        self.state = 1422
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    self.state = 1424
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,175,self._ctx)
                    if la_ == 1:
                        self.state = 1423
                        self.setQuantifier()


                    self.state = 1426
                    self.namedExpressionSeq()
                    self.state = 1428
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,176,self._ctx)
                    if la_ == 1:
                        self.state = 1427
                        self.fromClause()


                    pass
                elif token in [SparkSqlBaseParser.FROM]:
                    self.state = 1430
                    self.fromClause()
                    self.state = 1436
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,178,self._ctx)
                    if la_ == 1:
                        self.state = 1431
                        localctx.kind = self.match(SparkSqlBaseParser.SELECT)
                        self.state = 1433
                        self._errHandler.sync(self)
                        la_ = self._interp.adaptivePredict(self._input,177,self._ctx)
                        if la_ == 1:
                            self.state = 1432
                            self.setQuantifier()


                        self.state = 1435
                        self.namedExpressionSeq()


                    pass
                else:
                    raise NoViableAltException(self)

                self.state = 1443
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,180,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1440
                        self.lateralView() 
                    self.state = 1445
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,180,self._ctx)

                self.state = 1448
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,181,self._ctx)
                if la_ == 1:
                    self.state = 1446
                    self.match(SparkSqlBaseParser.WHERE)
                    self.state = 1447
                    localctx.where = self.booleanExpression(0)


                self.state = 1451
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,182,self._ctx)
                if la_ == 1:
                    self.state = 1450
                    self.aggregation()


                self.state = 1455
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,183,self._ctx)
                if la_ == 1:
                    self.state = 1453
                    self.match(SparkSqlBaseParser.HAVING)
                    self.state = 1454
                    localctx.having = self.booleanExpression(0)


                self.state = 1458
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,184,self._ctx)
                if la_ == 1:
                    self.state = 1457
                    self.windows()


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class HintContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._hintStatement = None # HintStatementContext
            self.hintStatements = list() # of HintStatementContexts

        def hintStatement(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.HintStatementContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.HintStatementContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_hint

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterHint" ):
                listener.enterHint(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitHint" ):
                listener.exitHint(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitHint" ):
                return visitor.visitHint(self)
            else:
                return visitor.visitChildren(self)




    def hint(self):

        localctx = SparkSqlBaseParser.HintContext(self, self._ctx, self.state)
        self.enterRule(localctx, 78, self.RULE_hint)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1462
            self.match(SparkSqlBaseParser.T__4)
            self.state = 1463
            localctx._hintStatement = self.hintStatement()
            localctx.hintStatements.append(localctx._hintStatement)
            self.state = 1470
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__2) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                self.state = 1465
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.T__2:
                    self.state = 1464
                    self.match(SparkSqlBaseParser.T__2)


                self.state = 1467
                localctx._hintStatement = self.hintStatement()
                localctx.hintStatements.append(localctx._hintStatement)
                self.state = 1472
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1473
            self.match(SparkSqlBaseParser.T__5)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class HintStatementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.hintName = None # IdentifierContext
            self._primaryExpression = None # PrimaryExpressionContext
            self.parameters = list() # of PrimaryExpressionContexts

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def primaryExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.PrimaryExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.PrimaryExpressionContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_hintStatement

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterHintStatement" ):
                listener.enterHintStatement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitHintStatement" ):
                listener.exitHintStatement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitHintStatement" ):
                return visitor.visitHintStatement(self)
            else:
                return visitor.visitChildren(self)




    def hintStatement(self):

        localctx = SparkSqlBaseParser.HintStatementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 80, self.RULE_hintStatement)
        self._la = 0 # Token type
        try:
            self.state = 1488
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,189,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1475
                localctx.hintName = self.identifier()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1476
                localctx.hintName = self.identifier()
                self.state = 1477
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1478
                localctx._primaryExpression = self.primaryExpression(0)
                localctx.parameters.append(localctx._primaryExpression)
                self.state = 1483
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1479
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1480
                    localctx._primaryExpression = self.primaryExpression(0)
                    localctx.parameters.append(localctx._primaryExpression)
                    self.state = 1485
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1486
                self.match(SparkSqlBaseParser.T__1)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class FromClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)

        def relation(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.RelationContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.RelationContext,i)


        def lateralView(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.LateralViewContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.LateralViewContext,i)


        def pivotClause(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PivotClauseContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_fromClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFromClause" ):
                listener.enterFromClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFromClause" ):
                listener.exitFromClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFromClause" ):
                return visitor.visitFromClause(self)
            else:
                return visitor.visitChildren(self)




    def fromClause(self):

        localctx = SparkSqlBaseParser.FromClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 82, self.RULE_fromClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1490
            self.match(SparkSqlBaseParser.FROM)
            self.state = 1491
            self.relation()
            self.state = 1496
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,190,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1492
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1493
                    self.relation() 
                self.state = 1498
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,190,self._ctx)

            self.state = 1502
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,191,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1499
                    self.lateralView() 
                self.state = 1504
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,191,self._ctx)

            self.state = 1506
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,192,self._ctx)
            if la_ == 1:
                self.state = 1505
                self.pivotClause()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class AggregationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._expression = None # ExpressionContext
            self.groupingExpressions = list() # of ExpressionContexts
            self.kind = None # Token

        def GROUP(self):
            return self.getToken(SparkSqlBaseParser.GROUP, 0)

        def BY(self):
            return self.getToken(SparkSqlBaseParser.BY, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def WITH(self):
            return self.getToken(SparkSqlBaseParser.WITH, 0)

        def SETS(self):
            return self.getToken(SparkSqlBaseParser.SETS, 0)

        def groupingSet(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.GroupingSetContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.GroupingSetContext,i)


        def ROLLUP(self):
            return self.getToken(SparkSqlBaseParser.ROLLUP, 0)

        def CUBE(self):
            return self.getToken(SparkSqlBaseParser.CUBE, 0)

        def GROUPING(self):
            return self.getToken(SparkSqlBaseParser.GROUPING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_aggregation

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAggregation" ):
                listener.enterAggregation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAggregation" ):
                listener.exitAggregation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAggregation" ):
                return visitor.visitAggregation(self)
            else:
                return visitor.visitChildren(self)




    def aggregation(self):

        localctx = SparkSqlBaseParser.AggregationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 84, self.RULE_aggregation)
        self._la = 0 # Token type
        try:
            self.state = 1552
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,197,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1508
                self.match(SparkSqlBaseParser.GROUP)
                self.state = 1509
                self.match(SparkSqlBaseParser.BY)
                self.state = 1510
                localctx._expression = self.expression()
                localctx.groupingExpressions.append(localctx._expression)
                self.state = 1515
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,193,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1511
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 1512
                        localctx._expression = self.expression()
                        localctx.groupingExpressions.append(localctx._expression) 
                    self.state = 1517
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,193,self._ctx)

                self.state = 1535
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,195,self._ctx)
                if la_ == 1:
                    self.state = 1518
                    self.match(SparkSqlBaseParser.WITH)
                    self.state = 1519
                    localctx.kind = self.match(SparkSqlBaseParser.ROLLUP)

                elif la_ == 2:
                    self.state = 1520
                    self.match(SparkSqlBaseParser.WITH)
                    self.state = 1521
                    localctx.kind = self.match(SparkSqlBaseParser.CUBE)

                elif la_ == 3:
                    self.state = 1522
                    localctx.kind = self.match(SparkSqlBaseParser.GROUPING)
                    self.state = 1523
                    self.match(SparkSqlBaseParser.SETS)
                    self.state = 1524
                    self.match(SparkSqlBaseParser.T__0)
                    self.state = 1525
                    self.groupingSet()
                    self.state = 1530
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__2:
                        self.state = 1526
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 1527
                        self.groupingSet()
                        self.state = 1532
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    self.state = 1533
                    self.match(SparkSqlBaseParser.T__1)


                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1537
                self.match(SparkSqlBaseParser.GROUP)
                self.state = 1538
                self.match(SparkSqlBaseParser.BY)
                self.state = 1539
                localctx.kind = self.match(SparkSqlBaseParser.GROUPING)
                self.state = 1540
                self.match(SparkSqlBaseParser.SETS)
                self.state = 1541
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1542
                self.groupingSet()
                self.state = 1547
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1543
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1544
                    self.groupingSet()
                    self.state = 1549
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1550
                self.match(SparkSqlBaseParser.T__1)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class GroupingSetContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_groupingSet

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterGroupingSet" ):
                listener.enterGroupingSet(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitGroupingSet" ):
                listener.exitGroupingSet(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitGroupingSet" ):
                return visitor.visitGroupingSet(self)
            else:
                return visitor.visitChildren(self)




    def groupingSet(self):

        localctx = SparkSqlBaseParser.GroupingSetContext(self, self._ctx, self.state)
        self.enterRule(localctx, 86, self.RULE_groupingSet)
        self._la = 0 # Token type
        try:
            self.state = 1567
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,200,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1554
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1563
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.PLUS - 128)) | (1 << (SparkSqlBaseParser.MINUS - 128)) | (1 << (SparkSqlBaseParser.ASTERISK - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.TILDE - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.STRING - 192)) | (1 << (SparkSqlBaseParser.BIGINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.SMALLINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.TINYINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.INTEGER_VALUE - 192)) | (1 << (SparkSqlBaseParser.DECIMAL_VALUE - 192)) | (1 << (SparkSqlBaseParser.DOUBLE_LITERAL - 192)) | (1 << (SparkSqlBaseParser.BIGDECIMAL_LITERAL - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                    self.state = 1555
                    self.expression()
                    self.state = 1560
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__2:
                        self.state = 1556
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 1557
                        self.expression()
                        self.state = 1562
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                self.state = 1565
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1566
                self.expression()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PivotClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.aggregates = None # NamedExpressionSeqContext
            self._pivotValue = None # PivotValueContext
            self.pivotValues = list() # of PivotValueContexts

        def PIVOT(self):
            return self.getToken(SparkSqlBaseParser.PIVOT, 0)

        def FOR(self):
            return self.getToken(SparkSqlBaseParser.FOR, 0)

        def pivotColumn(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PivotColumnContext,0)


        def IN(self):
            return self.getToken(SparkSqlBaseParser.IN, 0)

        def namedExpressionSeq(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.NamedExpressionSeqContext,0)


        def pivotValue(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.PivotValueContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.PivotValueContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_pivotClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPivotClause" ):
                listener.enterPivotClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPivotClause" ):
                listener.exitPivotClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPivotClause" ):
                return visitor.visitPivotClause(self)
            else:
                return visitor.visitChildren(self)




    def pivotClause(self):

        localctx = SparkSqlBaseParser.PivotClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 88, self.RULE_pivotClause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1569
            self.match(SparkSqlBaseParser.PIVOT)
            self.state = 1570
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1571
            localctx.aggregates = self.namedExpressionSeq()
            self.state = 1572
            self.match(SparkSqlBaseParser.FOR)
            self.state = 1573
            self.pivotColumn()
            self.state = 1574
            self.match(SparkSqlBaseParser.IN)
            self.state = 1575
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1576
            localctx._pivotValue = self.pivotValue()
            localctx.pivotValues.append(localctx._pivotValue)
            self.state = 1581
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1577
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1578
                localctx._pivotValue = self.pivotValue()
                localctx.pivotValues.append(localctx._pivotValue)
                self.state = 1583
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1584
            self.match(SparkSqlBaseParser.T__1)
            self.state = 1585
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PivotColumnContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self._identifier = None # IdentifierContext
            self.identifiers = list() # of IdentifierContexts

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_pivotColumn

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPivotColumn" ):
                listener.enterPivotColumn(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPivotColumn" ):
                listener.exitPivotColumn(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPivotColumn" ):
                return visitor.visitPivotColumn(self)
            else:
                return visitor.visitChildren(self)




    def pivotColumn(self):

        localctx = SparkSqlBaseParser.PivotColumnContext(self, self._ctx, self.state)
        self.enterRule(localctx, 90, self.RULE_pivotColumn)
        self._la = 0 # Token type
        try:
            self.state = 1599
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.ADD, SparkSqlBaseParser.AS, SparkSqlBaseParser.ALL, SparkSqlBaseParser.ANY, SparkSqlBaseParser.DISTINCT, SparkSqlBaseParser.WHERE, SparkSqlBaseParser.GROUP, SparkSqlBaseParser.BY, SparkSqlBaseParser.GROUPING, SparkSqlBaseParser.SETS, SparkSqlBaseParser.CUBE, SparkSqlBaseParser.ROLLUP, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.HAVING, SparkSqlBaseParser.LIMIT, SparkSqlBaseParser.AT, SparkSqlBaseParser.OR, SparkSqlBaseParser.AND, SparkSqlBaseParser.IN, SparkSqlBaseParser.NOT, SparkSqlBaseParser.NO, SparkSqlBaseParser.EXISTS, SparkSqlBaseParser.BETWEEN, SparkSqlBaseParser.LIKE, SparkSqlBaseParser.RLIKE, SparkSqlBaseParser.IS, SparkSqlBaseParser.NULL, SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE, SparkSqlBaseParser.NULLS, SparkSqlBaseParser.ASC, SparkSqlBaseParser.DESC, SparkSqlBaseParser.FOR, SparkSqlBaseParser.INTERVAL, SparkSqlBaseParser.CASE, SparkSqlBaseParser.WHEN, SparkSqlBaseParser.THEN, SparkSqlBaseParser.ELSE, SparkSqlBaseParser.END, SparkSqlBaseParser.JOIN, SparkSqlBaseParser.CROSS, SparkSqlBaseParser.OUTER, SparkSqlBaseParser.INNER, SparkSqlBaseParser.LEFT, SparkSqlBaseParser.SEMI, SparkSqlBaseParser.RIGHT, SparkSqlBaseParser.FULL, SparkSqlBaseParser.NATURAL, SparkSqlBaseParser.ON, SparkSqlBaseParser.PIVOT, SparkSqlBaseParser.LATERAL, SparkSqlBaseParser.WINDOW, SparkSqlBaseParser.OVER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.UNBOUNDED, SparkSqlBaseParser.PRECEDING, SparkSqlBaseParser.FOLLOWING, SparkSqlBaseParser.CURRENT, SparkSqlBaseParser.FIRST, SparkSqlBaseParser.AFTER, SparkSqlBaseParser.LAST, SparkSqlBaseParser.ROW, SparkSqlBaseParser.WITH, SparkSqlBaseParser.VALUES, SparkSqlBaseParser.CREATE, SparkSqlBaseParser.TABLE, SparkSqlBaseParser.DIRECTORY, SparkSqlBaseParser.VIEW, SparkSqlBaseParser.REPLACE, SparkSqlBaseParser.INSERT, SparkSqlBaseParser.DELETE, SparkSqlBaseParser.INTO, SparkSqlBaseParser.DESCRIBE, SparkSqlBaseParser.EXPLAIN, SparkSqlBaseParser.FORMAT, SparkSqlBaseParser.LOGICAL, SparkSqlBaseParser.CODEGEN, SparkSqlBaseParser.COST, SparkSqlBaseParser.CAST, SparkSqlBaseParser.SHOW, SparkSqlBaseParser.TABLES, SparkSqlBaseParser.COLUMNS, SparkSqlBaseParser.COLUMN, SparkSqlBaseParser.USE, SparkSqlBaseParser.PARTITIONS, SparkSqlBaseParser.FUNCTIONS, SparkSqlBaseParser.DROP, SparkSqlBaseParser.UNION, SparkSqlBaseParser.EXCEPT, SparkSqlBaseParser.SETMINUS, SparkSqlBaseParser.INTERSECT, SparkSqlBaseParser.TO, SparkSqlBaseParser.TABLESAMPLE, SparkSqlBaseParser.STRATIFY, SparkSqlBaseParser.ALTER, SparkSqlBaseParser.RENAME, SparkSqlBaseParser.ARRAY, SparkSqlBaseParser.MAP, SparkSqlBaseParser.STRUCT, SparkSqlBaseParser.COMMENT, SparkSqlBaseParser.SET, SparkSqlBaseParser.RESET, SparkSqlBaseParser.DATA, SparkSqlBaseParser.START, SparkSqlBaseParser.TRANSACTION, SparkSqlBaseParser.COMMIT, SparkSqlBaseParser.ROLLBACK, SparkSqlBaseParser.MACRO, SparkSqlBaseParser.IGNORE, SparkSqlBaseParser.BOTH, SparkSqlBaseParser.LEADING, SparkSqlBaseParser.TRAILING, SparkSqlBaseParser.IF, SparkSqlBaseParser.POSITION, SparkSqlBaseParser.EXTRACT, SparkSqlBaseParser.DIV, SparkSqlBaseParser.PERCENTLIT, SparkSqlBaseParser.BUCKET, SparkSqlBaseParser.OUT, SparkSqlBaseParser.OF, SparkSqlBaseParser.SORT, SparkSqlBaseParser.CLUSTER, SparkSqlBaseParser.DISTRIBUTE, SparkSqlBaseParser.OVERWRITE, SparkSqlBaseParser.TRANSFORM, SparkSqlBaseParser.REDUCE, SparkSqlBaseParser.SERDE, SparkSqlBaseParser.SERDEPROPERTIES, SparkSqlBaseParser.RECORDREADER, SparkSqlBaseParser.RECORDWRITER, SparkSqlBaseParser.DELIMITED, SparkSqlBaseParser.FIELDS, SparkSqlBaseParser.TERMINATED, SparkSqlBaseParser.COLLECTION, SparkSqlBaseParser.ITEMS, SparkSqlBaseParser.KEYS, SparkSqlBaseParser.ESCAPED, SparkSqlBaseParser.LINES, SparkSqlBaseParser.SEPARATED, SparkSqlBaseParser.FUNCTION, SparkSqlBaseParser.EXTENDED, SparkSqlBaseParser.REFRESH, SparkSqlBaseParser.CLEAR, SparkSqlBaseParser.CACHE, SparkSqlBaseParser.UNCACHE, SparkSqlBaseParser.LAZY, SparkSqlBaseParser.FORMATTED, SparkSqlBaseParser.GLOBAL, SparkSqlBaseParser.TEMPORARY, SparkSqlBaseParser.OPTIONS, SparkSqlBaseParser.UNSET, SparkSqlBaseParser.TBLPROPERTIES, SparkSqlBaseParser.DBPROPERTIES, SparkSqlBaseParser.BUCKETS, SparkSqlBaseParser.SKEWED, SparkSqlBaseParser.STORED, SparkSqlBaseParser.DIRECTORIES, SparkSqlBaseParser.LOCATION, SparkSqlBaseParser.EXCHANGE, SparkSqlBaseParser.ARCHIVE, SparkSqlBaseParser.UNARCHIVE, SparkSqlBaseParser.FILEFORMAT, SparkSqlBaseParser.TOUCH, SparkSqlBaseParser.COMPACT, SparkSqlBaseParser.CONCATENATE, SparkSqlBaseParser.CHANGE, SparkSqlBaseParser.CASCADE, SparkSqlBaseParser.RESTRICT, SparkSqlBaseParser.CLUSTERED, SparkSqlBaseParser.SORTED, SparkSqlBaseParser.PURGE, SparkSqlBaseParser.INPUTFORMAT, SparkSqlBaseParser.OUTPUTFORMAT, SparkSqlBaseParser.DATABASE, SparkSqlBaseParser.DATABASES, SparkSqlBaseParser.DFS, SparkSqlBaseParser.TRUNCATE, SparkSqlBaseParser.ANALYZE, SparkSqlBaseParser.COMPUTE, SparkSqlBaseParser.LIST, SparkSqlBaseParser.STATISTICS, SparkSqlBaseParser.PARTITIONED, SparkSqlBaseParser.EXTERNAL, SparkSqlBaseParser.DEFINED, SparkSqlBaseParser.REVOKE, SparkSqlBaseParser.GRANT, SparkSqlBaseParser.LOCK, SparkSqlBaseParser.UNLOCK, SparkSqlBaseParser.MSCK, SparkSqlBaseParser.REPAIR, SparkSqlBaseParser.RECOVER, SparkSqlBaseParser.EXPORT, SparkSqlBaseParser.IMPORT, SparkSqlBaseParser.LOAD, SparkSqlBaseParser.ROLE, SparkSqlBaseParser.ROLES, SparkSqlBaseParser.COMPACTIONS, SparkSqlBaseParser.PRINCIPALS, SparkSqlBaseParser.TRANSACTIONS, SparkSqlBaseParser.INDEX, SparkSqlBaseParser.INDEXES, SparkSqlBaseParser.LOCKS, SparkSqlBaseParser.OPTION, SparkSqlBaseParser.ANTI, SparkSqlBaseParser.LOCAL, SparkSqlBaseParser.INPATH, SparkSqlBaseParser.IDENTIFIER, SparkSqlBaseParser.BACKQUOTED_IDENTIFIER]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1587
                localctx._identifier = self.identifier()
                localctx.identifiers.append(localctx._identifier)
                pass
            elif token in [SparkSqlBaseParser.T__0]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1588
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1589
                localctx._identifier = self.identifier()
                localctx.identifiers.append(localctx._identifier)
                self.state = 1594
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1590
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1591
                    localctx._identifier = self.identifier()
                    localctx.identifiers.append(localctx._identifier)
                    self.state = 1596
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1597
                self.match(SparkSqlBaseParser.T__1)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PivotValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_pivotValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPivotValue" ):
                listener.enterPivotValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPivotValue" ):
                listener.exitPivotValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPivotValue" ):
                return visitor.visitPivotValue(self)
            else:
                return visitor.visitChildren(self)




    def pivotValue(self):

        localctx = SparkSqlBaseParser.PivotValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 92, self.RULE_pivotValue)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1601
            self.expression()
            self.state = 1606
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                self.state = 1603
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,204,self._ctx)
                if la_ == 1:
                    self.state = 1602
                    self.match(SparkSqlBaseParser.AS)


                self.state = 1605
                self.identifier()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class LateralViewContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.tblName = None # IdentifierContext
            self._identifier = None # IdentifierContext
            self.colName = list() # of IdentifierContexts

        def LATERAL(self):
            return self.getToken(SparkSqlBaseParser.LATERAL, 0)

        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)


        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def OUTER(self):
            return self.getToken(SparkSqlBaseParser.OUTER, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_lateralView

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLateralView" ):
                listener.enterLateralView(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLateralView" ):
                listener.exitLateralView(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLateralView" ):
                return visitor.visitLateralView(self)
            else:
                return visitor.visitChildren(self)




    def lateralView(self):

        localctx = SparkSqlBaseParser.LateralViewContext(self, self._ctx, self.state)
        self.enterRule(localctx, 94, self.RULE_lateralView)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1608
            self.match(SparkSqlBaseParser.LATERAL)
            self.state = 1609
            self.match(SparkSqlBaseParser.VIEW)
            self.state = 1611
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,206,self._ctx)
            if la_ == 1:
                self.state = 1610
                self.match(SparkSqlBaseParser.OUTER)


            self.state = 1613
            self.qualifiedName()
            self.state = 1614
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1623
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.PLUS - 128)) | (1 << (SparkSqlBaseParser.MINUS - 128)) | (1 << (SparkSqlBaseParser.ASTERISK - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.TILDE - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.STRING - 192)) | (1 << (SparkSqlBaseParser.BIGINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.SMALLINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.TINYINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.INTEGER_VALUE - 192)) | (1 << (SparkSqlBaseParser.DECIMAL_VALUE - 192)) | (1 << (SparkSqlBaseParser.DOUBLE_LITERAL - 192)) | (1 << (SparkSqlBaseParser.BIGDECIMAL_LITERAL - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                self.state = 1615
                self.expression()
                self.state = 1620
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1616
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1617
                    self.expression()
                    self.state = 1622
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 1625
            self.match(SparkSqlBaseParser.T__1)
            self.state = 1626
            localctx.tblName = self.identifier()
            self.state = 1638
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,211,self._ctx)
            if la_ == 1:
                self.state = 1628
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,209,self._ctx)
                if la_ == 1:
                    self.state = 1627
                    self.match(SparkSqlBaseParser.AS)


                self.state = 1630
                localctx._identifier = self.identifier()
                localctx.colName.append(localctx._identifier)
                self.state = 1635
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,210,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 1631
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 1632
                        localctx._identifier = self.identifier()
                        localctx.colName.append(localctx._identifier) 
                    self.state = 1637
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,210,self._ctx)



        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SetQuantifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def DISTINCT(self):
            return self.getToken(SparkSqlBaseParser.DISTINCT, 0)

        def ALL(self):
            return self.getToken(SparkSqlBaseParser.ALL, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_setQuantifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSetQuantifier" ):
                listener.enterSetQuantifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSetQuantifier" ):
                listener.exitSetQuantifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSetQuantifier" ):
                return visitor.visitSetQuantifier(self)
            else:
                return visitor.visitChildren(self)




    def setQuantifier(self):

        localctx = SparkSqlBaseParser.SetQuantifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 96, self.RULE_setQuantifier)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1640
            _la = self._input.LA(1)
            if not(_la==SparkSqlBaseParser.ALL or _la==SparkSqlBaseParser.DISTINCT):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class RelationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def relationPrimary(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.RelationPrimaryContext,0)


        def joinRelation(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.JoinRelationContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.JoinRelationContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_relation

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRelation" ):
                listener.enterRelation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRelation" ):
                listener.exitRelation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRelation" ):
                return visitor.visitRelation(self)
            else:
                return visitor.visitChildren(self)




    def relation(self):

        localctx = SparkSqlBaseParser.RelationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 98, self.RULE_relation)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1642
            self.relationPrimary()
            self.state = 1646
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,212,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1643
                    self.joinRelation() 
                self.state = 1648
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,212,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class JoinRelationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.right = None # RelationPrimaryContext

        def JOIN(self):
            return self.getToken(SparkSqlBaseParser.JOIN, 0)

        def relationPrimary(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.RelationPrimaryContext,0)


        def joinType(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.JoinTypeContext,0)


        def joinCriteria(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.JoinCriteriaContext,0)


        def NATURAL(self):
            return self.getToken(SparkSqlBaseParser.NATURAL, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_joinRelation

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoinRelation" ):
                listener.enterJoinRelation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoinRelation" ):
                listener.exitJoinRelation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoinRelation" ):
                return visitor.visitJoinRelation(self)
            else:
                return visitor.visitChildren(self)




    def joinRelation(self):

        localctx = SparkSqlBaseParser.JoinRelationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 100, self.RULE_joinRelation)
        try:
            self.state = 1660
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.JOIN, SparkSqlBaseParser.CROSS, SparkSqlBaseParser.INNER, SparkSqlBaseParser.LEFT, SparkSqlBaseParser.RIGHT, SparkSqlBaseParser.FULL, SparkSqlBaseParser.ANTI]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1649
                self.joinType()
                self.state = 1650
                self.match(SparkSqlBaseParser.JOIN)
                self.state = 1651
                localctx.right = self.relationPrimary()
                self.state = 1653
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,213,self._ctx)
                if la_ == 1:
                    self.state = 1652
                    self.joinCriteria()


                pass
            elif token in [SparkSqlBaseParser.NATURAL]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1655
                self.match(SparkSqlBaseParser.NATURAL)
                self.state = 1656
                self.joinType()
                self.state = 1657
                self.match(SparkSqlBaseParser.JOIN)
                self.state = 1658
                localctx.right = self.relationPrimary()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class JoinTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INNER(self):
            return self.getToken(SparkSqlBaseParser.INNER, 0)

        def CROSS(self):
            return self.getToken(SparkSqlBaseParser.CROSS, 0)

        def LEFT(self):
            return self.getToken(SparkSqlBaseParser.LEFT, 0)

        def OUTER(self):
            return self.getToken(SparkSqlBaseParser.OUTER, 0)

        def SEMI(self):
            return self.getToken(SparkSqlBaseParser.SEMI, 0)

        def RIGHT(self):
            return self.getToken(SparkSqlBaseParser.RIGHT, 0)

        def FULL(self):
            return self.getToken(SparkSqlBaseParser.FULL, 0)

        def ANTI(self):
            return self.getToken(SparkSqlBaseParser.ANTI, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_joinType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoinType" ):
                listener.enterJoinType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoinType" ):
                listener.exitJoinType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoinType" ):
                return visitor.visitJoinType(self)
            else:
                return visitor.visitChildren(self)




    def joinType(self):

        localctx = SparkSqlBaseParser.JoinTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 102, self.RULE_joinType)
        self._la = 0 # Token type
        try:
            self.state = 1684
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,220,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1663
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.INNER:
                    self.state = 1662
                    self.match(SparkSqlBaseParser.INNER)


                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1665
                self.match(SparkSqlBaseParser.CROSS)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 1666
                self.match(SparkSqlBaseParser.LEFT)
                self.state = 1668
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OUTER:
                    self.state = 1667
                    self.match(SparkSqlBaseParser.OUTER)


                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 1670
                self.match(SparkSqlBaseParser.LEFT)
                self.state = 1671
                self.match(SparkSqlBaseParser.SEMI)
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 1672
                self.match(SparkSqlBaseParser.RIGHT)
                self.state = 1674
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OUTER:
                    self.state = 1673
                    self.match(SparkSqlBaseParser.OUTER)


                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 1676
                self.match(SparkSqlBaseParser.FULL)
                self.state = 1678
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.OUTER:
                    self.state = 1677
                    self.match(SparkSqlBaseParser.OUTER)


                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 1681
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.LEFT:
                    self.state = 1680
                    self.match(SparkSqlBaseParser.LEFT)


                self.state = 1683
                self.match(SparkSqlBaseParser.ANTI)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class JoinCriteriaContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ON(self):
            return self.getToken(SparkSqlBaseParser.ON, 0)

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.BooleanExpressionContext,0)


        def USING(self):
            return self.getToken(SparkSqlBaseParser.USING, 0)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_joinCriteria

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoinCriteria" ):
                listener.enterJoinCriteria(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoinCriteria" ):
                listener.exitJoinCriteria(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoinCriteria" ):
                return visitor.visitJoinCriteria(self)
            else:
                return visitor.visitChildren(self)




    def joinCriteria(self):

        localctx = SparkSqlBaseParser.JoinCriteriaContext(self, self._ctx, self.state)
        self.enterRule(localctx, 104, self.RULE_joinCriteria)
        try:
            self.state = 1690
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.ON]:
                self.enterOuterAlt(localctx, 1)
                self.state = 1686
                self.match(SparkSqlBaseParser.ON)
                self.state = 1687
                self.booleanExpression(0)
                pass
            elif token in [SparkSqlBaseParser.USING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 1688
                self.match(SparkSqlBaseParser.USING)
                self.state = 1689
                self.identifierList()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SampleContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def TABLESAMPLE(self):
            return self.getToken(SparkSqlBaseParser.TABLESAMPLE, 0)

        def sampleMethod(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.SampleMethodContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_sample

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSample" ):
                listener.enterSample(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSample" ):
                listener.exitSample(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSample" ):
                return visitor.visitSample(self)
            else:
                return visitor.visitChildren(self)




    def sample(self):

        localctx = SparkSqlBaseParser.SampleContext(self, self._ctx, self.state)
        self.enterRule(localctx, 106, self.RULE_sample)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1692
            self.match(SparkSqlBaseParser.TABLESAMPLE)
            self.state = 1693
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1695
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.PLUS - 128)) | (1 << (SparkSqlBaseParser.MINUS - 128)) | (1 << (SparkSqlBaseParser.ASTERISK - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.TILDE - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.STRING - 192)) | (1 << (SparkSqlBaseParser.BIGINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.SMALLINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.TINYINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.INTEGER_VALUE - 192)) | (1 << (SparkSqlBaseParser.DECIMAL_VALUE - 192)) | (1 << (SparkSqlBaseParser.DOUBLE_LITERAL - 192)) | (1 << (SparkSqlBaseParser.BIGDECIMAL_LITERAL - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                self.state = 1694
                self.sampleMethod()


            self.state = 1697
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SampleMethodContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_sampleMethod

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class SampleByRowsContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.SampleMethodContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)

        def ROWS(self):
            return self.getToken(SparkSqlBaseParser.ROWS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByRows" ):
                listener.enterSampleByRows(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByRows" ):
                listener.exitSampleByRows(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByRows" ):
                return visitor.visitSampleByRows(self)
            else:
                return visitor.visitChildren(self)


    class SampleByPercentileContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.SampleMethodContext
            super().__init__(parser)
            self.negativeSign = None # Token
            self.percentage = None # Token
            self.copyFrom(ctx)

        def PERCENTLIT(self):
            return self.getToken(SparkSqlBaseParser.PERCENTLIT, 0)
        def INTEGER_VALUE(self):
            return self.getToken(SparkSqlBaseParser.INTEGER_VALUE, 0)
        def DECIMAL_VALUE(self):
            return self.getToken(SparkSqlBaseParser.DECIMAL_VALUE, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByPercentile" ):
                listener.enterSampleByPercentile(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByPercentile" ):
                listener.exitSampleByPercentile(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByPercentile" ):
                return visitor.visitSampleByPercentile(self)
            else:
                return visitor.visitChildren(self)


    class SampleByBucketContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.SampleMethodContext
            super().__init__(parser)
            self.sampleType = None # Token
            self.numerator = None # Token
            self.denominator = None # Token
            self.copyFrom(ctx)

        def OUT(self):
            return self.getToken(SparkSqlBaseParser.OUT, 0)
        def OF(self):
            return self.getToken(SparkSqlBaseParser.OF, 0)
        def BUCKET(self):
            return self.getToken(SparkSqlBaseParser.BUCKET, 0)
        def INTEGER_VALUE(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.INTEGER_VALUE)
            else:
                return self.getToken(SparkSqlBaseParser.INTEGER_VALUE, i)
        def ON(self):
            return self.getToken(SparkSqlBaseParser.ON, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByBucket" ):
                listener.enterSampleByBucket(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByBucket" ):
                listener.exitSampleByBucket(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByBucket" ):
                return visitor.visitSampleByBucket(self)
            else:
                return visitor.visitChildren(self)


    class SampleByBytesContext(SampleMethodContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.SampleMethodContext
            super().__init__(parser)
            self.bytes = None # ExpressionContext
            self.copyFrom(ctx)

        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSampleByBytes" ):
                listener.enterSampleByBytes(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSampleByBytes" ):
                listener.exitSampleByBytes(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSampleByBytes" ):
                return visitor.visitSampleByBytes(self)
            else:
                return visitor.visitChildren(self)



    def sampleMethod(self):

        localctx = SparkSqlBaseParser.SampleMethodContext(self, self._ctx, self.state)
        self.enterRule(localctx, 108, self.RULE_sampleMethod)
        self._la = 0 # Token type
        try:
            self.state = 1723
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,226,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.SampleByPercentileContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1700
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 1699
                    localctx.negativeSign = self.match(SparkSqlBaseParser.MINUS)


                self.state = 1702
                localctx.percentage = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.INTEGER_VALUE or _la==SparkSqlBaseParser.DECIMAL_VALUE):
                    localctx.percentage = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 1703
                self.match(SparkSqlBaseParser.PERCENTLIT)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.SampleByRowsContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1704
                self.expression()
                self.state = 1705
                self.match(SparkSqlBaseParser.ROWS)
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.SampleByBucketContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 1707
                localctx.sampleType = self.match(SparkSqlBaseParser.BUCKET)
                self.state = 1708
                localctx.numerator = self.match(SparkSqlBaseParser.INTEGER_VALUE)
                self.state = 1709
                self.match(SparkSqlBaseParser.OUT)
                self.state = 1710
                self.match(SparkSqlBaseParser.OF)
                self.state = 1711
                localctx.denominator = self.match(SparkSqlBaseParser.INTEGER_VALUE)
                self.state = 1720
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.ON:
                    self.state = 1712
                    self.match(SparkSqlBaseParser.ON)
                    self.state = 1718
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,224,self._ctx)
                    if la_ == 1:
                        self.state = 1713
                        self.identifier()
                        pass

                    elif la_ == 2:
                        self.state = 1714
                        self.qualifiedName()
                        self.state = 1715
                        self.match(SparkSqlBaseParser.T__0)
                        self.state = 1716
                        self.match(SparkSqlBaseParser.T__1)
                        pass




                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.SampleByBytesContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 1722
                localctx.bytes = self.expression()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IdentifierListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifierSeq(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierSeqContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_identifierList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierList" ):
                listener.enterIdentifierList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierList" ):
                listener.exitIdentifierList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierList" ):
                return visitor.visitIdentifierList(self)
            else:
                return visitor.visitChildren(self)




    def identifierList(self):

        localctx = SparkSqlBaseParser.IdentifierListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 110, self.RULE_identifierList)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1725
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1726
            self.identifierSeq()
            self.state = 1727
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IdentifierSeqContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_identifierSeq

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierSeq" ):
                listener.enterIdentifierSeq(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierSeq" ):
                listener.exitIdentifierSeq(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierSeq" ):
                return visitor.visitIdentifierSeq(self)
            else:
                return visitor.visitChildren(self)




    def identifierSeq(self):

        localctx = SparkSqlBaseParser.IdentifierSeqContext(self, self._ctx, self.state)
        self.enterRule(localctx, 112, self.RULE_identifierSeq)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1729
            self.identifier()
            self.state = 1734
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,227,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1730
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1731
                    self.identifier() 
                self.state = 1736
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,227,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class OrderedIdentifierListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def orderedIdentifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.OrderedIdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.OrderedIdentifierContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_orderedIdentifierList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrderedIdentifierList" ):
                listener.enterOrderedIdentifierList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrderedIdentifierList" ):
                listener.exitOrderedIdentifierList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrderedIdentifierList" ):
                return visitor.visitOrderedIdentifierList(self)
            else:
                return visitor.visitChildren(self)




    def orderedIdentifierList(self):

        localctx = SparkSqlBaseParser.OrderedIdentifierListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 114, self.RULE_orderedIdentifierList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1737
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1738
            self.orderedIdentifier()
            self.state = 1743
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1739
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1740
                self.orderedIdentifier()
                self.state = 1745
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1746
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class OrderedIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.ordering = None # Token

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def ASC(self):
            return self.getToken(SparkSqlBaseParser.ASC, 0)

        def DESC(self):
            return self.getToken(SparkSqlBaseParser.DESC, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_orderedIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrderedIdentifier" ):
                listener.enterOrderedIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrderedIdentifier" ):
                listener.exitOrderedIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrderedIdentifier" ):
                return visitor.visitOrderedIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def orderedIdentifier(self):

        localctx = SparkSqlBaseParser.OrderedIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 116, self.RULE_orderedIdentifier)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1748
            self.identifier()
            self.state = 1750
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.ASC or _la==SparkSqlBaseParser.DESC:
                self.state = 1749
                localctx.ordering = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.ASC or _la==SparkSqlBaseParser.DESC):
                    localctx.ordering = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IdentifierCommentListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifierComment(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierCommentContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierCommentContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_identifierCommentList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierCommentList" ):
                listener.enterIdentifierCommentList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierCommentList" ):
                listener.exitIdentifierCommentList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierCommentList" ):
                return visitor.visitIdentifierCommentList(self)
            else:
                return visitor.visitChildren(self)




    def identifierCommentList(self):

        localctx = SparkSqlBaseParser.IdentifierCommentListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 118, self.RULE_identifierCommentList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1752
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1753
            self.identifierComment()
            self.state = 1758
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 1754
                self.match(SparkSqlBaseParser.T__2)
                self.state = 1755
                self.identifierComment()
                self.state = 1760
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 1761
            self.match(SparkSqlBaseParser.T__1)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IdentifierCommentContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def COMMENT(self):
            return self.getToken(SparkSqlBaseParser.COMMENT, 0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_identifierComment

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifierComment" ):
                listener.enterIdentifierComment(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifierComment" ):
                listener.exitIdentifierComment(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifierComment" ):
                return visitor.visitIdentifierComment(self)
            else:
                return visitor.visitChildren(self)




    def identifierComment(self):

        localctx = SparkSqlBaseParser.IdentifierCommentContext(self, self._ctx, self.state)
        self.enterRule(localctx, 120, self.RULE_identifierComment)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1763
            self.identifier()
            self.state = 1766
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.COMMENT:
                self.state = 1764
                self.match(SparkSqlBaseParser.COMMENT)
                self.state = 1765
                self.match(SparkSqlBaseParser.STRING)


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class RelationPrimaryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_relationPrimary

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class TableValuedFunctionContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def functionTable(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.FunctionTableContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableValuedFunction" ):
                listener.enterTableValuedFunction(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableValuedFunction" ):
                listener.exitTableValuedFunction(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableValuedFunction" ):
                return visitor.visitTableValuedFunction(self)
            else:
                return visitor.visitChildren(self)


    class InlineTableDefault2Context(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def inlineTable(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.InlineTableContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInlineTableDefault2" ):
                listener.enterInlineTableDefault2(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInlineTableDefault2" ):
                listener.exitInlineTableDefault2(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInlineTableDefault2" ):
                return visitor.visitInlineTableDefault2(self)
            else:
                return visitor.visitChildren(self)


    class AliasedRelationContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def relation(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.RelationContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableAliasContext,0)

        def sample(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.SampleContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAliasedRelation" ):
                listener.enterAliasedRelation(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAliasedRelation" ):
                listener.exitAliasedRelation(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAliasedRelation" ):
                return visitor.visitAliasedRelation(self)
            else:
                return visitor.visitChildren(self)


    class AliasedQueryContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def queryNoWith(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryNoWithContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableAliasContext,0)

        def sample(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.SampleContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAliasedQuery" ):
                listener.enterAliasedQuery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAliasedQuery" ):
                listener.exitAliasedQuery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAliasedQuery" ):
                return visitor.visitAliasedQuery(self)
            else:
                return visitor.visitChildren(self)


    class TableNameContext(RelationPrimaryContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.RelationPrimaryContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def tableIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableIdentifierContext,0)

        def tableAlias(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableAliasContext,0)

        def sample(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.SampleContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableName" ):
                listener.enterTableName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableName" ):
                listener.exitTableName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableName" ):
                return visitor.visitTableName(self)
            else:
                return visitor.visitChildren(self)



    def relationPrimary(self):

        localctx = SparkSqlBaseParser.RelationPrimaryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 122, self.RULE_relationPrimary)
        try:
            self.state = 1792
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,235,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.TableNameContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1768
                self.tableIdentifier()
                self.state = 1770
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,232,self._ctx)
                if la_ == 1:
                    self.state = 1769
                    self.sample()


                self.state = 1772
                self.tableAlias()
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.AliasedQueryContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1774
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1775
                self.queryNoWith()
                self.state = 1776
                self.match(SparkSqlBaseParser.T__1)
                self.state = 1778
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,233,self._ctx)
                if la_ == 1:
                    self.state = 1777
                    self.sample()


                self.state = 1780
                self.tableAlias()
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.AliasedRelationContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 1782
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1783
                self.relation()
                self.state = 1784
                self.match(SparkSqlBaseParser.T__1)
                self.state = 1786
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,234,self._ctx)
                if la_ == 1:
                    self.state = 1785
                    self.sample()


                self.state = 1788
                self.tableAlias()
                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.InlineTableDefault2Context(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 1790
                self.inlineTable()
                pass

            elif la_ == 5:
                localctx = SparkSqlBaseParser.TableValuedFunctionContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 1791
                self.functionTable()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class InlineTableContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def VALUES(self):
            return self.getToken(SparkSqlBaseParser.VALUES, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def tableAlias(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableAliasContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_inlineTable

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInlineTable" ):
                listener.enterInlineTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInlineTable" ):
                listener.exitInlineTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInlineTable" ):
                return visitor.visitInlineTable(self)
            else:
                return visitor.visitChildren(self)




    def inlineTable(self):

        localctx = SparkSqlBaseParser.InlineTableContext(self, self._ctx, self.state)
        self.enterRule(localctx, 124, self.RULE_inlineTable)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1794
            self.match(SparkSqlBaseParser.VALUES)
            self.state = 1795
            self.expression()
            self.state = 1800
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,236,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1796
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1797
                    self.expression() 
                self.state = 1802
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,236,self._ctx)

            self.state = 1803
            self.tableAlias()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class FunctionTableContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def tableAlias(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TableAliasContext,0)


        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_functionTable

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFunctionTable" ):
                listener.enterFunctionTable(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFunctionTable" ):
                listener.exitFunctionTable(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFunctionTable" ):
                return visitor.visitFunctionTable(self)
            else:
                return visitor.visitChildren(self)




    def functionTable(self):

        localctx = SparkSqlBaseParser.FunctionTableContext(self, self._ctx, self.state)
        self.enterRule(localctx, 126, self.RULE_functionTable)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1805
            self.identifier()
            self.state = 1806
            self.match(SparkSqlBaseParser.T__0)
            self.state = 1815
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.PLUS - 128)) | (1 << (SparkSqlBaseParser.MINUS - 128)) | (1 << (SparkSqlBaseParser.ASTERISK - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.TILDE - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.STRING - 192)) | (1 << (SparkSqlBaseParser.BIGINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.SMALLINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.TINYINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.INTEGER_VALUE - 192)) | (1 << (SparkSqlBaseParser.DECIMAL_VALUE - 192)) | (1 << (SparkSqlBaseParser.DOUBLE_LITERAL - 192)) | (1 << (SparkSqlBaseParser.BIGDECIMAL_LITERAL - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                self.state = 1807
                self.expression()
                self.state = 1812
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1808
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1809
                    self.expression()
                    self.state = 1814
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)



            self.state = 1817
            self.match(SparkSqlBaseParser.T__1)
            self.state = 1818
            self.tableAlias()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class TableAliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def strictIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.StrictIdentifierContext,0)


        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def identifierList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierListContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_tableAlias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableAlias" ):
                listener.enterTableAlias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableAlias" ):
                listener.exitTableAlias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableAlias" ):
                return visitor.visitTableAlias(self)
            else:
                return visitor.visitChildren(self)




    def tableAlias(self):

        localctx = SparkSqlBaseParser.TableAliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 128, self.RULE_tableAlias)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1827
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,241,self._ctx)
            if la_ == 1:
                self.state = 1821
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,239,self._ctx)
                if la_ == 1:
                    self.state = 1820
                    self.match(SparkSqlBaseParser.AS)


                self.state = 1823
                self.strictIdentifier()
                self.state = 1825
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,240,self._ctx)
                if la_ == 1:
                    self.state = 1824
                    self.identifierList()




        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class RowFormatContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_rowFormat

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class RowFormatSerdeContext(RowFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.RowFormatContext
            super().__init__(parser)
            self.name = None # Token
            self.props = None # TablePropertyListContext
            self.copyFrom(ctx)

        def ROW(self):
            return self.getToken(SparkSqlBaseParser.ROW, 0)
        def FORMAT(self):
            return self.getToken(SparkSqlBaseParser.FORMAT, 0)
        def SERDE(self):
            return self.getToken(SparkSqlBaseParser.SERDE, 0)
        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)
        def WITH(self):
            return self.getToken(SparkSqlBaseParser.WITH, 0)
        def SERDEPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.SERDEPROPERTIES, 0)
        def tablePropertyList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.TablePropertyListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRowFormatSerde" ):
                listener.enterRowFormatSerde(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRowFormatSerde" ):
                listener.exitRowFormatSerde(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRowFormatSerde" ):
                return visitor.visitRowFormatSerde(self)
            else:
                return visitor.visitChildren(self)


    class RowFormatDelimitedContext(RowFormatContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.RowFormatContext
            super().__init__(parser)
            self.fieldsTerminatedBy = None # Token
            self.escapedBy = None # Token
            self.collectionItemsTerminatedBy = None # Token
            self.keysTerminatedBy = None # Token
            self.linesSeparatedBy = None # Token
            self.nullDefinedAs = None # Token
            self.copyFrom(ctx)

        def ROW(self):
            return self.getToken(SparkSqlBaseParser.ROW, 0)
        def FORMAT(self):
            return self.getToken(SparkSqlBaseParser.FORMAT, 0)
        def DELIMITED(self):
            return self.getToken(SparkSqlBaseParser.DELIMITED, 0)
        def FIELDS(self):
            return self.getToken(SparkSqlBaseParser.FIELDS, 0)
        def TERMINATED(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.TERMINATED)
            else:
                return self.getToken(SparkSqlBaseParser.TERMINATED, i)
        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.BY)
            else:
                return self.getToken(SparkSqlBaseParser.BY, i)
        def COLLECTION(self):
            return self.getToken(SparkSqlBaseParser.COLLECTION, 0)
        def ITEMS(self):
            return self.getToken(SparkSqlBaseParser.ITEMS, 0)
        def MAP(self):
            return self.getToken(SparkSqlBaseParser.MAP, 0)
        def KEYS(self):
            return self.getToken(SparkSqlBaseParser.KEYS, 0)
        def LINES(self):
            return self.getToken(SparkSqlBaseParser.LINES, 0)
        def NULL(self):
            return self.getToken(SparkSqlBaseParser.NULL, 0)
        def DEFINED(self):
            return self.getToken(SparkSqlBaseParser.DEFINED, 0)
        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)
        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.STRING)
            else:
                return self.getToken(SparkSqlBaseParser.STRING, i)
        def ESCAPED(self):
            return self.getToken(SparkSqlBaseParser.ESCAPED, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRowFormatDelimited" ):
                listener.enterRowFormatDelimited(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRowFormatDelimited" ):
                listener.exitRowFormatDelimited(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRowFormatDelimited" ):
                return visitor.visitRowFormatDelimited(self)
            else:
                return visitor.visitChildren(self)



    def rowFormat(self):

        localctx = SparkSqlBaseParser.RowFormatContext(self, self._ctx, self.state)
        self.enterRule(localctx, 130, self.RULE_rowFormat)
        try:
            self.state = 1878
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,249,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.RowFormatSerdeContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 1829
                self.match(SparkSqlBaseParser.ROW)
                self.state = 1830
                self.match(SparkSqlBaseParser.FORMAT)
                self.state = 1831
                self.match(SparkSqlBaseParser.SERDE)
                self.state = 1832
                localctx.name = self.match(SparkSqlBaseParser.STRING)
                self.state = 1836
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,242,self._ctx)
                if la_ == 1:
                    self.state = 1833
                    self.match(SparkSqlBaseParser.WITH)
                    self.state = 1834
                    self.match(SparkSqlBaseParser.SERDEPROPERTIES)
                    self.state = 1835
                    localctx.props = self.tablePropertyList()


                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.RowFormatDelimitedContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 1838
                self.match(SparkSqlBaseParser.ROW)
                self.state = 1839
                self.match(SparkSqlBaseParser.FORMAT)
                self.state = 1840
                self.match(SparkSqlBaseParser.DELIMITED)
                self.state = 1850
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,244,self._ctx)
                if la_ == 1:
                    self.state = 1841
                    self.match(SparkSqlBaseParser.FIELDS)
                    self.state = 1842
                    self.match(SparkSqlBaseParser.TERMINATED)
                    self.state = 1843
                    self.match(SparkSqlBaseParser.BY)
                    self.state = 1844
                    localctx.fieldsTerminatedBy = self.match(SparkSqlBaseParser.STRING)
                    self.state = 1848
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,243,self._ctx)
                    if la_ == 1:
                        self.state = 1845
                        self.match(SparkSqlBaseParser.ESCAPED)
                        self.state = 1846
                        self.match(SparkSqlBaseParser.BY)
                        self.state = 1847
                        localctx.escapedBy = self.match(SparkSqlBaseParser.STRING)




                self.state = 1857
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,245,self._ctx)
                if la_ == 1:
                    self.state = 1852
                    self.match(SparkSqlBaseParser.COLLECTION)
                    self.state = 1853
                    self.match(SparkSqlBaseParser.ITEMS)
                    self.state = 1854
                    self.match(SparkSqlBaseParser.TERMINATED)
                    self.state = 1855
                    self.match(SparkSqlBaseParser.BY)
                    self.state = 1856
                    localctx.collectionItemsTerminatedBy = self.match(SparkSqlBaseParser.STRING)


                self.state = 1864
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,246,self._ctx)
                if la_ == 1:
                    self.state = 1859
                    self.match(SparkSqlBaseParser.MAP)
                    self.state = 1860
                    self.match(SparkSqlBaseParser.KEYS)
                    self.state = 1861
                    self.match(SparkSqlBaseParser.TERMINATED)
                    self.state = 1862
                    self.match(SparkSqlBaseParser.BY)
                    self.state = 1863
                    localctx.keysTerminatedBy = self.match(SparkSqlBaseParser.STRING)


                self.state = 1870
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,247,self._ctx)
                if la_ == 1:
                    self.state = 1866
                    self.match(SparkSqlBaseParser.LINES)
                    self.state = 1867
                    self.match(SparkSqlBaseParser.TERMINATED)
                    self.state = 1868
                    self.match(SparkSqlBaseParser.BY)
                    self.state = 1869
                    localctx.linesSeparatedBy = self.match(SparkSqlBaseParser.STRING)


                self.state = 1876
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,248,self._ctx)
                if la_ == 1:
                    self.state = 1872
                    self.match(SparkSqlBaseParser.NULL)
                    self.state = 1873
                    self.match(SparkSqlBaseParser.DEFINED)
                    self.state = 1874
                    self.match(SparkSqlBaseParser.AS)
                    self.state = 1875
                    localctx.nullDefinedAs = self.match(SparkSqlBaseParser.STRING)


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class TableIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.db = None # IdentifierContext
            self.table = None # IdentifierContext

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_tableIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTableIdentifier" ):
                listener.enterTableIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTableIdentifier" ):
                listener.exitTableIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTableIdentifier" ):
                return visitor.visitTableIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def tableIdentifier(self):

        localctx = SparkSqlBaseParser.TableIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 132, self.RULE_tableIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1883
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,250,self._ctx)
            if la_ == 1:
                self.state = 1880
                localctx.db = self.identifier()
                self.state = 1881
                self.match(SparkSqlBaseParser.T__3)


            self.state = 1885
            localctx.table = self.identifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class FunctionIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.db = None # IdentifierContext
            self.function = None # IdentifierContext

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_functionIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFunctionIdentifier" ):
                listener.enterFunctionIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFunctionIdentifier" ):
                listener.exitFunctionIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFunctionIdentifier" ):
                return visitor.visitFunctionIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def functionIdentifier(self):

        localctx = SparkSqlBaseParser.FunctionIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 134, self.RULE_functionIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1890
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,251,self._ctx)
            if la_ == 1:
                self.state = 1887
                localctx.db = self.identifier()
                self.state = 1888
                self.match(SparkSqlBaseParser.T__3)


            self.state = 1892
            localctx.function = self.identifier()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NamedExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def identifierList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierListContext,0)


        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_namedExpression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedExpression" ):
                listener.enterNamedExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedExpression" ):
                listener.exitNamedExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedExpression" ):
                return visitor.visitNamedExpression(self)
            else:
                return visitor.visitChildren(self)




    def namedExpression(self):

        localctx = SparkSqlBaseParser.NamedExpressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 136, self.RULE_namedExpression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1894
            self.expression()
            self.state = 1902
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,254,self._ctx)
            if la_ == 1:
                self.state = 1896
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,252,self._ctx)
                if la_ == 1:
                    self.state = 1895
                    self.match(SparkSqlBaseParser.AS)


                self.state = 1900
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.ADD, SparkSqlBaseParser.AS, SparkSqlBaseParser.ALL, SparkSqlBaseParser.ANY, SparkSqlBaseParser.DISTINCT, SparkSqlBaseParser.WHERE, SparkSqlBaseParser.GROUP, SparkSqlBaseParser.BY, SparkSqlBaseParser.GROUPING, SparkSqlBaseParser.SETS, SparkSqlBaseParser.CUBE, SparkSqlBaseParser.ROLLUP, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.HAVING, SparkSqlBaseParser.LIMIT, SparkSqlBaseParser.AT, SparkSqlBaseParser.OR, SparkSqlBaseParser.AND, SparkSqlBaseParser.IN, SparkSqlBaseParser.NOT, SparkSqlBaseParser.NO, SparkSqlBaseParser.EXISTS, SparkSqlBaseParser.BETWEEN, SparkSqlBaseParser.LIKE, SparkSqlBaseParser.RLIKE, SparkSqlBaseParser.IS, SparkSqlBaseParser.NULL, SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE, SparkSqlBaseParser.NULLS, SparkSqlBaseParser.ASC, SparkSqlBaseParser.DESC, SparkSqlBaseParser.FOR, SparkSqlBaseParser.INTERVAL, SparkSqlBaseParser.CASE, SparkSqlBaseParser.WHEN, SparkSqlBaseParser.THEN, SparkSqlBaseParser.ELSE, SparkSqlBaseParser.END, SparkSqlBaseParser.JOIN, SparkSqlBaseParser.CROSS, SparkSqlBaseParser.OUTER, SparkSqlBaseParser.INNER, SparkSqlBaseParser.LEFT, SparkSqlBaseParser.SEMI, SparkSqlBaseParser.RIGHT, SparkSqlBaseParser.FULL, SparkSqlBaseParser.NATURAL, SparkSqlBaseParser.ON, SparkSqlBaseParser.PIVOT, SparkSqlBaseParser.LATERAL, SparkSqlBaseParser.WINDOW, SparkSqlBaseParser.OVER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.UNBOUNDED, SparkSqlBaseParser.PRECEDING, SparkSqlBaseParser.FOLLOWING, SparkSqlBaseParser.CURRENT, SparkSqlBaseParser.FIRST, SparkSqlBaseParser.AFTER, SparkSqlBaseParser.LAST, SparkSqlBaseParser.ROW, SparkSqlBaseParser.WITH, SparkSqlBaseParser.VALUES, SparkSqlBaseParser.CREATE, SparkSqlBaseParser.TABLE, SparkSqlBaseParser.DIRECTORY, SparkSqlBaseParser.VIEW, SparkSqlBaseParser.REPLACE, SparkSqlBaseParser.INSERT, SparkSqlBaseParser.DELETE, SparkSqlBaseParser.INTO, SparkSqlBaseParser.DESCRIBE, SparkSqlBaseParser.EXPLAIN, SparkSqlBaseParser.FORMAT, SparkSqlBaseParser.LOGICAL, SparkSqlBaseParser.CODEGEN, SparkSqlBaseParser.COST, SparkSqlBaseParser.CAST, SparkSqlBaseParser.SHOW, SparkSqlBaseParser.TABLES, SparkSqlBaseParser.COLUMNS, SparkSqlBaseParser.COLUMN, SparkSqlBaseParser.USE, SparkSqlBaseParser.PARTITIONS, SparkSqlBaseParser.FUNCTIONS, SparkSqlBaseParser.DROP, SparkSqlBaseParser.UNION, SparkSqlBaseParser.EXCEPT, SparkSqlBaseParser.SETMINUS, SparkSqlBaseParser.INTERSECT, SparkSqlBaseParser.TO, SparkSqlBaseParser.TABLESAMPLE, SparkSqlBaseParser.STRATIFY, SparkSqlBaseParser.ALTER, SparkSqlBaseParser.RENAME, SparkSqlBaseParser.ARRAY, SparkSqlBaseParser.MAP, SparkSqlBaseParser.STRUCT, SparkSqlBaseParser.COMMENT, SparkSqlBaseParser.SET, SparkSqlBaseParser.RESET, SparkSqlBaseParser.DATA, SparkSqlBaseParser.START, SparkSqlBaseParser.TRANSACTION, SparkSqlBaseParser.COMMIT, SparkSqlBaseParser.ROLLBACK, SparkSqlBaseParser.MACRO, SparkSqlBaseParser.IGNORE, SparkSqlBaseParser.BOTH, SparkSqlBaseParser.LEADING, SparkSqlBaseParser.TRAILING, SparkSqlBaseParser.IF, SparkSqlBaseParser.POSITION, SparkSqlBaseParser.EXTRACT, SparkSqlBaseParser.DIV, SparkSqlBaseParser.PERCENTLIT, SparkSqlBaseParser.BUCKET, SparkSqlBaseParser.OUT, SparkSqlBaseParser.OF, SparkSqlBaseParser.SORT, SparkSqlBaseParser.CLUSTER, SparkSqlBaseParser.DISTRIBUTE, SparkSqlBaseParser.OVERWRITE, SparkSqlBaseParser.TRANSFORM, SparkSqlBaseParser.REDUCE, SparkSqlBaseParser.SERDE, SparkSqlBaseParser.SERDEPROPERTIES, SparkSqlBaseParser.RECORDREADER, SparkSqlBaseParser.RECORDWRITER, SparkSqlBaseParser.DELIMITED, SparkSqlBaseParser.FIELDS, SparkSqlBaseParser.TERMINATED, SparkSqlBaseParser.COLLECTION, SparkSqlBaseParser.ITEMS, SparkSqlBaseParser.KEYS, SparkSqlBaseParser.ESCAPED, SparkSqlBaseParser.LINES, SparkSqlBaseParser.SEPARATED, SparkSqlBaseParser.FUNCTION, SparkSqlBaseParser.EXTENDED, SparkSqlBaseParser.REFRESH, SparkSqlBaseParser.CLEAR, SparkSqlBaseParser.CACHE, SparkSqlBaseParser.UNCACHE, SparkSqlBaseParser.LAZY, SparkSqlBaseParser.FORMATTED, SparkSqlBaseParser.GLOBAL, SparkSqlBaseParser.TEMPORARY, SparkSqlBaseParser.OPTIONS, SparkSqlBaseParser.UNSET, SparkSqlBaseParser.TBLPROPERTIES, SparkSqlBaseParser.DBPROPERTIES, SparkSqlBaseParser.BUCKETS, SparkSqlBaseParser.SKEWED, SparkSqlBaseParser.STORED, SparkSqlBaseParser.DIRECTORIES, SparkSqlBaseParser.LOCATION, SparkSqlBaseParser.EXCHANGE, SparkSqlBaseParser.ARCHIVE, SparkSqlBaseParser.UNARCHIVE, SparkSqlBaseParser.FILEFORMAT, SparkSqlBaseParser.TOUCH, SparkSqlBaseParser.COMPACT, SparkSqlBaseParser.CONCATENATE, SparkSqlBaseParser.CHANGE, SparkSqlBaseParser.CASCADE, SparkSqlBaseParser.RESTRICT, SparkSqlBaseParser.CLUSTERED, SparkSqlBaseParser.SORTED, SparkSqlBaseParser.PURGE, SparkSqlBaseParser.INPUTFORMAT, SparkSqlBaseParser.OUTPUTFORMAT, SparkSqlBaseParser.DATABASE, SparkSqlBaseParser.DATABASES, SparkSqlBaseParser.DFS, SparkSqlBaseParser.TRUNCATE, SparkSqlBaseParser.ANALYZE, SparkSqlBaseParser.COMPUTE, SparkSqlBaseParser.LIST, SparkSqlBaseParser.STATISTICS, SparkSqlBaseParser.PARTITIONED, SparkSqlBaseParser.EXTERNAL, SparkSqlBaseParser.DEFINED, SparkSqlBaseParser.REVOKE, SparkSqlBaseParser.GRANT, SparkSqlBaseParser.LOCK, SparkSqlBaseParser.UNLOCK, SparkSqlBaseParser.MSCK, SparkSqlBaseParser.REPAIR, SparkSqlBaseParser.RECOVER, SparkSqlBaseParser.EXPORT, SparkSqlBaseParser.IMPORT, SparkSqlBaseParser.LOAD, SparkSqlBaseParser.ROLE, SparkSqlBaseParser.ROLES, SparkSqlBaseParser.COMPACTIONS, SparkSqlBaseParser.PRINCIPALS, SparkSqlBaseParser.TRANSACTIONS, SparkSqlBaseParser.INDEX, SparkSqlBaseParser.INDEXES, SparkSqlBaseParser.LOCKS, SparkSqlBaseParser.OPTION, SparkSqlBaseParser.ANTI, SparkSqlBaseParser.LOCAL, SparkSqlBaseParser.INPATH, SparkSqlBaseParser.IDENTIFIER, SparkSqlBaseParser.BACKQUOTED_IDENTIFIER]:
                    self.state = 1898
                    self.identifier()
                    pass
                elif token in [SparkSqlBaseParser.T__0]:
                    self.state = 1899
                    self.identifierList()
                    pass
                else:
                    raise NoViableAltException(self)



        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NamedExpressionSeqContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def namedExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.NamedExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.NamedExpressionContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_namedExpressionSeq

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedExpressionSeq" ):
                listener.enterNamedExpressionSeq(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedExpressionSeq" ):
                listener.exitNamedExpressionSeq(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedExpressionSeq" ):
                return visitor.visitNamedExpressionSeq(self)
            else:
                return visitor.visitChildren(self)




    def namedExpressionSeq(self):

        localctx = SparkSqlBaseParser.NamedExpressionSeqContext(self, self._ctx, self.state)
        self.enterRule(localctx, 138, self.RULE_namedExpressionSeq)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1904
            self.namedExpression()
            self.state = 1909
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,255,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 1905
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1906
                    self.namedExpression() 
                self.state = 1911
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,255,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.BooleanExpressionContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExpression" ):
                listener.enterExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExpression" ):
                listener.exitExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExpression" ):
                return visitor.visitExpression(self)
            else:
                return visitor.visitChildren(self)




    def expression(self):

        localctx = SparkSqlBaseParser.ExpressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 140, self.RULE_expression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1912
            self.booleanExpression(0)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class BooleanExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_booleanExpression

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class LogicalNotContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.BooleanExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)
        def booleanExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.BooleanExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLogicalNot" ):
                listener.enterLogicalNot(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLogicalNot" ):
                listener.exitLogicalNot(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLogicalNot" ):
                return visitor.visitLogicalNot(self)
            else:
                return visitor.visitChildren(self)


    class PredicatedContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.BooleanExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,0)

        def predicate(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PredicateContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPredicated" ):
                listener.enterPredicated(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPredicated" ):
                listener.exitPredicated(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPredicated" ):
                return visitor.visitPredicated(self)
            else:
                return visitor.visitChildren(self)


    class ExistsContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.BooleanExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)
        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExists" ):
                listener.enterExists(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExists" ):
                listener.exitExists(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExists" ):
                return visitor.visitExists(self)
            else:
                return visitor.visitChildren(self)


    class LogicalBinaryContext(BooleanExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.BooleanExpressionContext
            super().__init__(parser)
            self.left = None # BooleanExpressionContext
            self.operator = None # Token
            self.right = None # BooleanExpressionContext
            self.copyFrom(ctx)

        def booleanExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.BooleanExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.BooleanExpressionContext,i)

        def AND(self):
            return self.getToken(SparkSqlBaseParser.AND, 0)
        def OR(self):
            return self.getToken(SparkSqlBaseParser.OR, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLogicalBinary" ):
                listener.enterLogicalBinary(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLogicalBinary" ):
                listener.exitLogicalBinary(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLogicalBinary" ):
                return visitor.visitLogicalBinary(self)
            else:
                return visitor.visitChildren(self)



    def booleanExpression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSqlBaseParser.BooleanExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 142
        self.enterRecursionRule(localctx, 142, self.RULE_booleanExpression, _p)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1926
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,257,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.LogicalNotContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx

                self.state = 1915
                self.match(SparkSqlBaseParser.NOT)
                self.state = 1916
                self.booleanExpression(5)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.ExistsContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 1917
                self.match(SparkSqlBaseParser.EXISTS)
                self.state = 1918
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1919
                self.query()
                self.state = 1920
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.PredicatedContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 1922
                self.valueExpression(0)
                self.state = 1924
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,256,self._ctx)
                if la_ == 1:
                    self.state = 1923
                    self.predicate()


                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 1936
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,259,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 1934
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,258,self._ctx)
                    if la_ == 1:
                        localctx = SparkSqlBaseParser.LogicalBinaryContext(self, SparkSqlBaseParser.BooleanExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_booleanExpression)
                        self.state = 1928
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 1929
                        localctx.operator = self.match(SparkSqlBaseParser.AND)
                        self.state = 1930
                        localctx.right = self.booleanExpression(3)
                        pass

                    elif la_ == 2:
                        localctx = SparkSqlBaseParser.LogicalBinaryContext(self, SparkSqlBaseParser.BooleanExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_booleanExpression)
                        self.state = 1931
                        if not self.precpred(self._ctx, 1):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
                        self.state = 1932
                        localctx.operator = self.match(SparkSqlBaseParser.OR)
                        self.state = 1933
                        localctx.right = self.booleanExpression(2)
                        pass

             
                self.state = 1938
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,259,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx

    class PredicateContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.kind = None # Token
            self.lower = None # ValueExpressionContext
            self.upper = None # ValueExpressionContext
            self.pattern = None # ValueExpressionContext
            self.right = None # ValueExpressionContext

        def AND(self):
            return self.getToken(SparkSqlBaseParser.AND, 0)

        def BETWEEN(self):
            return self.getToken(SparkSqlBaseParser.BETWEEN, 0)

        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,i)


        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def IN(self):
            return self.getToken(SparkSqlBaseParser.IN, 0)

        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)


        def RLIKE(self):
            return self.getToken(SparkSqlBaseParser.RLIKE, 0)

        def LIKE(self):
            return self.getToken(SparkSqlBaseParser.LIKE, 0)

        def IS(self):
            return self.getToken(SparkSqlBaseParser.IS, 0)

        def NULL(self):
            return self.getToken(SparkSqlBaseParser.NULL, 0)

        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)

        def DISTINCT(self):
            return self.getToken(SparkSqlBaseParser.DISTINCT, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_predicate

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPredicate" ):
                listener.enterPredicate(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPredicate" ):
                listener.exitPredicate(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPredicate" ):
                return visitor.visitPredicate(self)
            else:
                return visitor.visitChildren(self)




    def predicate(self):

        localctx = SparkSqlBaseParser.PredicateContext(self, self._ctx, self.state)
        self.enterRule(localctx, 144, self.RULE_predicate)
        self._la = 0 # Token type
        try:
            self.state = 1987
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,267,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 1940
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.NOT:
                    self.state = 1939
                    self.match(SparkSqlBaseParser.NOT)


                self.state = 1942
                localctx.kind = self.match(SparkSqlBaseParser.BETWEEN)
                self.state = 1943
                localctx.lower = self.valueExpression(0)
                self.state = 1944
                self.match(SparkSqlBaseParser.AND)
                self.state = 1945
                localctx.upper = self.valueExpression(0)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 1948
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.NOT:
                    self.state = 1947
                    self.match(SparkSqlBaseParser.NOT)


                self.state = 1950
                localctx.kind = self.match(SparkSqlBaseParser.IN)
                self.state = 1951
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1952
                self.expression()
                self.state = 1957
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparkSqlBaseParser.T__2:
                    self.state = 1953
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 1954
                    self.expression()
                    self.state = 1959
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 1960
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 1963
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.NOT:
                    self.state = 1962
                    self.match(SparkSqlBaseParser.NOT)


                self.state = 1965
                localctx.kind = self.match(SparkSqlBaseParser.IN)
                self.state = 1966
                self.match(SparkSqlBaseParser.T__0)
                self.state = 1967
                self.query()
                self.state = 1968
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 1971
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.NOT:
                    self.state = 1970
                    self.match(SparkSqlBaseParser.NOT)


                self.state = 1973
                localctx.kind = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.LIKE or _la==SparkSqlBaseParser.RLIKE):
                    localctx.kind = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 1974
                localctx.pattern = self.valueExpression(0)
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 1975
                self.match(SparkSqlBaseParser.IS)
                self.state = 1977
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.NOT:
                    self.state = 1976
                    self.match(SparkSqlBaseParser.NOT)


                self.state = 1979
                localctx.kind = self.match(SparkSqlBaseParser.NULL)
                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 1980
                self.match(SparkSqlBaseParser.IS)
                self.state = 1982
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.NOT:
                    self.state = 1981
                    self.match(SparkSqlBaseParser.NOT)


                self.state = 1984
                localctx.kind = self.match(SparkSqlBaseParser.DISTINCT)
                self.state = 1985
                self.match(SparkSqlBaseParser.FROM)
                self.state = 1986
                localctx.right = self.valueExpression(0)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ValueExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_valueExpression

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class ValueExpressionDefaultContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ValueExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def primaryExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PrimaryExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterValueExpressionDefault" ):
                listener.enterValueExpressionDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitValueExpressionDefault" ):
                listener.exitValueExpressionDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitValueExpressionDefault" ):
                return visitor.visitValueExpressionDefault(self)
            else:
                return visitor.visitChildren(self)


    class ComparisonContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ValueExpressionContext
            super().__init__(parser)
            self.left = None # ValueExpressionContext
            self.right = None # ValueExpressionContext
            self.copyFrom(ctx)

        def comparisonOperator(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ComparisonOperatorContext,0)

        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComparison" ):
                listener.enterComparison(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComparison" ):
                listener.exitComparison(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComparison" ):
                return visitor.visitComparison(self)
            else:
                return visitor.visitChildren(self)


    class ArithmeticBinaryContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ValueExpressionContext
            super().__init__(parser)
            self.left = None # ValueExpressionContext
            self.operator = None # Token
            self.right = None # ValueExpressionContext
            self.copyFrom(ctx)

        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,i)

        def ASTERISK(self):
            return self.getToken(SparkSqlBaseParser.ASTERISK, 0)
        def SLASH(self):
            return self.getToken(SparkSqlBaseParser.SLASH, 0)
        def PERCENT(self):
            return self.getToken(SparkSqlBaseParser.PERCENT, 0)
        def DIV(self):
            return self.getToken(SparkSqlBaseParser.DIV, 0)
        def PLUS(self):
            return self.getToken(SparkSqlBaseParser.PLUS, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)
        def CONCAT_PIPE(self):
            return self.getToken(SparkSqlBaseParser.CONCAT_PIPE, 0)
        def AMPERSAND(self):
            return self.getToken(SparkSqlBaseParser.AMPERSAND, 0)
        def HAT(self):
            return self.getToken(SparkSqlBaseParser.HAT, 0)
        def PIPE(self):
            return self.getToken(SparkSqlBaseParser.PIPE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterArithmeticBinary" ):
                listener.enterArithmeticBinary(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitArithmeticBinary" ):
                listener.exitArithmeticBinary(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitArithmeticBinary" ):
                return visitor.visitArithmeticBinary(self)
            else:
                return visitor.visitChildren(self)


    class ArithmeticUnaryContext(ValueExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ValueExpressionContext
            super().__init__(parser)
            self.operator = None # Token
            self.copyFrom(ctx)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,0)

        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)
        def PLUS(self):
            return self.getToken(SparkSqlBaseParser.PLUS, 0)
        def TILDE(self):
            return self.getToken(SparkSqlBaseParser.TILDE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterArithmeticUnary" ):
                listener.enterArithmeticUnary(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitArithmeticUnary" ):
                listener.exitArithmeticUnary(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitArithmeticUnary" ):
                return visitor.visitArithmeticUnary(self)
            else:
                return visitor.visitChildren(self)



    def valueExpression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSqlBaseParser.ValueExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 146
        self.enterRecursionRule(localctx, 146, self.RULE_valueExpression, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 1993
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,268,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.ValueExpressionDefaultContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx

                self.state = 1990
                self.primaryExpression(0)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.ArithmeticUnaryContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 1991
                localctx.operator = self._input.LT(1)
                _la = self._input.LA(1)
                if not(((((_la - 138)) & ~0x3f) == 0 and ((1 << (_la - 138)) & ((1 << (SparkSqlBaseParser.PLUS - 138)) | (1 << (SparkSqlBaseParser.MINUS - 138)) | (1 << (SparkSqlBaseParser.TILDE - 138)))) != 0)):
                    localctx.operator = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 1992
                self.valueExpression(7)
                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 2016
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,270,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 2014
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,269,self._ctx)
                    if la_ == 1:
                        localctx = SparkSqlBaseParser.ArithmeticBinaryContext(self, SparkSqlBaseParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 1995
                        if not self.precpred(self._ctx, 6):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 6)")
                        self.state = 1996
                        localctx.operator = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 140)) & ~0x3f) == 0 and ((1 << (_la - 140)) & ((1 << (SparkSqlBaseParser.ASTERISK - 140)) | (1 << (SparkSqlBaseParser.SLASH - 140)) | (1 << (SparkSqlBaseParser.PERCENT - 140)) | (1 << (SparkSqlBaseParser.DIV - 140)))) != 0)):
                            localctx.operator = self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 1997
                        localctx.right = self.valueExpression(7)
                        pass

                    elif la_ == 2:
                        localctx = SparkSqlBaseParser.ArithmeticBinaryContext(self, SparkSqlBaseParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 1998
                        if not self.precpred(self._ctx, 5):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 5)")
                        self.state = 1999
                        localctx.operator = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 138)) & ~0x3f) == 0 and ((1 << (_la - 138)) & ((1 << (SparkSqlBaseParser.PLUS - 138)) | (1 << (SparkSqlBaseParser.MINUS - 138)) | (1 << (SparkSqlBaseParser.CONCAT_PIPE - 138)))) != 0)):
                            localctx.operator = self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 2000
                        localctx.right = self.valueExpression(6)
                        pass

                    elif la_ == 3:
                        localctx = SparkSqlBaseParser.ArithmeticBinaryContext(self, SparkSqlBaseParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2001
                        if not self.precpred(self._ctx, 4):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 4)")
                        self.state = 2002
                        localctx.operator = self.match(SparkSqlBaseParser.AMPERSAND)
                        self.state = 2003
                        localctx.right = self.valueExpression(5)
                        pass

                    elif la_ == 4:
                        localctx = SparkSqlBaseParser.ArithmeticBinaryContext(self, SparkSqlBaseParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2004
                        if not self.precpred(self._ctx, 3):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 3)")
                        self.state = 2005
                        localctx.operator = self.match(SparkSqlBaseParser.HAT)
                        self.state = 2006
                        localctx.right = self.valueExpression(4)
                        pass

                    elif la_ == 5:
                        localctx = SparkSqlBaseParser.ArithmeticBinaryContext(self, SparkSqlBaseParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2007
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 2008
                        localctx.operator = self.match(SparkSqlBaseParser.PIPE)
                        self.state = 2009
                        localctx.right = self.valueExpression(3)
                        pass

                    elif la_ == 6:
                        localctx = SparkSqlBaseParser.ComparisonContext(self, SparkSqlBaseParser.ValueExpressionContext(self, _parentctx, _parentState))
                        localctx.left = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_valueExpression)
                        self.state = 2010
                        if not self.precpred(self._ctx, 1):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
                        self.state = 2011
                        self.comparisonOperator()
                        self.state = 2012
                        localctx.right = self.valueExpression(2)
                        pass

             
                self.state = 2018
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,270,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx

    class PrimaryExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_primaryExpression

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)


    class StructContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self._namedExpression = None # NamedExpressionContext
            self.argument = list() # of NamedExpressionContexts
            self.copyFrom(ctx)

        def STRUCT(self):
            return self.getToken(SparkSqlBaseParser.STRUCT, 0)
        def namedExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.NamedExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.NamedExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStruct" ):
                listener.enterStruct(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStruct" ):
                listener.exitStruct(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStruct" ):
                return visitor.visitStruct(self)
            else:
                return visitor.visitChildren(self)


    class DereferenceContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.base = None # PrimaryExpressionContext
            self.fieldName = None # IdentifierContext
            self.copyFrom(ctx)

        def primaryExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PrimaryExpressionContext,0)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDereference" ):
                listener.enterDereference(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDereference" ):
                listener.exitDereference(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDereference" ):
                return visitor.visitDereference(self)
            else:
                return visitor.visitChildren(self)


    class SimpleCaseContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.value = None # ExpressionContext
            self.elseExpression = None # ExpressionContext
            self.copyFrom(ctx)

        def CASE(self):
            return self.getToken(SparkSqlBaseParser.CASE, 0)
        def END(self):
            return self.getToken(SparkSqlBaseParser.END, 0)
        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)

        def whenClause(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.WhenClauseContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.WhenClauseContext,i)

        def ELSE(self):
            return self.getToken(SparkSqlBaseParser.ELSE, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSimpleCase" ):
                listener.enterSimpleCase(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSimpleCase" ):
                listener.exitSimpleCase(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSimpleCase" ):
                return visitor.visitSimpleCase(self)
            else:
                return visitor.visitChildren(self)


    class ColumnReferenceContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumnReference" ):
                listener.enterColumnReference(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumnReference" ):
                listener.exitColumnReference(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumnReference" ):
                return visitor.visitColumnReference(self)
            else:
                return visitor.visitChildren(self)


    class RowConstructorContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def namedExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.NamedExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.NamedExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRowConstructor" ):
                listener.enterRowConstructor(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRowConstructor" ):
                listener.exitRowConstructor(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRowConstructor" ):
                return visitor.visitRowConstructor(self)
            else:
                return visitor.visitChildren(self)


    class LastContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def LAST(self):
            return self.getToken(SparkSqlBaseParser.LAST, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)

        def IGNORE(self):
            return self.getToken(SparkSqlBaseParser.IGNORE, 0)
        def NULLS(self):
            return self.getToken(SparkSqlBaseParser.NULLS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLast" ):
                listener.enterLast(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLast" ):
                listener.exitLast(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLast" ):
                return visitor.visitLast(self)
            else:
                return visitor.visitChildren(self)


    class StarContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def ASTERISK(self):
            return self.getToken(SparkSqlBaseParser.ASTERISK, 0)
        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStar" ):
                listener.enterStar(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStar" ):
                listener.exitStar(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStar" ):
                return visitor.visitStar(self)
            else:
                return visitor.visitChildren(self)


    class SubscriptContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.value = None # PrimaryExpressionContext
            self.index = None # ValueExpressionContext
            self.copyFrom(ctx)

        def primaryExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.PrimaryExpressionContext,0)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubscript" ):
                listener.enterSubscript(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubscript" ):
                listener.exitSubscript(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubscript" ):
                return visitor.visitSubscript(self)
            else:
                return visitor.visitChildren(self)


    class SubqueryExpressionContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def query(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QueryContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubqueryExpression" ):
                listener.enterSubqueryExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubqueryExpression" ):
                listener.exitSubqueryExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubqueryExpression" ):
                return visitor.visitSubqueryExpression(self)
            else:
                return visitor.visitChildren(self)


    class CastContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def CAST(self):
            return self.getToken(SparkSqlBaseParser.CAST, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)
        def dataType(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.DataTypeContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCast" ):
                listener.enterCast(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCast" ):
                listener.exitCast(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCast" ):
                return visitor.visitCast(self)
            else:
                return visitor.visitChildren(self)


    class ConstantDefaultContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def constant(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ConstantContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterConstantDefault" ):
                listener.enterConstantDefault(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitConstantDefault" ):
                listener.exitConstantDefault(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitConstantDefault" ):
                return visitor.visitConstantDefault(self)
            else:
                return visitor.visitChildren(self)


    class LambdaContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def IDENTIFIER(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.IDENTIFIER)
            else:
                return self.getToken(SparkSqlBaseParser.IDENTIFIER, i)
        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLambda" ):
                listener.enterLambda(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLambda" ):
                listener.exitLambda(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLambda" ):
                return visitor.visitLambda(self)
            else:
                return visitor.visitChildren(self)


    class ParenthesizedExpressionContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterParenthesizedExpression" ):
                listener.enterParenthesizedExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitParenthesizedExpression" ):
                listener.exitParenthesizedExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitParenthesizedExpression" ):
                return visitor.visitParenthesizedExpression(self)
            else:
                return visitor.visitChildren(self)


    class ExtractContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.field = None # IdentifierContext
            self.source = None # ValueExpressionContext
            self.copyFrom(ctx)

        def EXTRACT(self):
            return self.getToken(SparkSqlBaseParser.EXTRACT, 0)
        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)
        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def valueExpression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExtract" ):
                listener.enterExtract(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExtract" ):
                listener.exitExtract(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExtract" ):
                return visitor.visitExtract(self)
            else:
                return visitor.visitChildren(self)


    class FunctionCallContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self._expression = None # ExpressionContext
            self.argument = list() # of ExpressionContexts
            self.trimOption = None # Token
            self.copyFrom(ctx)

        def qualifiedName(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QualifiedNameContext,0)

        def OVER(self):
            return self.getToken(SparkSqlBaseParser.OVER, 0)
        def windowSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.WindowSpecContext,0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)

        def setQuantifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.SetQuantifierContext,0)

        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)
        def BOTH(self):
            return self.getToken(SparkSqlBaseParser.BOTH, 0)
        def LEADING(self):
            return self.getToken(SparkSqlBaseParser.LEADING, 0)
        def TRAILING(self):
            return self.getToken(SparkSqlBaseParser.TRAILING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFunctionCall" ):
                listener.enterFunctionCall(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFunctionCall" ):
                listener.exitFunctionCall(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFunctionCall" ):
                return visitor.visitFunctionCall(self)
            else:
                return visitor.visitChildren(self)


    class SearchedCaseContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.elseExpression = None # ExpressionContext
            self.copyFrom(ctx)

        def CASE(self):
            return self.getToken(SparkSqlBaseParser.CASE, 0)
        def END(self):
            return self.getToken(SparkSqlBaseParser.END, 0)
        def whenClause(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.WhenClauseContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.WhenClauseContext,i)

        def ELSE(self):
            return self.getToken(SparkSqlBaseParser.ELSE, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearchedCase" ):
                listener.enterSearchedCase(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearchedCase" ):
                listener.exitSearchedCase(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearchedCase" ):
                return visitor.visitSearchedCase(self)
            else:
                return visitor.visitChildren(self)


    class PositionContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.substr = None # ValueExpressionContext
            self.str_ = None # ValueExpressionContext
            self.copyFrom(ctx)

        def POSITION(self):
            return self.getToken(SparkSqlBaseParser.POSITION, 0)
        def IN(self):
            return self.getToken(SparkSqlBaseParser.IN, 0)
        def valueExpression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ValueExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ValueExpressionContext,i)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPosition" ):
                listener.enterPosition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPosition" ):
                listener.exitPosition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPosition" ):
                return visitor.visitPosition(self)
            else:
                return visitor.visitChildren(self)


    class FirstContext(PrimaryExpressionContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.PrimaryExpressionContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def FIRST(self):
            return self.getToken(SparkSqlBaseParser.FIRST, 0)
        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)

        def IGNORE(self):
            return self.getToken(SparkSqlBaseParser.IGNORE, 0)
        def NULLS(self):
            return self.getToken(SparkSqlBaseParser.NULLS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFirst" ):
                listener.enterFirst(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFirst" ):
                listener.exitFirst(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFirst" ):
                return visitor.visitFirst(self)
            else:
                return visitor.visitChildren(self)



    def primaryExpression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparkSqlBaseParser.PrimaryExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 148
        self.enterRecursionRule(localctx, 148, self.RULE_primaryExpression, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2164
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,285,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.SearchedCaseContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx

                self.state = 2020
                self.match(SparkSqlBaseParser.CASE)
                self.state = 2022 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2021
                    self.whenClause()
                    self.state = 2024 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSqlBaseParser.WHEN):
                        break

                self.state = 2028
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.ELSE:
                    self.state = 2026
                    self.match(SparkSqlBaseParser.ELSE)
                    self.state = 2027
                    localctx.elseExpression = self.expression()


                self.state = 2030
                self.match(SparkSqlBaseParser.END)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.SimpleCaseContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2032
                self.match(SparkSqlBaseParser.CASE)
                self.state = 2033
                localctx.value = self.expression()
                self.state = 2035 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2034
                    self.whenClause()
                    self.state = 2037 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSqlBaseParser.WHEN):
                        break

                self.state = 2041
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.ELSE:
                    self.state = 2039
                    self.match(SparkSqlBaseParser.ELSE)
                    self.state = 2040
                    localctx.elseExpression = self.expression()


                self.state = 2043
                self.match(SparkSqlBaseParser.END)
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.CastContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2045
                self.match(SparkSqlBaseParser.CAST)
                self.state = 2046
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2047
                self.expression()
                self.state = 2048
                self.match(SparkSqlBaseParser.AS)
                self.state = 2049
                self.dataType()
                self.state = 2050
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.StructContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2052
                self.match(SparkSqlBaseParser.STRUCT)
                self.state = 2053
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2062
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.PLUS - 128)) | (1 << (SparkSqlBaseParser.MINUS - 128)) | (1 << (SparkSqlBaseParser.ASTERISK - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.TILDE - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.STRING - 192)) | (1 << (SparkSqlBaseParser.BIGINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.SMALLINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.TINYINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.INTEGER_VALUE - 192)) | (1 << (SparkSqlBaseParser.DECIMAL_VALUE - 192)) | (1 << (SparkSqlBaseParser.DOUBLE_LITERAL - 192)) | (1 << (SparkSqlBaseParser.BIGDECIMAL_LITERAL - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                    self.state = 2054
                    localctx._namedExpression = self.namedExpression()
                    localctx.argument.append(localctx._namedExpression)
                    self.state = 2059
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__2:
                        self.state = 2055
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 2056
                        localctx._namedExpression = self.namedExpression()
                        localctx.argument.append(localctx._namedExpression)
                        self.state = 2061
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                self.state = 2064
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 5:
                localctx = SparkSqlBaseParser.FirstContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2065
                self.match(SparkSqlBaseParser.FIRST)
                self.state = 2066
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2067
                self.expression()
                self.state = 2070
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.IGNORE:
                    self.state = 2068
                    self.match(SparkSqlBaseParser.IGNORE)
                    self.state = 2069
                    self.match(SparkSqlBaseParser.NULLS)


                self.state = 2072
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 6:
                localctx = SparkSqlBaseParser.LastContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2074
                self.match(SparkSqlBaseParser.LAST)
                self.state = 2075
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2076
                self.expression()
                self.state = 2079
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.IGNORE:
                    self.state = 2077
                    self.match(SparkSqlBaseParser.IGNORE)
                    self.state = 2078
                    self.match(SparkSqlBaseParser.NULLS)


                self.state = 2081
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 7:
                localctx = SparkSqlBaseParser.PositionContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2083
                self.match(SparkSqlBaseParser.POSITION)
                self.state = 2084
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2085
                localctx.substr = self.valueExpression(0)
                self.state = 2086
                self.match(SparkSqlBaseParser.IN)
                self.state = 2087
                localctx.str_ = self.valueExpression(0)
                self.state = 2088
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 8:
                localctx = SparkSqlBaseParser.ConstantDefaultContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2090
                self.constant()
                pass

            elif la_ == 9:
                localctx = SparkSqlBaseParser.StarContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2091
                self.match(SparkSqlBaseParser.ASTERISK)
                pass

            elif la_ == 10:
                localctx = SparkSqlBaseParser.StarContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2092
                self.qualifiedName()
                self.state = 2093
                self.match(SparkSqlBaseParser.T__3)
                self.state = 2094
                self.match(SparkSqlBaseParser.ASTERISK)
                pass

            elif la_ == 11:
                localctx = SparkSqlBaseParser.RowConstructorContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2096
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2097
                self.namedExpression()
                self.state = 2100 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2098
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 2099
                    self.namedExpression()
                    self.state = 2102 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSqlBaseParser.T__2):
                        break

                self.state = 2104
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 12:
                localctx = SparkSqlBaseParser.SubqueryExpressionContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2106
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2107
                self.query()
                self.state = 2108
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 13:
                localctx = SparkSqlBaseParser.FunctionCallContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2110
                self.qualifiedName()
                self.state = 2111
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2123
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.T__0) | (1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.PLUS - 128)) | (1 << (SparkSqlBaseParser.MINUS - 128)) | (1 << (SparkSqlBaseParser.ASTERISK - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.TILDE - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.STRING - 192)) | (1 << (SparkSqlBaseParser.BIGINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.SMALLINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.TINYINT_LITERAL - 192)) | (1 << (SparkSqlBaseParser.INTEGER_VALUE - 192)) | (1 << (SparkSqlBaseParser.DECIMAL_VALUE - 192)) | (1 << (SparkSqlBaseParser.DOUBLE_LITERAL - 192)) | (1 << (SparkSqlBaseParser.BIGDECIMAL_LITERAL - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                    self.state = 2113
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,280,self._ctx)
                    if la_ == 1:
                        self.state = 2112
                        self.setQuantifier()


                    self.state = 2115
                    localctx._expression = self.expression()
                    localctx.argument.append(localctx._expression)
                    self.state = 2120
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__2:
                        self.state = 2116
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 2117
                        localctx._expression = self.expression()
                        localctx.argument.append(localctx._expression)
                        self.state = 2122
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)



                self.state = 2125
                self.match(SparkSqlBaseParser.T__1)
                self.state = 2128
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,283,self._ctx)
                if la_ == 1:
                    self.state = 2126
                    self.match(SparkSqlBaseParser.OVER)
                    self.state = 2127
                    self.windowSpec()


                pass

            elif la_ == 14:
                localctx = SparkSqlBaseParser.FunctionCallContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2130
                self.qualifiedName()
                self.state = 2131
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2132
                localctx.trimOption = self._input.LT(1)
                _la = self._input.LA(1)
                if not(((((_la - 124)) & ~0x3f) == 0 and ((1 << (_la - 124)) & ((1 << (SparkSqlBaseParser.BOTH - 124)) | (1 << (SparkSqlBaseParser.LEADING - 124)) | (1 << (SparkSqlBaseParser.TRAILING - 124)))) != 0)):
                    localctx.trimOption = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                self.state = 2133
                localctx._expression = self.expression()
                localctx.argument.append(localctx._expression)
                self.state = 2134
                self.match(SparkSqlBaseParser.FROM)
                self.state = 2135
                localctx._expression = self.expression()
                localctx.argument.append(localctx._expression)
                self.state = 2136
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 15:
                localctx = SparkSqlBaseParser.LambdaContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2138
                self.match(SparkSqlBaseParser.IDENTIFIER)
                self.state = 2139
                self.match(SparkSqlBaseParser.T__6)
                self.state = 2140
                self.expression()
                pass

            elif la_ == 16:
                localctx = SparkSqlBaseParser.LambdaContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2141
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2142
                self.match(SparkSqlBaseParser.IDENTIFIER)
                self.state = 2145 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 2143
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 2144
                    self.match(SparkSqlBaseParser.IDENTIFIER)
                    self.state = 2147 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparkSqlBaseParser.T__2):
                        break

                self.state = 2149
                self.match(SparkSqlBaseParser.T__1)
                self.state = 2150
                self.match(SparkSqlBaseParser.T__6)
                self.state = 2151
                self.expression()
                pass

            elif la_ == 17:
                localctx = SparkSqlBaseParser.ColumnReferenceContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2152
                self.identifier()
                pass

            elif la_ == 18:
                localctx = SparkSqlBaseParser.ParenthesizedExpressionContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2153
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2154
                self.expression()
                self.state = 2155
                self.match(SparkSqlBaseParser.T__1)
                pass

            elif la_ == 19:
                localctx = SparkSqlBaseParser.ExtractContext(self, localctx)
                self._ctx = localctx
                _prevctx = localctx
                self.state = 2157
                self.match(SparkSqlBaseParser.EXTRACT)
                self.state = 2158
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2159
                localctx.field = self.identifier()
                self.state = 2160
                self.match(SparkSqlBaseParser.FROM)
                self.state = 2161
                localctx.source = self.valueExpression(0)
                self.state = 2162
                self.match(SparkSqlBaseParser.T__1)
                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 2176
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,287,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 2174
                    self._errHandler.sync(self)
                    la_ = self._interp.adaptivePredict(self._input,286,self._ctx)
                    if la_ == 1:
                        localctx = SparkSqlBaseParser.SubscriptContext(self, SparkSqlBaseParser.PrimaryExpressionContext(self, _parentctx, _parentState))
                        localctx.value = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_primaryExpression)
                        self.state = 2166
                        if not self.precpred(self._ctx, 5):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 5)")
                        self.state = 2167
                        self.match(SparkSqlBaseParser.T__7)
                        self.state = 2168
                        localctx.index = self.valueExpression(0)
                        self.state = 2169
                        self.match(SparkSqlBaseParser.T__8)
                        pass

                    elif la_ == 2:
                        localctx = SparkSqlBaseParser.DereferenceContext(self, SparkSqlBaseParser.PrimaryExpressionContext(self, _parentctx, _parentState))
                        localctx.base = _prevctx
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_primaryExpression)
                        self.state = 2171
                        if not self.precpred(self._ctx, 3):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 3)")
                        self.state = 2172
                        self.match(SparkSqlBaseParser.T__3)
                        self.state = 2173
                        localctx.fieldName = self.identifier()
                        pass

             
                self.state = 2178
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,287,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx

    class ConstantContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_constant

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class NullLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def NULL(self):
            return self.getToken(SparkSqlBaseParser.NULL, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNullLiteral" ):
                listener.enterNullLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNullLiteral" ):
                listener.exitNullLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNullLiteral" ):
                return visitor.visitNullLiteral(self)
            else:
                return visitor.visitChildren(self)


    class StringLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def STRING(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.STRING)
            else:
                return self.getToken(SparkSqlBaseParser.STRING, i)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterStringLiteral" ):
                listener.enterStringLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitStringLiteral" ):
                listener.exitStringLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitStringLiteral" ):
                return visitor.visitStringLiteral(self)
            else:
                return visitor.visitChildren(self)


    class TypeConstructorContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTypeConstructor" ):
                listener.enterTypeConstructor(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTypeConstructor" ):
                listener.exitTypeConstructor(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTypeConstructor" ):
                return visitor.visitTypeConstructor(self)
            else:
                return visitor.visitChildren(self)


    class IntervalLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def interval(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IntervalContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIntervalLiteral" ):
                listener.enterIntervalLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIntervalLiteral" ):
                listener.exitIntervalLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIntervalLiteral" ):
                return visitor.visitIntervalLiteral(self)
            else:
                return visitor.visitChildren(self)


    class NumericLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def number(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.NumberContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNumericLiteral" ):
                listener.enterNumericLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNumericLiteral" ):
                listener.exitNumericLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNumericLiteral" ):
                return visitor.visitNumericLiteral(self)
            else:
                return visitor.visitChildren(self)


    class BooleanLiteralContext(ConstantContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.ConstantContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def booleanValue(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.BooleanValueContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBooleanLiteral" ):
                listener.enterBooleanLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBooleanLiteral" ):
                listener.exitBooleanLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBooleanLiteral" ):
                return visitor.visitBooleanLiteral(self)
            else:
                return visitor.visitChildren(self)



    def constant(self):

        localctx = SparkSqlBaseParser.ConstantContext(self, self._ctx, self.state)
        self.enterRule(localctx, 150, self.RULE_constant)
        try:
            self.state = 2191
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,289,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.NullLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2179
                self.match(SparkSqlBaseParser.NULL)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.IntervalLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2180
                self.interval()
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.TypeConstructorContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2181
                self.identifier()
                self.state = 2182
                self.match(SparkSqlBaseParser.STRING)
                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.NumericLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2184
                self.number()
                pass

            elif la_ == 5:
                localctx = SparkSqlBaseParser.BooleanLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 2185
                self.booleanValue()
                pass

            elif la_ == 6:
                localctx = SparkSqlBaseParser.StringLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 6)
                self.state = 2187 
                self._errHandler.sync(self)
                _alt = 1
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt == 1:
                        self.state = 2186
                        self.match(SparkSqlBaseParser.STRING)

                    else:
                        raise NoViableAltException(self)
                    self.state = 2189 
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,288,self._ctx)

                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ComparisonOperatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def EQ(self):
            return self.getToken(SparkSqlBaseParser.EQ, 0)

        def NEQ(self):
            return self.getToken(SparkSqlBaseParser.NEQ, 0)

        def NEQJ(self):
            return self.getToken(SparkSqlBaseParser.NEQJ, 0)

        def LT(self):
            return self.getToken(SparkSqlBaseParser.LT, 0)

        def LTE(self):
            return self.getToken(SparkSqlBaseParser.LTE, 0)

        def GT(self):
            return self.getToken(SparkSqlBaseParser.GT, 0)

        def GTE(self):
            return self.getToken(SparkSqlBaseParser.GTE, 0)

        def NSEQ(self):
            return self.getToken(SparkSqlBaseParser.NSEQ, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_comparisonOperator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComparisonOperator" ):
                listener.enterComparisonOperator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComparisonOperator" ):
                listener.exitComparisonOperator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComparisonOperator" ):
                return visitor.visitComparisonOperator(self)
            else:
                return visitor.visitChildren(self)




    def comparisonOperator(self):

        localctx = SparkSqlBaseParser.ComparisonOperatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 152, self.RULE_comparisonOperator)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2193
            _la = self._input.LA(1)
            if not(((((_la - 130)) & ~0x3f) == 0 and ((1 << (_la - 130)) & ((1 << (SparkSqlBaseParser.EQ - 130)) | (1 << (SparkSqlBaseParser.NSEQ - 130)) | (1 << (SparkSqlBaseParser.NEQ - 130)) | (1 << (SparkSqlBaseParser.NEQJ - 130)) | (1 << (SparkSqlBaseParser.LT - 130)) | (1 << (SparkSqlBaseParser.LTE - 130)) | (1 << (SparkSqlBaseParser.GT - 130)) | (1 << (SparkSqlBaseParser.GTE - 130)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ArithmeticOperatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def PLUS(self):
            return self.getToken(SparkSqlBaseParser.PLUS, 0)

        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def ASTERISK(self):
            return self.getToken(SparkSqlBaseParser.ASTERISK, 0)

        def SLASH(self):
            return self.getToken(SparkSqlBaseParser.SLASH, 0)

        def PERCENT(self):
            return self.getToken(SparkSqlBaseParser.PERCENT, 0)

        def DIV(self):
            return self.getToken(SparkSqlBaseParser.DIV, 0)

        def TILDE(self):
            return self.getToken(SparkSqlBaseParser.TILDE, 0)

        def AMPERSAND(self):
            return self.getToken(SparkSqlBaseParser.AMPERSAND, 0)

        def PIPE(self):
            return self.getToken(SparkSqlBaseParser.PIPE, 0)

        def CONCAT_PIPE(self):
            return self.getToken(SparkSqlBaseParser.CONCAT_PIPE, 0)

        def HAT(self):
            return self.getToken(SparkSqlBaseParser.HAT, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_arithmeticOperator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterArithmeticOperator" ):
                listener.enterArithmeticOperator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitArithmeticOperator" ):
                listener.exitArithmeticOperator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitArithmeticOperator" ):
                return visitor.visitArithmeticOperator(self)
            else:
                return visitor.visitChildren(self)




    def arithmeticOperator(self):

        localctx = SparkSqlBaseParser.ArithmeticOperatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 154, self.RULE_arithmeticOperator)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2195
            _la = self._input.LA(1)
            if not(((((_la - 138)) & ~0x3f) == 0 and ((1 << (_la - 138)) & ((1 << (SparkSqlBaseParser.PLUS - 138)) | (1 << (SparkSqlBaseParser.MINUS - 138)) | (1 << (SparkSqlBaseParser.ASTERISK - 138)) | (1 << (SparkSqlBaseParser.SLASH - 138)) | (1 << (SparkSqlBaseParser.PERCENT - 138)) | (1 << (SparkSqlBaseParser.DIV - 138)) | (1 << (SparkSqlBaseParser.TILDE - 138)) | (1 << (SparkSqlBaseParser.AMPERSAND - 138)) | (1 << (SparkSqlBaseParser.PIPE - 138)) | (1 << (SparkSqlBaseParser.CONCAT_PIPE - 138)) | (1 << (SparkSqlBaseParser.HAT - 138)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PredicateOperatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def OR(self):
            return self.getToken(SparkSqlBaseParser.OR, 0)

        def AND(self):
            return self.getToken(SparkSqlBaseParser.AND, 0)

        def IN(self):
            return self.getToken(SparkSqlBaseParser.IN, 0)

        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_predicateOperator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPredicateOperator" ):
                listener.enterPredicateOperator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPredicateOperator" ):
                listener.exitPredicateOperator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPredicateOperator" ):
                return visitor.visitPredicateOperator(self)
            else:
                return visitor.visitChildren(self)




    def predicateOperator(self):

        localctx = SparkSqlBaseParser.PredicateOperatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 156, self.RULE_predicateOperator)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2197
            _la = self._input.LA(1)
            if not((((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class BooleanValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def TRUE(self):
            return self.getToken(SparkSqlBaseParser.TRUE, 0)

        def FALSE(self):
            return self.getToken(SparkSqlBaseParser.FALSE, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_booleanValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBooleanValue" ):
                listener.enterBooleanValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBooleanValue" ):
                listener.exitBooleanValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBooleanValue" ):
                return visitor.visitBooleanValue(self)
            else:
                return visitor.visitChildren(self)




    def booleanValue(self):

        localctx = SparkSqlBaseParser.BooleanValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 158, self.RULE_booleanValue)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2199
            _la = self._input.LA(1)
            if not(_la==SparkSqlBaseParser.TRUE or _la==SparkSqlBaseParser.FALSE):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IntervalContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INTERVAL(self):
            return self.getToken(SparkSqlBaseParser.INTERVAL, 0)

        def intervalField(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IntervalFieldContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IntervalFieldContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_interval

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterInterval" ):
                listener.enterInterval(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitInterval" ):
                listener.exitInterval(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitInterval" ):
                return visitor.visitInterval(self)
            else:
                return visitor.visitChildren(self)




    def interval(self):

        localctx = SparkSqlBaseParser.IntervalContext(self, self._ctx, self.state)
        self.enterRule(localctx, 160, self.RULE_interval)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2201
            self.match(SparkSqlBaseParser.INTERVAL)
            self.state = 2205
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,290,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2202
                    self.intervalField() 
                self.state = 2207
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,290,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IntervalFieldContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.value = None # IntervalValueContext
            self.unit = None # IdentifierContext
            self.to = None # IdentifierContext

        def intervalValue(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IntervalValueContext,0)


        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def TO(self):
            return self.getToken(SparkSqlBaseParser.TO, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_intervalField

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIntervalField" ):
                listener.enterIntervalField(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIntervalField" ):
                listener.exitIntervalField(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIntervalField" ):
                return visitor.visitIntervalField(self)
            else:
                return visitor.visitChildren(self)




    def intervalField(self):

        localctx = SparkSqlBaseParser.IntervalFieldContext(self, self._ctx, self.state)
        self.enterRule(localctx, 162, self.RULE_intervalField)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2208
            localctx.value = self.intervalValue()
            self.state = 2209
            localctx.unit = self.identifier()
            self.state = 2212
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,291,self._ctx)
            if la_ == 1:
                self.state = 2210
                self.match(SparkSqlBaseParser.TO)
                self.state = 2211
                localctx.to = self.identifier()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IntervalValueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INTEGER_VALUE(self):
            return self.getToken(SparkSqlBaseParser.INTEGER_VALUE, 0)

        def DECIMAL_VALUE(self):
            return self.getToken(SparkSqlBaseParser.DECIMAL_VALUE, 0)

        def PLUS(self):
            return self.getToken(SparkSqlBaseParser.PLUS, 0)

        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_intervalValue

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIntervalValue" ):
                listener.enterIntervalValue(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIntervalValue" ):
                listener.exitIntervalValue(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIntervalValue" ):
                return visitor.visitIntervalValue(self)
            else:
                return visitor.visitChildren(self)




    def intervalValue(self):

        localctx = SparkSqlBaseParser.IntervalValueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 164, self.RULE_intervalValue)
        self._la = 0 # Token type
        try:
            self.state = 2219
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.PLUS, SparkSqlBaseParser.MINUS, SparkSqlBaseParser.INTEGER_VALUE, SparkSqlBaseParser.DECIMAL_VALUE]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2215
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.PLUS or _la==SparkSqlBaseParser.MINUS:
                    self.state = 2214
                    _la = self._input.LA(1)
                    if not(_la==SparkSqlBaseParser.PLUS or _la==SparkSqlBaseParser.MINUS):
                        self._errHandler.recoverInline(self)
                    else:
                        self._errHandler.reportMatch(self)
                        self.consume()


                self.state = 2217
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.INTEGER_VALUE or _la==SparkSqlBaseParser.DECIMAL_VALUE):
                    self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass
            elif token in [SparkSqlBaseParser.STRING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2218
                self.match(SparkSqlBaseParser.STRING)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ColPositionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def FIRST(self):
            return self.getToken(SparkSqlBaseParser.FIRST, 0)

        def AFTER(self):
            return self.getToken(SparkSqlBaseParser.AFTER, 0)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_colPosition

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColPosition" ):
                listener.enterColPosition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColPosition" ):
                listener.exitColPosition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColPosition" ):
                return visitor.visitColPosition(self)
            else:
                return visitor.visitChildren(self)




    def colPosition(self):

        localctx = SparkSqlBaseParser.ColPositionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 166, self.RULE_colPosition)
        try:
            self.state = 2224
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.FIRST]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2221
                self.match(SparkSqlBaseParser.FIRST)
                pass
            elif token in [SparkSqlBaseParser.AFTER]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2222
                self.match(SparkSqlBaseParser.AFTER)
                self.state = 2223
                self.identifier()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class DataTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_dataType

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class Complex_DataTypeContext(DataTypeContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.DataTypeContext
            super().__init__(parser)
            self.complex_ = None # Token
            self.copyFrom(ctx)

        def dataType(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.DataTypeContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.DataTypeContext,i)

        def ARRAY(self):
            return self.getToken(SparkSqlBaseParser.ARRAY, 0)
        def MAP(self):
            return self.getToken(SparkSqlBaseParser.MAP, 0)
        def STRUCT(self):
            return self.getToken(SparkSqlBaseParser.STRUCT, 0)
        def NEQ(self):
            return self.getToken(SparkSqlBaseParser.NEQ, 0)
        def complex_ColTypeList(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.Complex_ColTypeListContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComplex_DataType" ):
                listener.enterComplex_DataType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComplex_DataType" ):
                listener.exitComplex_DataType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComplex_DataType" ):
                return visitor.visitComplex_DataType(self)
            else:
                return visitor.visitChildren(self)


    class PrimitiveDataTypeContext(DataTypeContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.DataTypeContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)

        def INTEGER_VALUE(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.INTEGER_VALUE)
            else:
                return self.getToken(SparkSqlBaseParser.INTEGER_VALUE, i)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPrimitiveDataType" ):
                listener.enterPrimitiveDataType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPrimitiveDataType" ):
                listener.exitPrimitiveDataType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPrimitiveDataType" ):
                return visitor.visitPrimitiveDataType(self)
            else:
                return visitor.visitChildren(self)



    def dataType(self):

        localctx = SparkSqlBaseParser.DataTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 168, self.RULE_dataType)
        self._la = 0 # Token type
        try:
            self.state = 2260
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,299,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.Complex_DataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2226
                localctx.complex_ = self.match(SparkSqlBaseParser.ARRAY)
                self.state = 2227
                self.match(SparkSqlBaseParser.LT)
                self.state = 2228
                self.dataType()
                self.state = 2229
                self.match(SparkSqlBaseParser.GT)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.Complex_DataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2231
                localctx.complex_ = self.match(SparkSqlBaseParser.MAP)
                self.state = 2232
                self.match(SparkSqlBaseParser.LT)
                self.state = 2233
                self.dataType()
                self.state = 2234
                self.match(SparkSqlBaseParser.T__2)
                self.state = 2235
                self.dataType()
                self.state = 2236
                self.match(SparkSqlBaseParser.GT)
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.Complex_DataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2238
                localctx.complex_ = self.match(SparkSqlBaseParser.STRUCT)
                self.state = 2245
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSqlBaseParser.LT]:
                    self.state = 2239
                    self.match(SparkSqlBaseParser.LT)
                    self.state = 2241
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if (((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.JOIN) | (1 << SparkSqlBaseParser.CROSS) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.INNER) | (1 << SparkSqlBaseParser.LEFT) | (1 << SparkSqlBaseParser.SEMI) | (1 << SparkSqlBaseParser.RIGHT) | (1 << SparkSqlBaseParser.FULL) | (1 << SparkSqlBaseParser.NATURAL) | (1 << SparkSqlBaseParser.ON) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.UNION - 64)) | (1 << (SparkSqlBaseParser.EXCEPT - 64)) | (1 << (SparkSqlBaseParser.SETMINUS - 64)) | (1 << (SparkSqlBaseParser.INTERSECT - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.ANTI - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)) | (1 << (SparkSqlBaseParser.IDENTIFIER - 192)) | (1 << (SparkSqlBaseParser.BACKQUOTED_IDENTIFIER - 192)))) != 0):
                        self.state = 2240
                        self.complex_ColTypeList()


                    self.state = 2243
                    self.match(SparkSqlBaseParser.GT)
                    pass
                elif token in [SparkSqlBaseParser.NEQ]:
                    self.state = 2244
                    self.match(SparkSqlBaseParser.NEQ)
                    pass
                else:
                    raise NoViableAltException(self)

                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.PrimitiveDataTypeContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2247
                self.identifier()
                self.state = 2258
                self._errHandler.sync(self)
                la_ = self._interp.adaptivePredict(self._input,298,self._ctx)
                if la_ == 1:
                    self.state = 2248
                    self.match(SparkSqlBaseParser.T__0)
                    self.state = 2249
                    self.match(SparkSqlBaseParser.INTEGER_VALUE)
                    self.state = 2254
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__2:
                        self.state = 2250
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 2251
                        self.match(SparkSqlBaseParser.INTEGER_VALUE)
                        self.state = 2256
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    self.state = 2257
                    self.match(SparkSqlBaseParser.T__1)


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ColTypeListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def colType(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ColTypeContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ColTypeContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_colTypeList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColTypeList" ):
                listener.enterColTypeList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColTypeList" ):
                listener.exitColTypeList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColTypeList" ):
                return visitor.visitColTypeList(self)
            else:
                return visitor.visitChildren(self)




    def colTypeList(self):

        localctx = SparkSqlBaseParser.ColTypeListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 170, self.RULE_colTypeList)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2262
            self.colType()
            self.state = 2267
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,300,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2263
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 2264
                    self.colType() 
                self.state = 2269
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,300,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ColTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def dataType(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.DataTypeContext,0)


        def COMMENT(self):
            return self.getToken(SparkSqlBaseParser.COMMENT, 0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_colType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColType" ):
                listener.enterColType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColType" ):
                listener.exitColType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColType" ):
                return visitor.visitColType(self)
            else:
                return visitor.visitChildren(self)




    def colType(self):

        localctx = SparkSqlBaseParser.ColTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 172, self.RULE_colType)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2270
            self.identifier()
            self.state = 2271
            self.dataType()
            self.state = 2274
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,301,self._ctx)
            if la_ == 1:
                self.state = 2272
                self.match(SparkSqlBaseParser.COMMENT)
                self.state = 2273
                self.match(SparkSqlBaseParser.STRING)


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Complex_ColTypeListContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def complex_ColType(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.Complex_ColTypeContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.Complex_ColTypeContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_complex_ColTypeList

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComplex_ColTypeList" ):
                listener.enterComplex_ColTypeList(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComplex_ColTypeList" ):
                listener.exitComplex_ColTypeList(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComplex_ColTypeList" ):
                return visitor.visitComplex_ColTypeList(self)
            else:
                return visitor.visitChildren(self)




    def complex_ColTypeList(self):

        localctx = SparkSqlBaseParser.Complex_ColTypeListContext(self, self._ctx, self.state)
        self.enterRule(localctx, 174, self.RULE_complex_ColTypeList)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2276
            self.complex_ColType()
            self.state = 2281
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparkSqlBaseParser.T__2:
                self.state = 2277
                self.match(SparkSqlBaseParser.T__2)
                self.state = 2278
                self.complex_ColType()
                self.state = 2283
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Complex_ColTypeContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def dataType(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.DataTypeContext,0)


        def COMMENT(self):
            return self.getToken(SparkSqlBaseParser.COMMENT, 0)

        def STRING(self):
            return self.getToken(SparkSqlBaseParser.STRING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_complex_ColType

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComplex_ColType" ):
                listener.enterComplex_ColType(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComplex_ColType" ):
                listener.exitComplex_ColType(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComplex_ColType" ):
                return visitor.visitComplex_ColType(self)
            else:
                return visitor.visitChildren(self)




    def complex_ColType(self):

        localctx = SparkSqlBaseParser.Complex_ColTypeContext(self, self._ctx, self.state)
        self.enterRule(localctx, 176, self.RULE_complex_ColType)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2284
            self.identifier()
            self.state = 2285
            self.match(SparkSqlBaseParser.T__9)
            self.state = 2286
            self.dataType()
            self.state = 2289
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            if _la==SparkSqlBaseParser.COMMENT:
                self.state = 2287
                self.match(SparkSqlBaseParser.COMMENT)
                self.state = 2288
                self.match(SparkSqlBaseParser.STRING)


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class WhenClauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.condition = None # ExpressionContext
            self.result = None # ExpressionContext

        def WHEN(self):
            return self.getToken(SparkSqlBaseParser.WHEN, 0)

        def THEN(self):
            return self.getToken(SparkSqlBaseParser.THEN, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_whenClause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWhenClause" ):
                listener.enterWhenClause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWhenClause" ):
                listener.exitWhenClause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWhenClause" ):
                return visitor.visitWhenClause(self)
            else:
                return visitor.visitChildren(self)




    def whenClause(self):

        localctx = SparkSqlBaseParser.WhenClauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 178, self.RULE_whenClause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2291
            self.match(SparkSqlBaseParser.WHEN)
            self.state = 2292
            localctx.condition = self.expression()
            self.state = 2293
            self.match(SparkSqlBaseParser.THEN)
            self.state = 2294
            localctx.result = self.expression()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class WindowsContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def WINDOW(self):
            return self.getToken(SparkSqlBaseParser.WINDOW, 0)

        def namedWindow(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.NamedWindowContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.NamedWindowContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_windows

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindows" ):
                listener.enterWindows(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindows" ):
                listener.exitWindows(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindows" ):
                return visitor.visitWindows(self)
            else:
                return visitor.visitChildren(self)




    def windows(self):

        localctx = SparkSqlBaseParser.WindowsContext(self, self._ctx, self.state)
        self.enterRule(localctx, 180, self.RULE_windows)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2296
            self.match(SparkSqlBaseParser.WINDOW)
            self.state = 2297
            self.namedWindow()
            self.state = 2302
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,304,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2298
                    self.match(SparkSqlBaseParser.T__2)
                    self.state = 2299
                    self.namedWindow() 
                self.state = 2304
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,304,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NamedWindowContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def windowSpec(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.WindowSpecContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_namedWindow

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNamedWindow" ):
                listener.enterNamedWindow(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNamedWindow" ):
                listener.exitNamedWindow(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNamedWindow" ):
                return visitor.visitNamedWindow(self)
            else:
                return visitor.visitChildren(self)




    def namedWindow(self):

        localctx = SparkSqlBaseParser.NamedWindowContext(self, self._ctx, self.state)
        self.enterRule(localctx, 182, self.RULE_namedWindow)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2305
            self.identifier()
            self.state = 2306
            self.match(SparkSqlBaseParser.AS)
            self.state = 2307
            self.windowSpec()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class WindowSpecContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_windowSpec

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class WindowRefContext(WindowSpecContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.WindowSpecContext
            super().__init__(parser)
            self.name = None # IdentifierContext
            self.copyFrom(ctx)

        def identifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindowRef" ):
                listener.enterWindowRef(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindowRef" ):
                listener.exitWindowRef(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindowRef" ):
                return visitor.visitWindowRef(self)
            else:
                return visitor.visitChildren(self)


    class WindowDefContext(WindowSpecContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.WindowSpecContext
            super().__init__(parser)
            self._expression = None # ExpressionContext
            self.partition = list() # of ExpressionContexts
            self.copyFrom(ctx)

        def CLUSTER(self):
            return self.getToken(SparkSqlBaseParser.CLUSTER, 0)
        def BY(self, i:int=None):
            if i is None:
                return self.getTokens(SparkSqlBaseParser.BY)
            else:
                return self.getToken(SparkSqlBaseParser.BY, i)
        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,i)

        def windowFrame(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.WindowFrameContext,0)

        def sortItem(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.SortItemContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.SortItemContext,i)

        def PARTITION(self):
            return self.getToken(SparkSqlBaseParser.PARTITION, 0)
        def DISTRIBUTE(self):
            return self.getToken(SparkSqlBaseParser.DISTRIBUTE, 0)
        def ORDER(self):
            return self.getToken(SparkSqlBaseParser.ORDER, 0)
        def SORT(self):
            return self.getToken(SparkSqlBaseParser.SORT, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindowDef" ):
                listener.enterWindowDef(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindowDef" ):
                listener.exitWindowDef(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindowDef" ):
                return visitor.visitWindowDef(self)
            else:
                return visitor.visitChildren(self)



    def windowSpec(self):

        localctx = SparkSqlBaseParser.WindowSpecContext(self, self._ctx, self.state)
        self.enterRule(localctx, 184, self.RULE_windowSpec)
        self._la = 0 # Token type
        try:
            self.state = 2351
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.ADD, SparkSqlBaseParser.AS, SparkSqlBaseParser.ALL, SparkSqlBaseParser.ANY, SparkSqlBaseParser.DISTINCT, SparkSqlBaseParser.WHERE, SparkSqlBaseParser.GROUP, SparkSqlBaseParser.BY, SparkSqlBaseParser.GROUPING, SparkSqlBaseParser.SETS, SparkSqlBaseParser.CUBE, SparkSqlBaseParser.ROLLUP, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.HAVING, SparkSqlBaseParser.LIMIT, SparkSqlBaseParser.AT, SparkSqlBaseParser.OR, SparkSqlBaseParser.AND, SparkSqlBaseParser.IN, SparkSqlBaseParser.NOT, SparkSqlBaseParser.NO, SparkSqlBaseParser.EXISTS, SparkSqlBaseParser.BETWEEN, SparkSqlBaseParser.LIKE, SparkSqlBaseParser.RLIKE, SparkSqlBaseParser.IS, SparkSqlBaseParser.NULL, SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE, SparkSqlBaseParser.NULLS, SparkSqlBaseParser.ASC, SparkSqlBaseParser.DESC, SparkSqlBaseParser.FOR, SparkSqlBaseParser.INTERVAL, SparkSqlBaseParser.CASE, SparkSqlBaseParser.WHEN, SparkSqlBaseParser.THEN, SparkSqlBaseParser.ELSE, SparkSqlBaseParser.END, SparkSqlBaseParser.JOIN, SparkSqlBaseParser.CROSS, SparkSqlBaseParser.OUTER, SparkSqlBaseParser.INNER, SparkSqlBaseParser.LEFT, SparkSqlBaseParser.SEMI, SparkSqlBaseParser.RIGHT, SparkSqlBaseParser.FULL, SparkSqlBaseParser.NATURAL, SparkSqlBaseParser.ON, SparkSqlBaseParser.PIVOT, SparkSqlBaseParser.LATERAL, SparkSqlBaseParser.WINDOW, SparkSqlBaseParser.OVER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.UNBOUNDED, SparkSqlBaseParser.PRECEDING, SparkSqlBaseParser.FOLLOWING, SparkSqlBaseParser.CURRENT, SparkSqlBaseParser.FIRST, SparkSqlBaseParser.AFTER, SparkSqlBaseParser.LAST, SparkSqlBaseParser.ROW, SparkSqlBaseParser.WITH, SparkSqlBaseParser.VALUES, SparkSqlBaseParser.CREATE, SparkSqlBaseParser.TABLE, SparkSqlBaseParser.DIRECTORY, SparkSqlBaseParser.VIEW, SparkSqlBaseParser.REPLACE, SparkSqlBaseParser.INSERT, SparkSqlBaseParser.DELETE, SparkSqlBaseParser.INTO, SparkSqlBaseParser.DESCRIBE, SparkSqlBaseParser.EXPLAIN, SparkSqlBaseParser.FORMAT, SparkSqlBaseParser.LOGICAL, SparkSqlBaseParser.CODEGEN, SparkSqlBaseParser.COST, SparkSqlBaseParser.CAST, SparkSqlBaseParser.SHOW, SparkSqlBaseParser.TABLES, SparkSqlBaseParser.COLUMNS, SparkSqlBaseParser.COLUMN, SparkSqlBaseParser.USE, SparkSqlBaseParser.PARTITIONS, SparkSqlBaseParser.FUNCTIONS, SparkSqlBaseParser.DROP, SparkSqlBaseParser.UNION, SparkSqlBaseParser.EXCEPT, SparkSqlBaseParser.SETMINUS, SparkSqlBaseParser.INTERSECT, SparkSqlBaseParser.TO, SparkSqlBaseParser.TABLESAMPLE, SparkSqlBaseParser.STRATIFY, SparkSqlBaseParser.ALTER, SparkSqlBaseParser.RENAME, SparkSqlBaseParser.ARRAY, SparkSqlBaseParser.MAP, SparkSqlBaseParser.STRUCT, SparkSqlBaseParser.COMMENT, SparkSqlBaseParser.SET, SparkSqlBaseParser.RESET, SparkSqlBaseParser.DATA, SparkSqlBaseParser.START, SparkSqlBaseParser.TRANSACTION, SparkSqlBaseParser.COMMIT, SparkSqlBaseParser.ROLLBACK, SparkSqlBaseParser.MACRO, SparkSqlBaseParser.IGNORE, SparkSqlBaseParser.BOTH, SparkSqlBaseParser.LEADING, SparkSqlBaseParser.TRAILING, SparkSqlBaseParser.IF, SparkSqlBaseParser.POSITION, SparkSqlBaseParser.EXTRACT, SparkSqlBaseParser.DIV, SparkSqlBaseParser.PERCENTLIT, SparkSqlBaseParser.BUCKET, SparkSqlBaseParser.OUT, SparkSqlBaseParser.OF, SparkSqlBaseParser.SORT, SparkSqlBaseParser.CLUSTER, SparkSqlBaseParser.DISTRIBUTE, SparkSqlBaseParser.OVERWRITE, SparkSqlBaseParser.TRANSFORM, SparkSqlBaseParser.REDUCE, SparkSqlBaseParser.SERDE, SparkSqlBaseParser.SERDEPROPERTIES, SparkSqlBaseParser.RECORDREADER, SparkSqlBaseParser.RECORDWRITER, SparkSqlBaseParser.DELIMITED, SparkSqlBaseParser.FIELDS, SparkSqlBaseParser.TERMINATED, SparkSqlBaseParser.COLLECTION, SparkSqlBaseParser.ITEMS, SparkSqlBaseParser.KEYS, SparkSqlBaseParser.ESCAPED, SparkSqlBaseParser.LINES, SparkSqlBaseParser.SEPARATED, SparkSqlBaseParser.FUNCTION, SparkSqlBaseParser.EXTENDED, SparkSqlBaseParser.REFRESH, SparkSqlBaseParser.CLEAR, SparkSqlBaseParser.CACHE, SparkSqlBaseParser.UNCACHE, SparkSqlBaseParser.LAZY, SparkSqlBaseParser.FORMATTED, SparkSqlBaseParser.GLOBAL, SparkSqlBaseParser.TEMPORARY, SparkSqlBaseParser.OPTIONS, SparkSqlBaseParser.UNSET, SparkSqlBaseParser.TBLPROPERTIES, SparkSqlBaseParser.DBPROPERTIES, SparkSqlBaseParser.BUCKETS, SparkSqlBaseParser.SKEWED, SparkSqlBaseParser.STORED, SparkSqlBaseParser.DIRECTORIES, SparkSqlBaseParser.LOCATION, SparkSqlBaseParser.EXCHANGE, SparkSqlBaseParser.ARCHIVE, SparkSqlBaseParser.UNARCHIVE, SparkSqlBaseParser.FILEFORMAT, SparkSqlBaseParser.TOUCH, SparkSqlBaseParser.COMPACT, SparkSqlBaseParser.CONCATENATE, SparkSqlBaseParser.CHANGE, SparkSqlBaseParser.CASCADE, SparkSqlBaseParser.RESTRICT, SparkSqlBaseParser.CLUSTERED, SparkSqlBaseParser.SORTED, SparkSqlBaseParser.PURGE, SparkSqlBaseParser.INPUTFORMAT, SparkSqlBaseParser.OUTPUTFORMAT, SparkSqlBaseParser.DATABASE, SparkSqlBaseParser.DATABASES, SparkSqlBaseParser.DFS, SparkSqlBaseParser.TRUNCATE, SparkSqlBaseParser.ANALYZE, SparkSqlBaseParser.COMPUTE, SparkSqlBaseParser.LIST, SparkSqlBaseParser.STATISTICS, SparkSqlBaseParser.PARTITIONED, SparkSqlBaseParser.EXTERNAL, SparkSqlBaseParser.DEFINED, SparkSqlBaseParser.REVOKE, SparkSqlBaseParser.GRANT, SparkSqlBaseParser.LOCK, SparkSqlBaseParser.UNLOCK, SparkSqlBaseParser.MSCK, SparkSqlBaseParser.REPAIR, SparkSqlBaseParser.RECOVER, SparkSqlBaseParser.EXPORT, SparkSqlBaseParser.IMPORT, SparkSqlBaseParser.LOAD, SparkSqlBaseParser.ROLE, SparkSqlBaseParser.ROLES, SparkSqlBaseParser.COMPACTIONS, SparkSqlBaseParser.PRINCIPALS, SparkSqlBaseParser.TRANSACTIONS, SparkSqlBaseParser.INDEX, SparkSqlBaseParser.INDEXES, SparkSqlBaseParser.LOCKS, SparkSqlBaseParser.OPTION, SparkSqlBaseParser.ANTI, SparkSqlBaseParser.LOCAL, SparkSqlBaseParser.INPATH, SparkSqlBaseParser.IDENTIFIER, SparkSqlBaseParser.BACKQUOTED_IDENTIFIER]:
                localctx = SparkSqlBaseParser.WindowRefContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2309
                localctx.name = self.identifier()
                pass
            elif token in [SparkSqlBaseParser.T__0]:
                localctx = SparkSqlBaseParser.WindowDefContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2310
                self.match(SparkSqlBaseParser.T__0)
                self.state = 2345
                self._errHandler.sync(self)
                token = self._input.LA(1)
                if token in [SparkSqlBaseParser.CLUSTER]:
                    self.state = 2311
                    self.match(SparkSqlBaseParser.CLUSTER)
                    self.state = 2312
                    self.match(SparkSqlBaseParser.BY)
                    self.state = 2313
                    localctx._expression = self.expression()
                    localctx.partition.append(localctx._expression)
                    self.state = 2318
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    while _la==SparkSqlBaseParser.T__2:
                        self.state = 2314
                        self.match(SparkSqlBaseParser.T__2)
                        self.state = 2315
                        localctx._expression = self.expression()
                        localctx.partition.append(localctx._expression)
                        self.state = 2320
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)

                    pass
                elif token in [SparkSqlBaseParser.T__1, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.SORT, SparkSqlBaseParser.DISTRIBUTE]:
                    self.state = 2331
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.PARTITION or _la==SparkSqlBaseParser.DISTRIBUTE:
                        self.state = 2321
                        _la = self._input.LA(1)
                        if not(_la==SparkSqlBaseParser.PARTITION or _la==SparkSqlBaseParser.DISTRIBUTE):
                            self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 2322
                        self.match(SparkSqlBaseParser.BY)
                        self.state = 2323
                        localctx._expression = self.expression()
                        localctx.partition.append(localctx._expression)
                        self.state = 2328
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        while _la==SparkSqlBaseParser.T__2:
                            self.state = 2324
                            self.match(SparkSqlBaseParser.T__2)
                            self.state = 2325
                            localctx._expression = self.expression()
                            localctx.partition.append(localctx._expression)
                            self.state = 2330
                            self._errHandler.sync(self)
                            _la = self._input.LA(1)



                    self.state = 2343
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if _la==SparkSqlBaseParser.ORDER or _la==SparkSqlBaseParser.SORT:
                        self.state = 2333
                        _la = self._input.LA(1)
                        if not(_la==SparkSqlBaseParser.ORDER or _la==SparkSqlBaseParser.SORT):
                            self._errHandler.recoverInline(self)
                        else:
                            self._errHandler.reportMatch(self)
                            self.consume()
                        self.state = 2334
                        self.match(SparkSqlBaseParser.BY)
                        self.state = 2335
                        self.sortItem()
                        self.state = 2340
                        self._errHandler.sync(self)
                        _la = self._input.LA(1)
                        while _la==SparkSqlBaseParser.T__2:
                            self.state = 2336
                            self.match(SparkSqlBaseParser.T__2)
                            self.state = 2337
                            self.sortItem()
                            self.state = 2342
                            self._errHandler.sync(self)
                            _la = self._input.LA(1)



                    pass
                else:
                    raise NoViableAltException(self)

                self.state = 2348
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.RANGE or _la==SparkSqlBaseParser.ROWS:
                    self.state = 2347
                    self.windowFrame()


                self.state = 2350
                self.match(SparkSqlBaseParser.T__1)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class WindowFrameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.frameType = None # Token
            self.start = None # FrameBoundContext
            self.end = None # FrameBoundContext

        def RANGE(self):
            return self.getToken(SparkSqlBaseParser.RANGE, 0)

        def frameBound(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.FrameBoundContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.FrameBoundContext,i)


        def ROWS(self):
            return self.getToken(SparkSqlBaseParser.ROWS, 0)

        def BETWEEN(self):
            return self.getToken(SparkSqlBaseParser.BETWEEN, 0)

        def AND(self):
            return self.getToken(SparkSqlBaseParser.AND, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_windowFrame

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWindowFrame" ):
                listener.enterWindowFrame(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWindowFrame" ):
                listener.exitWindowFrame(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWindowFrame" ):
                return visitor.visitWindowFrame(self)
            else:
                return visitor.visitChildren(self)




    def windowFrame(self):

        localctx = SparkSqlBaseParser.WindowFrameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 186, self.RULE_windowFrame)
        try:
            self.state = 2369
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,313,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 2353
                localctx.frameType = self.match(SparkSqlBaseParser.RANGE)
                self.state = 2354
                localctx.start = self.frameBound()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 2355
                localctx.frameType = self.match(SparkSqlBaseParser.ROWS)
                self.state = 2356
                localctx.start = self.frameBound()
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 2357
                localctx.frameType = self.match(SparkSqlBaseParser.RANGE)
                self.state = 2358
                self.match(SparkSqlBaseParser.BETWEEN)
                self.state = 2359
                localctx.start = self.frameBound()
                self.state = 2360
                self.match(SparkSqlBaseParser.AND)
                self.state = 2361
                localctx.end = self.frameBound()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 2363
                localctx.frameType = self.match(SparkSqlBaseParser.ROWS)
                self.state = 2364
                self.match(SparkSqlBaseParser.BETWEEN)
                self.state = 2365
                localctx.start = self.frameBound()
                self.state = 2366
                self.match(SparkSqlBaseParser.AND)
                self.state = 2367
                localctx.end = self.frameBound()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class FrameBoundContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.boundType = None # Token

        def UNBOUNDED(self):
            return self.getToken(SparkSqlBaseParser.UNBOUNDED, 0)

        def PRECEDING(self):
            return self.getToken(SparkSqlBaseParser.PRECEDING, 0)

        def FOLLOWING(self):
            return self.getToken(SparkSqlBaseParser.FOLLOWING, 0)

        def ROW(self):
            return self.getToken(SparkSqlBaseParser.ROW, 0)

        def CURRENT(self):
            return self.getToken(SparkSqlBaseParser.CURRENT, 0)

        def expression(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.ExpressionContext,0)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_frameBound

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFrameBound" ):
                listener.enterFrameBound(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFrameBound" ):
                listener.exitFrameBound(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFrameBound" ):
                return visitor.visitFrameBound(self)
            else:
                return visitor.visitChildren(self)




    def frameBound(self):

        localctx = SparkSqlBaseParser.FrameBoundContext(self, self._ctx, self.state)
        self.enterRule(localctx, 188, self.RULE_frameBound)
        self._la = 0 # Token type
        try:
            self.state = 2378
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,314,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 2371
                self.match(SparkSqlBaseParser.UNBOUNDED)
                self.state = 2372
                localctx.boundType = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.PRECEDING or _la==SparkSqlBaseParser.FOLLOWING):
                    localctx.boundType = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 2373
                localctx.boundType = self.match(SparkSqlBaseParser.CURRENT)
                self.state = 2374
                self.match(SparkSqlBaseParser.ROW)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 2375
                self.expression()
                self.state = 2376
                localctx.boundType = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparkSqlBaseParser.PRECEDING or _la==SparkSqlBaseParser.FOLLOWING):
                    localctx.boundType = self._errHandler.recoverInline(self)
                else:
                    self._errHandler.reportMatch(self)
                    self.consume()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class QualifiedNameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def identifier(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparkSqlBaseParser.IdentifierContext)
            else:
                return self.getTypedRuleContext(SparkSqlBaseParser.IdentifierContext,i)


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_qualifiedName

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQualifiedName" ):
                listener.enterQualifiedName(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQualifiedName" ):
                listener.exitQualifiedName(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQualifiedName" ):
                return visitor.visitQualifiedName(self)
            else:
                return visitor.visitChildren(self)




    def qualifiedName(self):

        localctx = SparkSqlBaseParser.QualifiedNameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 190, self.RULE_qualifiedName)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2380
            self.identifier()
            self.state = 2385
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,315,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 2381
                    self.match(SparkSqlBaseParser.T__3)
                    self.state = 2382
                    self.identifier() 
                self.state = 2387
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,315,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class IdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def strictIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.StrictIdentifierContext,0)


        def ANTI(self):
            return self.getToken(SparkSqlBaseParser.ANTI, 0)

        def FULL(self):
            return self.getToken(SparkSqlBaseParser.FULL, 0)

        def INNER(self):
            return self.getToken(SparkSqlBaseParser.INNER, 0)

        def LEFT(self):
            return self.getToken(SparkSqlBaseParser.LEFT, 0)

        def SEMI(self):
            return self.getToken(SparkSqlBaseParser.SEMI, 0)

        def RIGHT(self):
            return self.getToken(SparkSqlBaseParser.RIGHT, 0)

        def NATURAL(self):
            return self.getToken(SparkSqlBaseParser.NATURAL, 0)

        def JOIN(self):
            return self.getToken(SparkSqlBaseParser.JOIN, 0)

        def CROSS(self):
            return self.getToken(SparkSqlBaseParser.CROSS, 0)

        def ON(self):
            return self.getToken(SparkSqlBaseParser.ON, 0)

        def UNION(self):
            return self.getToken(SparkSqlBaseParser.UNION, 0)

        def INTERSECT(self):
            return self.getToken(SparkSqlBaseParser.INTERSECT, 0)

        def EXCEPT(self):
            return self.getToken(SparkSqlBaseParser.EXCEPT, 0)

        def SETMINUS(self):
            return self.getToken(SparkSqlBaseParser.SETMINUS, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_identifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIdentifier" ):
                listener.enterIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIdentifier" ):
                listener.exitIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIdentifier" ):
                return visitor.visitIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def identifier(self):

        localctx = SparkSqlBaseParser.IdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 192, self.RULE_identifier)
        try:
            self.state = 2403
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.ADD, SparkSqlBaseParser.AS, SparkSqlBaseParser.ALL, SparkSqlBaseParser.ANY, SparkSqlBaseParser.DISTINCT, SparkSqlBaseParser.WHERE, SparkSqlBaseParser.GROUP, SparkSqlBaseParser.BY, SparkSqlBaseParser.GROUPING, SparkSqlBaseParser.SETS, SparkSqlBaseParser.CUBE, SparkSqlBaseParser.ROLLUP, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.HAVING, SparkSqlBaseParser.LIMIT, SparkSqlBaseParser.AT, SparkSqlBaseParser.OR, SparkSqlBaseParser.AND, SparkSqlBaseParser.IN, SparkSqlBaseParser.NOT, SparkSqlBaseParser.NO, SparkSqlBaseParser.EXISTS, SparkSqlBaseParser.BETWEEN, SparkSqlBaseParser.LIKE, SparkSqlBaseParser.RLIKE, SparkSqlBaseParser.IS, SparkSqlBaseParser.NULL, SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE, SparkSqlBaseParser.NULLS, SparkSqlBaseParser.ASC, SparkSqlBaseParser.DESC, SparkSqlBaseParser.FOR, SparkSqlBaseParser.INTERVAL, SparkSqlBaseParser.CASE, SparkSqlBaseParser.WHEN, SparkSqlBaseParser.THEN, SparkSqlBaseParser.ELSE, SparkSqlBaseParser.END, SparkSqlBaseParser.OUTER, SparkSqlBaseParser.PIVOT, SparkSqlBaseParser.LATERAL, SparkSqlBaseParser.WINDOW, SparkSqlBaseParser.OVER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.UNBOUNDED, SparkSqlBaseParser.PRECEDING, SparkSqlBaseParser.FOLLOWING, SparkSqlBaseParser.CURRENT, SparkSqlBaseParser.FIRST, SparkSqlBaseParser.AFTER, SparkSqlBaseParser.LAST, SparkSqlBaseParser.ROW, SparkSqlBaseParser.WITH, SparkSqlBaseParser.VALUES, SparkSqlBaseParser.CREATE, SparkSqlBaseParser.TABLE, SparkSqlBaseParser.DIRECTORY, SparkSqlBaseParser.VIEW, SparkSqlBaseParser.REPLACE, SparkSqlBaseParser.INSERT, SparkSqlBaseParser.DELETE, SparkSqlBaseParser.INTO, SparkSqlBaseParser.DESCRIBE, SparkSqlBaseParser.EXPLAIN, SparkSqlBaseParser.FORMAT, SparkSqlBaseParser.LOGICAL, SparkSqlBaseParser.CODEGEN, SparkSqlBaseParser.COST, SparkSqlBaseParser.CAST, SparkSqlBaseParser.SHOW, SparkSqlBaseParser.TABLES, SparkSqlBaseParser.COLUMNS, SparkSqlBaseParser.COLUMN, SparkSqlBaseParser.USE, SparkSqlBaseParser.PARTITIONS, SparkSqlBaseParser.FUNCTIONS, SparkSqlBaseParser.DROP, SparkSqlBaseParser.TO, SparkSqlBaseParser.TABLESAMPLE, SparkSqlBaseParser.STRATIFY, SparkSqlBaseParser.ALTER, SparkSqlBaseParser.RENAME, SparkSqlBaseParser.ARRAY, SparkSqlBaseParser.MAP, SparkSqlBaseParser.STRUCT, SparkSqlBaseParser.COMMENT, SparkSqlBaseParser.SET, SparkSqlBaseParser.RESET, SparkSqlBaseParser.DATA, SparkSqlBaseParser.START, SparkSqlBaseParser.TRANSACTION, SparkSqlBaseParser.COMMIT, SparkSqlBaseParser.ROLLBACK, SparkSqlBaseParser.MACRO, SparkSqlBaseParser.IGNORE, SparkSqlBaseParser.BOTH, SparkSqlBaseParser.LEADING, SparkSqlBaseParser.TRAILING, SparkSqlBaseParser.IF, SparkSqlBaseParser.POSITION, SparkSqlBaseParser.EXTRACT, SparkSqlBaseParser.DIV, SparkSqlBaseParser.PERCENTLIT, SparkSqlBaseParser.BUCKET, SparkSqlBaseParser.OUT, SparkSqlBaseParser.OF, SparkSqlBaseParser.SORT, SparkSqlBaseParser.CLUSTER, SparkSqlBaseParser.DISTRIBUTE, SparkSqlBaseParser.OVERWRITE, SparkSqlBaseParser.TRANSFORM, SparkSqlBaseParser.REDUCE, SparkSqlBaseParser.SERDE, SparkSqlBaseParser.SERDEPROPERTIES, SparkSqlBaseParser.RECORDREADER, SparkSqlBaseParser.RECORDWRITER, SparkSqlBaseParser.DELIMITED, SparkSqlBaseParser.FIELDS, SparkSqlBaseParser.TERMINATED, SparkSqlBaseParser.COLLECTION, SparkSqlBaseParser.ITEMS, SparkSqlBaseParser.KEYS, SparkSqlBaseParser.ESCAPED, SparkSqlBaseParser.LINES, SparkSqlBaseParser.SEPARATED, SparkSqlBaseParser.FUNCTION, SparkSqlBaseParser.EXTENDED, SparkSqlBaseParser.REFRESH, SparkSqlBaseParser.CLEAR, SparkSqlBaseParser.CACHE, SparkSqlBaseParser.UNCACHE, SparkSqlBaseParser.LAZY, SparkSqlBaseParser.FORMATTED, SparkSqlBaseParser.GLOBAL, SparkSqlBaseParser.TEMPORARY, SparkSqlBaseParser.OPTIONS, SparkSqlBaseParser.UNSET, SparkSqlBaseParser.TBLPROPERTIES, SparkSqlBaseParser.DBPROPERTIES, SparkSqlBaseParser.BUCKETS, SparkSqlBaseParser.SKEWED, SparkSqlBaseParser.STORED, SparkSqlBaseParser.DIRECTORIES, SparkSqlBaseParser.LOCATION, SparkSqlBaseParser.EXCHANGE, SparkSqlBaseParser.ARCHIVE, SparkSqlBaseParser.UNARCHIVE, SparkSqlBaseParser.FILEFORMAT, SparkSqlBaseParser.TOUCH, SparkSqlBaseParser.COMPACT, SparkSqlBaseParser.CONCATENATE, SparkSqlBaseParser.CHANGE, SparkSqlBaseParser.CASCADE, SparkSqlBaseParser.RESTRICT, SparkSqlBaseParser.CLUSTERED, SparkSqlBaseParser.SORTED, SparkSqlBaseParser.PURGE, SparkSqlBaseParser.INPUTFORMAT, SparkSqlBaseParser.OUTPUTFORMAT, SparkSqlBaseParser.DATABASE, SparkSqlBaseParser.DATABASES, SparkSqlBaseParser.DFS, SparkSqlBaseParser.TRUNCATE, SparkSqlBaseParser.ANALYZE, SparkSqlBaseParser.COMPUTE, SparkSqlBaseParser.LIST, SparkSqlBaseParser.STATISTICS, SparkSqlBaseParser.PARTITIONED, SparkSqlBaseParser.EXTERNAL, SparkSqlBaseParser.DEFINED, SparkSqlBaseParser.REVOKE, SparkSqlBaseParser.GRANT, SparkSqlBaseParser.LOCK, SparkSqlBaseParser.UNLOCK, SparkSqlBaseParser.MSCK, SparkSqlBaseParser.REPAIR, SparkSqlBaseParser.RECOVER, SparkSqlBaseParser.EXPORT, SparkSqlBaseParser.IMPORT, SparkSqlBaseParser.LOAD, SparkSqlBaseParser.ROLE, SparkSqlBaseParser.ROLES, SparkSqlBaseParser.COMPACTIONS, SparkSqlBaseParser.PRINCIPALS, SparkSqlBaseParser.TRANSACTIONS, SparkSqlBaseParser.INDEX, SparkSqlBaseParser.INDEXES, SparkSqlBaseParser.LOCKS, SparkSqlBaseParser.OPTION, SparkSqlBaseParser.LOCAL, SparkSqlBaseParser.INPATH, SparkSqlBaseParser.IDENTIFIER, SparkSqlBaseParser.BACKQUOTED_IDENTIFIER]:
                self.enterOuterAlt(localctx, 1)
                self.state = 2388
                self.strictIdentifier()
                pass
            elif token in [SparkSqlBaseParser.ANTI]:
                self.enterOuterAlt(localctx, 2)
                self.state = 2389
                self.match(SparkSqlBaseParser.ANTI)
                pass
            elif token in [SparkSqlBaseParser.FULL]:
                self.enterOuterAlt(localctx, 3)
                self.state = 2390
                self.match(SparkSqlBaseParser.FULL)
                pass
            elif token in [SparkSqlBaseParser.INNER]:
                self.enterOuterAlt(localctx, 4)
                self.state = 2391
                self.match(SparkSqlBaseParser.INNER)
                pass
            elif token in [SparkSqlBaseParser.LEFT]:
                self.enterOuterAlt(localctx, 5)
                self.state = 2392
                self.match(SparkSqlBaseParser.LEFT)
                pass
            elif token in [SparkSqlBaseParser.SEMI]:
                self.enterOuterAlt(localctx, 6)
                self.state = 2393
                self.match(SparkSqlBaseParser.SEMI)
                pass
            elif token in [SparkSqlBaseParser.RIGHT]:
                self.enterOuterAlt(localctx, 7)
                self.state = 2394
                self.match(SparkSqlBaseParser.RIGHT)
                pass
            elif token in [SparkSqlBaseParser.NATURAL]:
                self.enterOuterAlt(localctx, 8)
                self.state = 2395
                self.match(SparkSqlBaseParser.NATURAL)
                pass
            elif token in [SparkSqlBaseParser.JOIN]:
                self.enterOuterAlt(localctx, 9)
                self.state = 2396
                self.match(SparkSqlBaseParser.JOIN)
                pass
            elif token in [SparkSqlBaseParser.CROSS]:
                self.enterOuterAlt(localctx, 10)
                self.state = 2397
                self.match(SparkSqlBaseParser.CROSS)
                pass
            elif token in [SparkSqlBaseParser.ON]:
                self.enterOuterAlt(localctx, 11)
                self.state = 2398
                self.match(SparkSqlBaseParser.ON)
                pass
            elif token in [SparkSqlBaseParser.UNION]:
                self.enterOuterAlt(localctx, 12)
                self.state = 2399
                self.match(SparkSqlBaseParser.UNION)
                pass
            elif token in [SparkSqlBaseParser.INTERSECT]:
                self.enterOuterAlt(localctx, 13)
                self.state = 2400
                self.match(SparkSqlBaseParser.INTERSECT)
                pass
            elif token in [SparkSqlBaseParser.EXCEPT]:
                self.enterOuterAlt(localctx, 14)
                self.state = 2401
                self.match(SparkSqlBaseParser.EXCEPT)
                pass
            elif token in [SparkSqlBaseParser.SETMINUS]:
                self.enterOuterAlt(localctx, 15)
                self.state = 2402
                self.match(SparkSqlBaseParser.SETMINUS)
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class StrictIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_strictIdentifier

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class QuotedIdentifierAlternativeContext(StrictIdentifierContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StrictIdentifierContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def quotedIdentifier(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.QuotedIdentifierContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuotedIdentifierAlternative" ):
                listener.enterQuotedIdentifierAlternative(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuotedIdentifierAlternative" ):
                listener.exitQuotedIdentifierAlternative(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuotedIdentifierAlternative" ):
                return visitor.visitQuotedIdentifierAlternative(self)
            else:
                return visitor.visitChildren(self)


    class UnquotedIdentifierContext(StrictIdentifierContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.StrictIdentifierContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def IDENTIFIER(self):
            return self.getToken(SparkSqlBaseParser.IDENTIFIER, 0)
        def nonReserved(self):
            return self.getTypedRuleContext(SparkSqlBaseParser.NonReservedContext,0)


        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnquotedIdentifier" ):
                listener.enterUnquotedIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnquotedIdentifier" ):
                listener.exitUnquotedIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnquotedIdentifier" ):
                return visitor.visitUnquotedIdentifier(self)
            else:
                return visitor.visitChildren(self)



    def strictIdentifier(self):

        localctx = SparkSqlBaseParser.StrictIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 194, self.RULE_strictIdentifier)
        try:
            self.state = 2408
            self._errHandler.sync(self)
            token = self._input.LA(1)
            if token in [SparkSqlBaseParser.IDENTIFIER]:
                localctx = SparkSqlBaseParser.UnquotedIdentifierContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2405
                self.match(SparkSqlBaseParser.IDENTIFIER)
                pass
            elif token in [SparkSqlBaseParser.BACKQUOTED_IDENTIFIER]:
                localctx = SparkSqlBaseParser.QuotedIdentifierAlternativeContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2406
                self.quotedIdentifier()
                pass
            elif token in [SparkSqlBaseParser.SELECT, SparkSqlBaseParser.FROM, SparkSqlBaseParser.ADD, SparkSqlBaseParser.AS, SparkSqlBaseParser.ALL, SparkSqlBaseParser.ANY, SparkSqlBaseParser.DISTINCT, SparkSqlBaseParser.WHERE, SparkSqlBaseParser.GROUP, SparkSqlBaseParser.BY, SparkSqlBaseParser.GROUPING, SparkSqlBaseParser.SETS, SparkSqlBaseParser.CUBE, SparkSqlBaseParser.ROLLUP, SparkSqlBaseParser.ORDER, SparkSqlBaseParser.HAVING, SparkSqlBaseParser.LIMIT, SparkSqlBaseParser.AT, SparkSqlBaseParser.OR, SparkSqlBaseParser.AND, SparkSqlBaseParser.IN, SparkSqlBaseParser.NOT, SparkSqlBaseParser.NO, SparkSqlBaseParser.EXISTS, SparkSqlBaseParser.BETWEEN, SparkSqlBaseParser.LIKE, SparkSqlBaseParser.RLIKE, SparkSqlBaseParser.IS, SparkSqlBaseParser.NULL, SparkSqlBaseParser.TRUE, SparkSqlBaseParser.FALSE, SparkSqlBaseParser.NULLS, SparkSqlBaseParser.ASC, SparkSqlBaseParser.DESC, SparkSqlBaseParser.FOR, SparkSqlBaseParser.INTERVAL, SparkSqlBaseParser.CASE, SparkSqlBaseParser.WHEN, SparkSqlBaseParser.THEN, SparkSqlBaseParser.ELSE, SparkSqlBaseParser.END, SparkSqlBaseParser.OUTER, SparkSqlBaseParser.PIVOT, SparkSqlBaseParser.LATERAL, SparkSqlBaseParser.WINDOW, SparkSqlBaseParser.OVER, SparkSqlBaseParser.PARTITION, SparkSqlBaseParser.RANGE, SparkSqlBaseParser.ROWS, SparkSqlBaseParser.UNBOUNDED, SparkSqlBaseParser.PRECEDING, SparkSqlBaseParser.FOLLOWING, SparkSqlBaseParser.CURRENT, SparkSqlBaseParser.FIRST, SparkSqlBaseParser.AFTER, SparkSqlBaseParser.LAST, SparkSqlBaseParser.ROW, SparkSqlBaseParser.WITH, SparkSqlBaseParser.VALUES, SparkSqlBaseParser.CREATE, SparkSqlBaseParser.TABLE, SparkSqlBaseParser.DIRECTORY, SparkSqlBaseParser.VIEW, SparkSqlBaseParser.REPLACE, SparkSqlBaseParser.INSERT, SparkSqlBaseParser.DELETE, SparkSqlBaseParser.INTO, SparkSqlBaseParser.DESCRIBE, SparkSqlBaseParser.EXPLAIN, SparkSqlBaseParser.FORMAT, SparkSqlBaseParser.LOGICAL, SparkSqlBaseParser.CODEGEN, SparkSqlBaseParser.COST, SparkSqlBaseParser.CAST, SparkSqlBaseParser.SHOW, SparkSqlBaseParser.TABLES, SparkSqlBaseParser.COLUMNS, SparkSqlBaseParser.COLUMN, SparkSqlBaseParser.USE, SparkSqlBaseParser.PARTITIONS, SparkSqlBaseParser.FUNCTIONS, SparkSqlBaseParser.DROP, SparkSqlBaseParser.TO, SparkSqlBaseParser.TABLESAMPLE, SparkSqlBaseParser.STRATIFY, SparkSqlBaseParser.ALTER, SparkSqlBaseParser.RENAME, SparkSqlBaseParser.ARRAY, SparkSqlBaseParser.MAP, SparkSqlBaseParser.STRUCT, SparkSqlBaseParser.COMMENT, SparkSqlBaseParser.SET, SparkSqlBaseParser.RESET, SparkSqlBaseParser.DATA, SparkSqlBaseParser.START, SparkSqlBaseParser.TRANSACTION, SparkSqlBaseParser.COMMIT, SparkSqlBaseParser.ROLLBACK, SparkSqlBaseParser.MACRO, SparkSqlBaseParser.IGNORE, SparkSqlBaseParser.BOTH, SparkSqlBaseParser.LEADING, SparkSqlBaseParser.TRAILING, SparkSqlBaseParser.IF, SparkSqlBaseParser.POSITION, SparkSqlBaseParser.EXTRACT, SparkSqlBaseParser.DIV, SparkSqlBaseParser.PERCENTLIT, SparkSqlBaseParser.BUCKET, SparkSqlBaseParser.OUT, SparkSqlBaseParser.OF, SparkSqlBaseParser.SORT, SparkSqlBaseParser.CLUSTER, SparkSqlBaseParser.DISTRIBUTE, SparkSqlBaseParser.OVERWRITE, SparkSqlBaseParser.TRANSFORM, SparkSqlBaseParser.REDUCE, SparkSqlBaseParser.SERDE, SparkSqlBaseParser.SERDEPROPERTIES, SparkSqlBaseParser.RECORDREADER, SparkSqlBaseParser.RECORDWRITER, SparkSqlBaseParser.DELIMITED, SparkSqlBaseParser.FIELDS, SparkSqlBaseParser.TERMINATED, SparkSqlBaseParser.COLLECTION, SparkSqlBaseParser.ITEMS, SparkSqlBaseParser.KEYS, SparkSqlBaseParser.ESCAPED, SparkSqlBaseParser.LINES, SparkSqlBaseParser.SEPARATED, SparkSqlBaseParser.FUNCTION, SparkSqlBaseParser.EXTENDED, SparkSqlBaseParser.REFRESH, SparkSqlBaseParser.CLEAR, SparkSqlBaseParser.CACHE, SparkSqlBaseParser.UNCACHE, SparkSqlBaseParser.LAZY, SparkSqlBaseParser.FORMATTED, SparkSqlBaseParser.GLOBAL, SparkSqlBaseParser.TEMPORARY, SparkSqlBaseParser.OPTIONS, SparkSqlBaseParser.UNSET, SparkSqlBaseParser.TBLPROPERTIES, SparkSqlBaseParser.DBPROPERTIES, SparkSqlBaseParser.BUCKETS, SparkSqlBaseParser.SKEWED, SparkSqlBaseParser.STORED, SparkSqlBaseParser.DIRECTORIES, SparkSqlBaseParser.LOCATION, SparkSqlBaseParser.EXCHANGE, SparkSqlBaseParser.ARCHIVE, SparkSqlBaseParser.UNARCHIVE, SparkSqlBaseParser.FILEFORMAT, SparkSqlBaseParser.TOUCH, SparkSqlBaseParser.COMPACT, SparkSqlBaseParser.CONCATENATE, SparkSqlBaseParser.CHANGE, SparkSqlBaseParser.CASCADE, SparkSqlBaseParser.RESTRICT, SparkSqlBaseParser.CLUSTERED, SparkSqlBaseParser.SORTED, SparkSqlBaseParser.PURGE, SparkSqlBaseParser.INPUTFORMAT, SparkSqlBaseParser.OUTPUTFORMAT, SparkSqlBaseParser.DATABASE, SparkSqlBaseParser.DATABASES, SparkSqlBaseParser.DFS, SparkSqlBaseParser.TRUNCATE, SparkSqlBaseParser.ANALYZE, SparkSqlBaseParser.COMPUTE, SparkSqlBaseParser.LIST, SparkSqlBaseParser.STATISTICS, SparkSqlBaseParser.PARTITIONED, SparkSqlBaseParser.EXTERNAL, SparkSqlBaseParser.DEFINED, SparkSqlBaseParser.REVOKE, SparkSqlBaseParser.GRANT, SparkSqlBaseParser.LOCK, SparkSqlBaseParser.UNLOCK, SparkSqlBaseParser.MSCK, SparkSqlBaseParser.REPAIR, SparkSqlBaseParser.RECOVER, SparkSqlBaseParser.EXPORT, SparkSqlBaseParser.IMPORT, SparkSqlBaseParser.LOAD, SparkSqlBaseParser.ROLE, SparkSqlBaseParser.ROLES, SparkSqlBaseParser.COMPACTIONS, SparkSqlBaseParser.PRINCIPALS, SparkSqlBaseParser.TRANSACTIONS, SparkSqlBaseParser.INDEX, SparkSqlBaseParser.INDEXES, SparkSqlBaseParser.LOCKS, SparkSqlBaseParser.OPTION, SparkSqlBaseParser.LOCAL, SparkSqlBaseParser.INPATH]:
                localctx = SparkSqlBaseParser.UnquotedIdentifierContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2407
                self.nonReserved()
                pass
            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class QuotedIdentifierContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def BACKQUOTED_IDENTIFIER(self):
            return self.getToken(SparkSqlBaseParser.BACKQUOTED_IDENTIFIER, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_quotedIdentifier

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuotedIdentifier" ):
                listener.enterQuotedIdentifier(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuotedIdentifier" ):
                listener.exitQuotedIdentifier(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuotedIdentifier" ):
                return visitor.visitQuotedIdentifier(self)
            else:
                return visitor.visitChildren(self)




    def quotedIdentifier(self):

        localctx = SparkSqlBaseParser.QuotedIdentifierContext(self, self._ctx, self.state)
        self.enterRule(localctx, 196, self.RULE_quotedIdentifier)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2410
            self.match(SparkSqlBaseParser.BACKQUOTED_IDENTIFIER)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NumberContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_number

     
        def copyFrom(self, ctx:ParserRuleContext):
            super().copyFrom(ctx)



    class DecimalLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DECIMAL_VALUE(self):
            return self.getToken(SparkSqlBaseParser.DECIMAL_VALUE, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDecimalLiteral" ):
                listener.enterDecimalLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDecimalLiteral" ):
                listener.exitDecimalLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDecimalLiteral" ):
                return visitor.visitDecimalLiteral(self)
            else:
                return visitor.visitChildren(self)


    class BigIntLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def BIGINT_LITERAL(self):
            return self.getToken(SparkSqlBaseParser.BIGINT_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBigIntLiteral" ):
                listener.enterBigIntLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBigIntLiteral" ):
                listener.exitBigIntLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBigIntLiteral" ):
                return visitor.visitBigIntLiteral(self)
            else:
                return visitor.visitChildren(self)


    class TinyIntLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def TINYINT_LITERAL(self):
            return self.getToken(SparkSqlBaseParser.TINYINT_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTinyIntLiteral" ):
                listener.enterTinyIntLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTinyIntLiteral" ):
                listener.exitTinyIntLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTinyIntLiteral" ):
                return visitor.visitTinyIntLiteral(self)
            else:
                return visitor.visitChildren(self)


    class BigDecimalLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def BIGDECIMAL_LITERAL(self):
            return self.getToken(SparkSqlBaseParser.BIGDECIMAL_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterBigDecimalLiteral" ):
                listener.enterBigDecimalLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitBigDecimalLiteral" ):
                listener.exitBigDecimalLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitBigDecimalLiteral" ):
                return visitor.visitBigDecimalLiteral(self)
            else:
                return visitor.visitChildren(self)


    class DoubleLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def DOUBLE_LITERAL(self):
            return self.getToken(SparkSqlBaseParser.DOUBLE_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDoubleLiteral" ):
                listener.enterDoubleLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDoubleLiteral" ):
                listener.exitDoubleLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDoubleLiteral" ):
                return visitor.visitDoubleLiteral(self)
            else:
                return visitor.visitChildren(self)


    class IntegerLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def INTEGER_VALUE(self):
            return self.getToken(SparkSqlBaseParser.INTEGER_VALUE, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIntegerLiteral" ):
                listener.enterIntegerLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIntegerLiteral" ):
                listener.exitIntegerLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIntegerLiteral" ):
                return visitor.visitIntegerLiteral(self)
            else:
                return visitor.visitChildren(self)


    class SmallIntLiteralContext(NumberContext):

        def __init__(self, parser, ctx:ParserRuleContext): # actually a SparkSqlBaseParser.NumberContext
            super().__init__(parser)
            self.copyFrom(ctx)

        def SMALLINT_LITERAL(self):
            return self.getToken(SparkSqlBaseParser.SMALLINT_LITERAL, 0)
        def MINUS(self):
            return self.getToken(SparkSqlBaseParser.MINUS, 0)

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSmallIntLiteral" ):
                listener.enterSmallIntLiteral(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSmallIntLiteral" ):
                listener.exitSmallIntLiteral(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSmallIntLiteral" ):
                return visitor.visitSmallIntLiteral(self)
            else:
                return visitor.visitChildren(self)



    def number(self):

        localctx = SparkSqlBaseParser.NumberContext(self, self._ctx, self.state)
        self.enterRule(localctx, 198, self.RULE_number)
        self._la = 0 # Token type
        try:
            self.state = 2440
            self._errHandler.sync(self)
            la_ = self._interp.adaptivePredict(self._input,325,self._ctx)
            if la_ == 1:
                localctx = SparkSqlBaseParser.DecimalLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 1)
                self.state = 2413
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 2412
                    self.match(SparkSqlBaseParser.MINUS)


                self.state = 2415
                self.match(SparkSqlBaseParser.DECIMAL_VALUE)
                pass

            elif la_ == 2:
                localctx = SparkSqlBaseParser.IntegerLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 2)
                self.state = 2417
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 2416
                    self.match(SparkSqlBaseParser.MINUS)


                self.state = 2419
                self.match(SparkSqlBaseParser.INTEGER_VALUE)
                pass

            elif la_ == 3:
                localctx = SparkSqlBaseParser.BigIntLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 3)
                self.state = 2421
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 2420
                    self.match(SparkSqlBaseParser.MINUS)


                self.state = 2423
                self.match(SparkSqlBaseParser.BIGINT_LITERAL)
                pass

            elif la_ == 4:
                localctx = SparkSqlBaseParser.SmallIntLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 4)
                self.state = 2425
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 2424
                    self.match(SparkSqlBaseParser.MINUS)


                self.state = 2427
                self.match(SparkSqlBaseParser.SMALLINT_LITERAL)
                pass

            elif la_ == 5:
                localctx = SparkSqlBaseParser.TinyIntLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 5)
                self.state = 2429
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 2428
                    self.match(SparkSqlBaseParser.MINUS)


                self.state = 2431
                self.match(SparkSqlBaseParser.TINYINT_LITERAL)
                pass

            elif la_ == 6:
                localctx = SparkSqlBaseParser.DoubleLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 6)
                self.state = 2433
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 2432
                    self.match(SparkSqlBaseParser.MINUS)


                self.state = 2435
                self.match(SparkSqlBaseParser.DOUBLE_LITERAL)
                pass

            elif la_ == 7:
                localctx = SparkSqlBaseParser.BigDecimalLiteralContext(self, localctx)
                self.enterOuterAlt(localctx, 7)
                self.state = 2437
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                if _la==SparkSqlBaseParser.MINUS:
                    self.state = 2436
                    self.match(SparkSqlBaseParser.MINUS)


                self.state = 2439
                self.match(SparkSqlBaseParser.BIGDECIMAL_LITERAL)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NonReservedContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def SHOW(self):
            return self.getToken(SparkSqlBaseParser.SHOW, 0)

        def TABLES(self):
            return self.getToken(SparkSqlBaseParser.TABLES, 0)

        def COLUMNS(self):
            return self.getToken(SparkSqlBaseParser.COLUMNS, 0)

        def COLUMN(self):
            return self.getToken(SparkSqlBaseParser.COLUMN, 0)

        def PARTITIONS(self):
            return self.getToken(SparkSqlBaseParser.PARTITIONS, 0)

        def FUNCTIONS(self):
            return self.getToken(SparkSqlBaseParser.FUNCTIONS, 0)

        def DATABASES(self):
            return self.getToken(SparkSqlBaseParser.DATABASES, 0)

        def ADD(self):
            return self.getToken(SparkSqlBaseParser.ADD, 0)

        def OVER(self):
            return self.getToken(SparkSqlBaseParser.OVER, 0)

        def PARTITION(self):
            return self.getToken(SparkSqlBaseParser.PARTITION, 0)

        def RANGE(self):
            return self.getToken(SparkSqlBaseParser.RANGE, 0)

        def ROWS(self):
            return self.getToken(SparkSqlBaseParser.ROWS, 0)

        def PRECEDING(self):
            return self.getToken(SparkSqlBaseParser.PRECEDING, 0)

        def FOLLOWING(self):
            return self.getToken(SparkSqlBaseParser.FOLLOWING, 0)

        def CURRENT(self):
            return self.getToken(SparkSqlBaseParser.CURRENT, 0)

        def ROW(self):
            return self.getToken(SparkSqlBaseParser.ROW, 0)

        def LAST(self):
            return self.getToken(SparkSqlBaseParser.LAST, 0)

        def FIRST(self):
            return self.getToken(SparkSqlBaseParser.FIRST, 0)

        def AFTER(self):
            return self.getToken(SparkSqlBaseParser.AFTER, 0)

        def MAP(self):
            return self.getToken(SparkSqlBaseParser.MAP, 0)

        def ARRAY(self):
            return self.getToken(SparkSqlBaseParser.ARRAY, 0)

        def STRUCT(self):
            return self.getToken(SparkSqlBaseParser.STRUCT, 0)

        def PIVOT(self):
            return self.getToken(SparkSqlBaseParser.PIVOT, 0)

        def LATERAL(self):
            return self.getToken(SparkSqlBaseParser.LATERAL, 0)

        def WINDOW(self):
            return self.getToken(SparkSqlBaseParser.WINDOW, 0)

        def REDUCE(self):
            return self.getToken(SparkSqlBaseParser.REDUCE, 0)

        def TRANSFORM(self):
            return self.getToken(SparkSqlBaseParser.TRANSFORM, 0)

        def SERDE(self):
            return self.getToken(SparkSqlBaseParser.SERDE, 0)

        def SERDEPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.SERDEPROPERTIES, 0)

        def RECORDREADER(self):
            return self.getToken(SparkSqlBaseParser.RECORDREADER, 0)

        def DELIMITED(self):
            return self.getToken(SparkSqlBaseParser.DELIMITED, 0)

        def FIELDS(self):
            return self.getToken(SparkSqlBaseParser.FIELDS, 0)

        def TERMINATED(self):
            return self.getToken(SparkSqlBaseParser.TERMINATED, 0)

        def COLLECTION(self):
            return self.getToken(SparkSqlBaseParser.COLLECTION, 0)

        def ITEMS(self):
            return self.getToken(SparkSqlBaseParser.ITEMS, 0)

        def KEYS(self):
            return self.getToken(SparkSqlBaseParser.KEYS, 0)

        def ESCAPED(self):
            return self.getToken(SparkSqlBaseParser.ESCAPED, 0)

        def LINES(self):
            return self.getToken(SparkSqlBaseParser.LINES, 0)

        def SEPARATED(self):
            return self.getToken(SparkSqlBaseParser.SEPARATED, 0)

        def EXTENDED(self):
            return self.getToken(SparkSqlBaseParser.EXTENDED, 0)

        def REFRESH(self):
            return self.getToken(SparkSqlBaseParser.REFRESH, 0)

        def CLEAR(self):
            return self.getToken(SparkSqlBaseParser.CLEAR, 0)

        def CACHE(self):
            return self.getToken(SparkSqlBaseParser.CACHE, 0)

        def UNCACHE(self):
            return self.getToken(SparkSqlBaseParser.UNCACHE, 0)

        def LAZY(self):
            return self.getToken(SparkSqlBaseParser.LAZY, 0)

        def GLOBAL(self):
            return self.getToken(SparkSqlBaseParser.GLOBAL, 0)

        def TEMPORARY(self):
            return self.getToken(SparkSqlBaseParser.TEMPORARY, 0)

        def OPTIONS(self):
            return self.getToken(SparkSqlBaseParser.OPTIONS, 0)

        def GROUPING(self):
            return self.getToken(SparkSqlBaseParser.GROUPING, 0)

        def CUBE(self):
            return self.getToken(SparkSqlBaseParser.CUBE, 0)

        def ROLLUP(self):
            return self.getToken(SparkSqlBaseParser.ROLLUP, 0)

        def EXPLAIN(self):
            return self.getToken(SparkSqlBaseParser.EXPLAIN, 0)

        def FORMAT(self):
            return self.getToken(SparkSqlBaseParser.FORMAT, 0)

        def LOGICAL(self):
            return self.getToken(SparkSqlBaseParser.LOGICAL, 0)

        def FORMATTED(self):
            return self.getToken(SparkSqlBaseParser.FORMATTED, 0)

        def CODEGEN(self):
            return self.getToken(SparkSqlBaseParser.CODEGEN, 0)

        def COST(self):
            return self.getToken(SparkSqlBaseParser.COST, 0)

        def TABLESAMPLE(self):
            return self.getToken(SparkSqlBaseParser.TABLESAMPLE, 0)

        def USE(self):
            return self.getToken(SparkSqlBaseParser.USE, 0)

        def TO(self):
            return self.getToken(SparkSqlBaseParser.TO, 0)

        def BUCKET(self):
            return self.getToken(SparkSqlBaseParser.BUCKET, 0)

        def PERCENTLIT(self):
            return self.getToken(SparkSqlBaseParser.PERCENTLIT, 0)

        def OUT(self):
            return self.getToken(SparkSqlBaseParser.OUT, 0)

        def OF(self):
            return self.getToken(SparkSqlBaseParser.OF, 0)

        def SET(self):
            return self.getToken(SparkSqlBaseParser.SET, 0)

        def RESET(self):
            return self.getToken(SparkSqlBaseParser.RESET, 0)

        def VIEW(self):
            return self.getToken(SparkSqlBaseParser.VIEW, 0)

        def REPLACE(self):
            return self.getToken(SparkSqlBaseParser.REPLACE, 0)

        def IF(self):
            return self.getToken(SparkSqlBaseParser.IF, 0)

        def POSITION(self):
            return self.getToken(SparkSqlBaseParser.POSITION, 0)

        def EXTRACT(self):
            return self.getToken(SparkSqlBaseParser.EXTRACT, 0)

        def NO(self):
            return self.getToken(SparkSqlBaseParser.NO, 0)

        def DATA(self):
            return self.getToken(SparkSqlBaseParser.DATA, 0)

        def START(self):
            return self.getToken(SparkSqlBaseParser.START, 0)

        def TRANSACTION(self):
            return self.getToken(SparkSqlBaseParser.TRANSACTION, 0)

        def COMMIT(self):
            return self.getToken(SparkSqlBaseParser.COMMIT, 0)

        def ROLLBACK(self):
            return self.getToken(SparkSqlBaseParser.ROLLBACK, 0)

        def IGNORE(self):
            return self.getToken(SparkSqlBaseParser.IGNORE, 0)

        def SORT(self):
            return self.getToken(SparkSqlBaseParser.SORT, 0)

        def CLUSTER(self):
            return self.getToken(SparkSqlBaseParser.CLUSTER, 0)

        def DISTRIBUTE(self):
            return self.getToken(SparkSqlBaseParser.DISTRIBUTE, 0)

        def UNSET(self):
            return self.getToken(SparkSqlBaseParser.UNSET, 0)

        def TBLPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.TBLPROPERTIES, 0)

        def SKEWED(self):
            return self.getToken(SparkSqlBaseParser.SKEWED, 0)

        def STORED(self):
            return self.getToken(SparkSqlBaseParser.STORED, 0)

        def DIRECTORIES(self):
            return self.getToken(SparkSqlBaseParser.DIRECTORIES, 0)

        def LOCATION(self):
            return self.getToken(SparkSqlBaseParser.LOCATION, 0)

        def EXCHANGE(self):
            return self.getToken(SparkSqlBaseParser.EXCHANGE, 0)

        def ARCHIVE(self):
            return self.getToken(SparkSqlBaseParser.ARCHIVE, 0)

        def UNARCHIVE(self):
            return self.getToken(SparkSqlBaseParser.UNARCHIVE, 0)

        def FILEFORMAT(self):
            return self.getToken(SparkSqlBaseParser.FILEFORMAT, 0)

        def TOUCH(self):
            return self.getToken(SparkSqlBaseParser.TOUCH, 0)

        def COMPACT(self):
            return self.getToken(SparkSqlBaseParser.COMPACT, 0)

        def CONCATENATE(self):
            return self.getToken(SparkSqlBaseParser.CONCATENATE, 0)

        def CHANGE(self):
            return self.getToken(SparkSqlBaseParser.CHANGE, 0)

        def CASCADE(self):
            return self.getToken(SparkSqlBaseParser.CASCADE, 0)

        def RESTRICT(self):
            return self.getToken(SparkSqlBaseParser.RESTRICT, 0)

        def BUCKETS(self):
            return self.getToken(SparkSqlBaseParser.BUCKETS, 0)

        def CLUSTERED(self):
            return self.getToken(SparkSqlBaseParser.CLUSTERED, 0)

        def SORTED(self):
            return self.getToken(SparkSqlBaseParser.SORTED, 0)

        def PURGE(self):
            return self.getToken(SparkSqlBaseParser.PURGE, 0)

        def INPUTFORMAT(self):
            return self.getToken(SparkSqlBaseParser.INPUTFORMAT, 0)

        def OUTPUTFORMAT(self):
            return self.getToken(SparkSqlBaseParser.OUTPUTFORMAT, 0)

        def DBPROPERTIES(self):
            return self.getToken(SparkSqlBaseParser.DBPROPERTIES, 0)

        def DFS(self):
            return self.getToken(SparkSqlBaseParser.DFS, 0)

        def TRUNCATE(self):
            return self.getToken(SparkSqlBaseParser.TRUNCATE, 0)

        def COMPUTE(self):
            return self.getToken(SparkSqlBaseParser.COMPUTE, 0)

        def LIST(self):
            return self.getToken(SparkSqlBaseParser.LIST, 0)

        def STATISTICS(self):
            return self.getToken(SparkSqlBaseParser.STATISTICS, 0)

        def ANALYZE(self):
            return self.getToken(SparkSqlBaseParser.ANALYZE, 0)

        def PARTITIONED(self):
            return self.getToken(SparkSqlBaseParser.PARTITIONED, 0)

        def EXTERNAL(self):
            return self.getToken(SparkSqlBaseParser.EXTERNAL, 0)

        def DEFINED(self):
            return self.getToken(SparkSqlBaseParser.DEFINED, 0)

        def RECORDWRITER(self):
            return self.getToken(SparkSqlBaseParser.RECORDWRITER, 0)

        def REVOKE(self):
            return self.getToken(SparkSqlBaseParser.REVOKE, 0)

        def GRANT(self):
            return self.getToken(SparkSqlBaseParser.GRANT, 0)

        def LOCK(self):
            return self.getToken(SparkSqlBaseParser.LOCK, 0)

        def UNLOCK(self):
            return self.getToken(SparkSqlBaseParser.UNLOCK, 0)

        def MSCK(self):
            return self.getToken(SparkSqlBaseParser.MSCK, 0)

        def REPAIR(self):
            return self.getToken(SparkSqlBaseParser.REPAIR, 0)

        def RECOVER(self):
            return self.getToken(SparkSqlBaseParser.RECOVER, 0)

        def EXPORT(self):
            return self.getToken(SparkSqlBaseParser.EXPORT, 0)

        def IMPORT(self):
            return self.getToken(SparkSqlBaseParser.IMPORT, 0)

        def LOAD(self):
            return self.getToken(SparkSqlBaseParser.LOAD, 0)

        def VALUES(self):
            return self.getToken(SparkSqlBaseParser.VALUES, 0)

        def COMMENT(self):
            return self.getToken(SparkSqlBaseParser.COMMENT, 0)

        def ROLE(self):
            return self.getToken(SparkSqlBaseParser.ROLE, 0)

        def ROLES(self):
            return self.getToken(SparkSqlBaseParser.ROLES, 0)

        def COMPACTIONS(self):
            return self.getToken(SparkSqlBaseParser.COMPACTIONS, 0)

        def PRINCIPALS(self):
            return self.getToken(SparkSqlBaseParser.PRINCIPALS, 0)

        def TRANSACTIONS(self):
            return self.getToken(SparkSqlBaseParser.TRANSACTIONS, 0)

        def INDEX(self):
            return self.getToken(SparkSqlBaseParser.INDEX, 0)

        def INDEXES(self):
            return self.getToken(SparkSqlBaseParser.INDEXES, 0)

        def LOCKS(self):
            return self.getToken(SparkSqlBaseParser.LOCKS, 0)

        def OPTION(self):
            return self.getToken(SparkSqlBaseParser.OPTION, 0)

        def LOCAL(self):
            return self.getToken(SparkSqlBaseParser.LOCAL, 0)

        def INPATH(self):
            return self.getToken(SparkSqlBaseParser.INPATH, 0)

        def ASC(self):
            return self.getToken(SparkSqlBaseParser.ASC, 0)

        def DESC(self):
            return self.getToken(SparkSqlBaseParser.DESC, 0)

        def LIMIT(self):
            return self.getToken(SparkSqlBaseParser.LIMIT, 0)

        def RENAME(self):
            return self.getToken(SparkSqlBaseParser.RENAME, 0)

        def SETS(self):
            return self.getToken(SparkSqlBaseParser.SETS, 0)

        def AT(self):
            return self.getToken(SparkSqlBaseParser.AT, 0)

        def NULLS(self):
            return self.getToken(SparkSqlBaseParser.NULLS, 0)

        def OVERWRITE(self):
            return self.getToken(SparkSqlBaseParser.OVERWRITE, 0)

        def ALL(self):
            return self.getToken(SparkSqlBaseParser.ALL, 0)

        def ANY(self):
            return self.getToken(SparkSqlBaseParser.ANY, 0)

        def ALTER(self):
            return self.getToken(SparkSqlBaseParser.ALTER, 0)

        def AS(self):
            return self.getToken(SparkSqlBaseParser.AS, 0)

        def BETWEEN(self):
            return self.getToken(SparkSqlBaseParser.BETWEEN, 0)

        def BY(self):
            return self.getToken(SparkSqlBaseParser.BY, 0)

        def CREATE(self):
            return self.getToken(SparkSqlBaseParser.CREATE, 0)

        def DELETE(self):
            return self.getToken(SparkSqlBaseParser.DELETE, 0)

        def DESCRIBE(self):
            return self.getToken(SparkSqlBaseParser.DESCRIBE, 0)

        def DROP(self):
            return self.getToken(SparkSqlBaseParser.DROP, 0)

        def EXISTS(self):
            return self.getToken(SparkSqlBaseParser.EXISTS, 0)

        def FALSE(self):
            return self.getToken(SparkSqlBaseParser.FALSE, 0)

        def FOR(self):
            return self.getToken(SparkSqlBaseParser.FOR, 0)

        def GROUP(self):
            return self.getToken(SparkSqlBaseParser.GROUP, 0)

        def IN(self):
            return self.getToken(SparkSqlBaseParser.IN, 0)

        def INSERT(self):
            return self.getToken(SparkSqlBaseParser.INSERT, 0)

        def INTO(self):
            return self.getToken(SparkSqlBaseParser.INTO, 0)

        def IS(self):
            return self.getToken(SparkSqlBaseParser.IS, 0)

        def LIKE(self):
            return self.getToken(SparkSqlBaseParser.LIKE, 0)

        def NULL(self):
            return self.getToken(SparkSqlBaseParser.NULL, 0)

        def ORDER(self):
            return self.getToken(SparkSqlBaseParser.ORDER, 0)

        def OUTER(self):
            return self.getToken(SparkSqlBaseParser.OUTER, 0)

        def TABLE(self):
            return self.getToken(SparkSqlBaseParser.TABLE, 0)

        def TRUE(self):
            return self.getToken(SparkSqlBaseParser.TRUE, 0)

        def WITH(self):
            return self.getToken(SparkSqlBaseParser.WITH, 0)

        def RLIKE(self):
            return self.getToken(SparkSqlBaseParser.RLIKE, 0)

        def AND(self):
            return self.getToken(SparkSqlBaseParser.AND, 0)

        def CASE(self):
            return self.getToken(SparkSqlBaseParser.CASE, 0)

        def CAST(self):
            return self.getToken(SparkSqlBaseParser.CAST, 0)

        def DISTINCT(self):
            return self.getToken(SparkSqlBaseParser.DISTINCT, 0)

        def DIV(self):
            return self.getToken(SparkSqlBaseParser.DIV, 0)

        def ELSE(self):
            return self.getToken(SparkSqlBaseParser.ELSE, 0)

        def END(self):
            return self.getToken(SparkSqlBaseParser.END, 0)

        def FUNCTION(self):
            return self.getToken(SparkSqlBaseParser.FUNCTION, 0)

        def INTERVAL(self):
            return self.getToken(SparkSqlBaseParser.INTERVAL, 0)

        def MACRO(self):
            return self.getToken(SparkSqlBaseParser.MACRO, 0)

        def OR(self):
            return self.getToken(SparkSqlBaseParser.OR, 0)

        def STRATIFY(self):
            return self.getToken(SparkSqlBaseParser.STRATIFY, 0)

        def THEN(self):
            return self.getToken(SparkSqlBaseParser.THEN, 0)

        def UNBOUNDED(self):
            return self.getToken(SparkSqlBaseParser.UNBOUNDED, 0)

        def WHEN(self):
            return self.getToken(SparkSqlBaseParser.WHEN, 0)

        def DATABASE(self):
            return self.getToken(SparkSqlBaseParser.DATABASE, 0)

        def SELECT(self):
            return self.getToken(SparkSqlBaseParser.SELECT, 0)

        def FROM(self):
            return self.getToken(SparkSqlBaseParser.FROM, 0)

        def WHERE(self):
            return self.getToken(SparkSqlBaseParser.WHERE, 0)

        def HAVING(self):
            return self.getToken(SparkSqlBaseParser.HAVING, 0)

        def NOT(self):
            return self.getToken(SparkSqlBaseParser.NOT, 0)

        def DIRECTORY(self):
            return self.getToken(SparkSqlBaseParser.DIRECTORY, 0)

        def BOTH(self):
            return self.getToken(SparkSqlBaseParser.BOTH, 0)

        def LEADING(self):
            return self.getToken(SparkSqlBaseParser.LEADING, 0)

        def TRAILING(self):
            return self.getToken(SparkSqlBaseParser.TRAILING, 0)

        def getRuleIndex(self):
            return SparkSqlBaseParser.RULE_nonReserved

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNonReserved" ):
                listener.enterNonReserved(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNonReserved" ):
                listener.exitNonReserved(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNonReserved" ):
                return visitor.visitNonReserved(self)
            else:
                return visitor.visitChildren(self)




    def nonReserved(self):

        localctx = SparkSqlBaseParser.NonReservedContext(self, self._ctx, self.state)
        self.enterRule(localctx, 200, self.RULE_nonReserved)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 2442
            _la = self._input.LA(1)
            if not((((_la) & ~0x3f) == 0 and ((1 << _la) & ((1 << SparkSqlBaseParser.SELECT) | (1 << SparkSqlBaseParser.FROM) | (1 << SparkSqlBaseParser.ADD) | (1 << SparkSqlBaseParser.AS) | (1 << SparkSqlBaseParser.ALL) | (1 << SparkSqlBaseParser.ANY) | (1 << SparkSqlBaseParser.DISTINCT) | (1 << SparkSqlBaseParser.WHERE) | (1 << SparkSqlBaseParser.GROUP) | (1 << SparkSqlBaseParser.BY) | (1 << SparkSqlBaseParser.GROUPING) | (1 << SparkSqlBaseParser.SETS) | (1 << SparkSqlBaseParser.CUBE) | (1 << SparkSqlBaseParser.ROLLUP) | (1 << SparkSqlBaseParser.ORDER) | (1 << SparkSqlBaseParser.HAVING) | (1 << SparkSqlBaseParser.LIMIT) | (1 << SparkSqlBaseParser.AT) | (1 << SparkSqlBaseParser.OR) | (1 << SparkSqlBaseParser.AND) | (1 << SparkSqlBaseParser.IN) | (1 << SparkSqlBaseParser.NOT) | (1 << SparkSqlBaseParser.NO) | (1 << SparkSqlBaseParser.EXISTS) | (1 << SparkSqlBaseParser.BETWEEN) | (1 << SparkSqlBaseParser.LIKE) | (1 << SparkSqlBaseParser.RLIKE) | (1 << SparkSqlBaseParser.IS) | (1 << SparkSqlBaseParser.NULL) | (1 << SparkSqlBaseParser.TRUE) | (1 << SparkSqlBaseParser.FALSE) | (1 << SparkSqlBaseParser.NULLS) | (1 << SparkSqlBaseParser.ASC) | (1 << SparkSqlBaseParser.DESC) | (1 << SparkSqlBaseParser.FOR) | (1 << SparkSqlBaseParser.INTERVAL) | (1 << SparkSqlBaseParser.CASE) | (1 << SparkSqlBaseParser.WHEN) | (1 << SparkSqlBaseParser.THEN) | (1 << SparkSqlBaseParser.ELSE) | (1 << SparkSqlBaseParser.END) | (1 << SparkSqlBaseParser.OUTER) | (1 << SparkSqlBaseParser.PIVOT) | (1 << SparkSqlBaseParser.LATERAL))) != 0) or ((((_la - 64)) & ~0x3f) == 0 and ((1 << (_la - 64)) & ((1 << (SparkSqlBaseParser.WINDOW - 64)) | (1 << (SparkSqlBaseParser.OVER - 64)) | (1 << (SparkSqlBaseParser.PARTITION - 64)) | (1 << (SparkSqlBaseParser.RANGE - 64)) | (1 << (SparkSqlBaseParser.ROWS - 64)) | (1 << (SparkSqlBaseParser.UNBOUNDED - 64)) | (1 << (SparkSqlBaseParser.PRECEDING - 64)) | (1 << (SparkSqlBaseParser.FOLLOWING - 64)) | (1 << (SparkSqlBaseParser.CURRENT - 64)) | (1 << (SparkSqlBaseParser.FIRST - 64)) | (1 << (SparkSqlBaseParser.AFTER - 64)) | (1 << (SparkSqlBaseParser.LAST - 64)) | (1 << (SparkSqlBaseParser.ROW - 64)) | (1 << (SparkSqlBaseParser.WITH - 64)) | (1 << (SparkSqlBaseParser.VALUES - 64)) | (1 << (SparkSqlBaseParser.CREATE - 64)) | (1 << (SparkSqlBaseParser.TABLE - 64)) | (1 << (SparkSqlBaseParser.DIRECTORY - 64)) | (1 << (SparkSqlBaseParser.VIEW - 64)) | (1 << (SparkSqlBaseParser.REPLACE - 64)) | (1 << (SparkSqlBaseParser.INSERT - 64)) | (1 << (SparkSqlBaseParser.DELETE - 64)) | (1 << (SparkSqlBaseParser.INTO - 64)) | (1 << (SparkSqlBaseParser.DESCRIBE - 64)) | (1 << (SparkSqlBaseParser.EXPLAIN - 64)) | (1 << (SparkSqlBaseParser.FORMAT - 64)) | (1 << (SparkSqlBaseParser.LOGICAL - 64)) | (1 << (SparkSqlBaseParser.CODEGEN - 64)) | (1 << (SparkSqlBaseParser.COST - 64)) | (1 << (SparkSqlBaseParser.CAST - 64)) | (1 << (SparkSqlBaseParser.SHOW - 64)) | (1 << (SparkSqlBaseParser.TABLES - 64)) | (1 << (SparkSqlBaseParser.COLUMNS - 64)) | (1 << (SparkSqlBaseParser.COLUMN - 64)) | (1 << (SparkSqlBaseParser.USE - 64)) | (1 << (SparkSqlBaseParser.PARTITIONS - 64)) | (1 << (SparkSqlBaseParser.FUNCTIONS - 64)) | (1 << (SparkSqlBaseParser.DROP - 64)) | (1 << (SparkSqlBaseParser.TO - 64)) | (1 << (SparkSqlBaseParser.TABLESAMPLE - 64)) | (1 << (SparkSqlBaseParser.STRATIFY - 64)) | (1 << (SparkSqlBaseParser.ALTER - 64)) | (1 << (SparkSqlBaseParser.RENAME - 64)) | (1 << (SparkSqlBaseParser.ARRAY - 64)) | (1 << (SparkSqlBaseParser.MAP - 64)) | (1 << (SparkSqlBaseParser.STRUCT - 64)) | (1 << (SparkSqlBaseParser.COMMENT - 64)) | (1 << (SparkSqlBaseParser.SET - 64)) | (1 << (SparkSqlBaseParser.RESET - 64)) | (1 << (SparkSqlBaseParser.DATA - 64)) | (1 << (SparkSqlBaseParser.START - 64)) | (1 << (SparkSqlBaseParser.TRANSACTION - 64)) | (1 << (SparkSqlBaseParser.COMMIT - 64)) | (1 << (SparkSqlBaseParser.ROLLBACK - 64)) | (1 << (SparkSqlBaseParser.MACRO - 64)) | (1 << (SparkSqlBaseParser.IGNORE - 64)) | (1 << (SparkSqlBaseParser.BOTH - 64)) | (1 << (SparkSqlBaseParser.LEADING - 64)) | (1 << (SparkSqlBaseParser.TRAILING - 64)) | (1 << (SparkSqlBaseParser.IF - 64)))) != 0) or ((((_la - 128)) & ~0x3f) == 0 and ((1 << (_la - 128)) & ((1 << (SparkSqlBaseParser.POSITION - 128)) | (1 << (SparkSqlBaseParser.EXTRACT - 128)) | (1 << (SparkSqlBaseParser.DIV - 128)) | (1 << (SparkSqlBaseParser.PERCENTLIT - 128)) | (1 << (SparkSqlBaseParser.BUCKET - 128)) | (1 << (SparkSqlBaseParser.OUT - 128)) | (1 << (SparkSqlBaseParser.OF - 128)) | (1 << (SparkSqlBaseParser.SORT - 128)) | (1 << (SparkSqlBaseParser.CLUSTER - 128)) | (1 << (SparkSqlBaseParser.DISTRIBUTE - 128)) | (1 << (SparkSqlBaseParser.OVERWRITE - 128)) | (1 << (SparkSqlBaseParser.TRANSFORM - 128)) | (1 << (SparkSqlBaseParser.REDUCE - 128)) | (1 << (SparkSqlBaseParser.SERDE - 128)) | (1 << (SparkSqlBaseParser.SERDEPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.RECORDREADER - 128)) | (1 << (SparkSqlBaseParser.RECORDWRITER - 128)) | (1 << (SparkSqlBaseParser.DELIMITED - 128)) | (1 << (SparkSqlBaseParser.FIELDS - 128)) | (1 << (SparkSqlBaseParser.TERMINATED - 128)) | (1 << (SparkSqlBaseParser.COLLECTION - 128)) | (1 << (SparkSqlBaseParser.ITEMS - 128)) | (1 << (SparkSqlBaseParser.KEYS - 128)) | (1 << (SparkSqlBaseParser.ESCAPED - 128)) | (1 << (SparkSqlBaseParser.LINES - 128)) | (1 << (SparkSqlBaseParser.SEPARATED - 128)) | (1 << (SparkSqlBaseParser.FUNCTION - 128)) | (1 << (SparkSqlBaseParser.EXTENDED - 128)) | (1 << (SparkSqlBaseParser.REFRESH - 128)) | (1 << (SparkSqlBaseParser.CLEAR - 128)) | (1 << (SparkSqlBaseParser.CACHE - 128)) | (1 << (SparkSqlBaseParser.UNCACHE - 128)) | (1 << (SparkSqlBaseParser.LAZY - 128)) | (1 << (SparkSqlBaseParser.FORMATTED - 128)) | (1 << (SparkSqlBaseParser.GLOBAL - 128)) | (1 << (SparkSqlBaseParser.TEMPORARY - 128)) | (1 << (SparkSqlBaseParser.OPTIONS - 128)) | (1 << (SparkSqlBaseParser.UNSET - 128)) | (1 << (SparkSqlBaseParser.TBLPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.DBPROPERTIES - 128)) | (1 << (SparkSqlBaseParser.BUCKETS - 128)) | (1 << (SparkSqlBaseParser.SKEWED - 128)) | (1 << (SparkSqlBaseParser.STORED - 128)) | (1 << (SparkSqlBaseParser.DIRECTORIES - 128)) | (1 << (SparkSqlBaseParser.LOCATION - 128)))) != 0) or ((((_la - 192)) & ~0x3f) == 0 and ((1 << (_la - 192)) & ((1 << (SparkSqlBaseParser.EXCHANGE - 192)) | (1 << (SparkSqlBaseParser.ARCHIVE - 192)) | (1 << (SparkSqlBaseParser.UNARCHIVE - 192)) | (1 << (SparkSqlBaseParser.FILEFORMAT - 192)) | (1 << (SparkSqlBaseParser.TOUCH - 192)) | (1 << (SparkSqlBaseParser.COMPACT - 192)) | (1 << (SparkSqlBaseParser.CONCATENATE - 192)) | (1 << (SparkSqlBaseParser.CHANGE - 192)) | (1 << (SparkSqlBaseParser.CASCADE - 192)) | (1 << (SparkSqlBaseParser.RESTRICT - 192)) | (1 << (SparkSqlBaseParser.CLUSTERED - 192)) | (1 << (SparkSqlBaseParser.SORTED - 192)) | (1 << (SparkSqlBaseParser.PURGE - 192)) | (1 << (SparkSqlBaseParser.INPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.OUTPUTFORMAT - 192)) | (1 << (SparkSqlBaseParser.DATABASE - 192)) | (1 << (SparkSqlBaseParser.DATABASES - 192)) | (1 << (SparkSqlBaseParser.DFS - 192)) | (1 << (SparkSqlBaseParser.TRUNCATE - 192)) | (1 << (SparkSqlBaseParser.ANALYZE - 192)) | (1 << (SparkSqlBaseParser.COMPUTE - 192)) | (1 << (SparkSqlBaseParser.LIST - 192)) | (1 << (SparkSqlBaseParser.STATISTICS - 192)) | (1 << (SparkSqlBaseParser.PARTITIONED - 192)) | (1 << (SparkSqlBaseParser.EXTERNAL - 192)) | (1 << (SparkSqlBaseParser.DEFINED - 192)) | (1 << (SparkSqlBaseParser.REVOKE - 192)) | (1 << (SparkSqlBaseParser.GRANT - 192)) | (1 << (SparkSqlBaseParser.LOCK - 192)) | (1 << (SparkSqlBaseParser.UNLOCK - 192)) | (1 << (SparkSqlBaseParser.MSCK - 192)) | (1 << (SparkSqlBaseParser.REPAIR - 192)) | (1 << (SparkSqlBaseParser.RECOVER - 192)) | (1 << (SparkSqlBaseParser.EXPORT - 192)) | (1 << (SparkSqlBaseParser.IMPORT - 192)) | (1 << (SparkSqlBaseParser.LOAD - 192)) | (1 << (SparkSqlBaseParser.ROLE - 192)) | (1 << (SparkSqlBaseParser.ROLES - 192)) | (1 << (SparkSqlBaseParser.COMPACTIONS - 192)) | (1 << (SparkSqlBaseParser.PRINCIPALS - 192)) | (1 << (SparkSqlBaseParser.TRANSACTIONS - 192)) | (1 << (SparkSqlBaseParser.INDEX - 192)) | (1 << (SparkSqlBaseParser.INDEXES - 192)) | (1 << (SparkSqlBaseParser.LOCKS - 192)) | (1 << (SparkSqlBaseParser.OPTION - 192)) | (1 << (SparkSqlBaseParser.LOCAL - 192)) | (1 << (SparkSqlBaseParser.INPATH - 192)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self._errHandler.reportMatch(self)
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx



    def sempred(self, localctx:RuleContext, ruleIndex:int, predIndex:int):
        if self._predicates == None:
            self._predicates = dict()
        self._predicates[35] = self.queryTerm_sempred
        self._predicates[71] = self.booleanExpression_sempred
        self._predicates[73] = self.valueExpression_sempred
        self._predicates[74] = self.primaryExpression_sempred
        pred = self._predicates.get(ruleIndex, None)
        if pred is None:
            raise Exception("No predicate with index:" + str(ruleIndex))
        else:
            return pred(localctx, predIndex)

    def queryTerm_sempred(self, localctx:QueryTermContext, predIndex:int):
            if predIndex == 0:
                return self.precpred(self._ctx, 3)
         

            if predIndex == 1:
                return False
         

            if predIndex == 2:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 3:
                return not False
         

            if predIndex == 4:
                return self.precpred(self._ctx, 1)
         

            if predIndex == 5:
                return not False
         

    def booleanExpression_sempred(self, localctx:BooleanExpressionContext, predIndex:int):
            if predIndex == 6:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 7:
                return self.precpred(self._ctx, 1)
         

    def valueExpression_sempred(self, localctx:ValueExpressionContext, predIndex:int):
            if predIndex == 8:
                return self.precpred(self._ctx, 6)
         

            if predIndex == 9:
                return self.precpred(self._ctx, 5)
         

            if predIndex == 10:
                return self.precpred(self._ctx, 4)
         

            if predIndex == 11:
                return self.precpred(self._ctx, 3)
         

            if predIndex == 12:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 13:
                return self.precpred(self._ctx, 1)
         

    def primaryExpression_sempred(self, localctx:PrimaryExpressionContext, predIndex:int):
            if predIndex == 14:
                return self.precpred(self._ctx, 5)
         

            if predIndex == 15:
                return self.precpred(self._ctx, 3)

    @property
    def interp(self):
        return self._interp
         




